THEDRAGONHATCHLING: THEMISSINGLINK
BETWEEN THETRANSFORMER ANDMODELS OF THEBRAIN
Adrian Kosowski∗† Przemysław Uzna´nski† Jan Chorowski† Zuzanna Stamirowska†
Michał Bartoszkiewicz†
Pathway, Palo Alto, USA
research@pathway.com
ABSTRACT
The relationship between computing systems and the brain has served as motivation for pioneering
theoreticians since John von Neumann and Alan Turing. Uniform, scale-free biological networks,
such as the brain, have powerful properties, including generalizing over time, which is the main
barrier for Machine Learning on the path to Universal Reasoning Models.
We introduce ‘Dragon Hatchling’ (BDH), a new Large Language Model architecture based on a
scale-free biologically inspired network ofnlocally-interacting neuron particles. BDH couples
strong theoretical foundations and inherent interpretability without sacrificing Transformer-like per-
formance.
BDH is a practical, performant state-of-the-art attention-based state space sequence learning archi-
tecture. In addition to being a graph model, BDH admits a GPU-friendly formulation. It exhibits
Transformer-like scaling laws: we find empirically that BDH rivals GPT2-architecture Transformer
performance on language and translation tasks, at the same number of parameters (10M to 1B), for
the same training data.
BDH provides theoretical foundations for understanding model behavior in the limit of large size and
reasoning time. Our results, formalized as a chain of reductions of expressiveness in the framework
of computational Complexity Theory and Distributed Computing, and combined with findings on
the BDH model, show a macro-to-micro correspondence of function between the general attention
mechanisms in state-of-the-art Language Models, and attention mechanisms observed in the brain.
These attention mechanisms formally converge as closed-form local graph dynamics at neurons and
synapses: “the equations of reasoning”.
BDH can be represented as a brain model. It containsnneurons, organized as an excitatory circuit
and an inhibitory circuit with integrate-and-fire thresholding of input signals at neurons. The work-
ing memory of BDH during inference entirely relies on synaptic plasticity with Hebbian learning
using spiking neurons, at potentiation scales of minutes for the brain (up to hundreds of tokens). We
confirm empirically that specific, individual synapses strengthen connection whenever BDH hears
or reasons about a specific concept while processing language inputs. The neuron interaction net-
work of BDH is a graph of high modularity with heavy-tailed degree distribution. The BDH model
is biologically plausible, explaining one possible mechanism which human neurons could use to
achieve speech.
BDH is designed for interpretability. Activation vectors of BDH are sparse and positive. We demon-
strate monosemanticity in BDH on language tasks, including representation of concept abstractions,
which happens even for small models, below 100M-parameter scale. Interpretability of state, which
goes beyond interpretability of neurons and model parameters, is an inherent feature of the BDH
architecture.
We believe BDH opens the door to a new theory of “Thermodynamic Limit” behavior for language
and reasoning models, with the ultimate goal of Probably Approximately Correct (PAC)-like bounds
for generalization of reasoning over time.
▷Technical blog entry:https://pathway.com/research/bdh.
▷Code listings:https://github.com/pathwaycom/bdh.
∗Author contributions are listed at the end of the paper.
†Corresponding author.
arXiv:2509.26507v1  [cs.NE]  30 Sep 2025
Contents
1 Introduction 3
1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.2 Intuition of results: combiningmodus ponensreasoning with Hebbian learning . . . . . . . . . . . . 5
1.3 Contribution of this work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
1.4 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2 BDH: a language model architecture given by local distributed graph dynamics 10
2.1 Formalism for local graph-based language models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
2.2 Definition of BDH as a local edge-reweighting process (equations of reasoning) . . . . . . . . . . . . 12
2.3 Interpretation of attention as a micro-inductive bias of reasoning . . . . . . . . . . . . . . . . . . . . 14
2.4 Interpretation of BDH as an oscillator network toy-model . . . . . . . . . . . . . . . . . . . . . . . . 14
2.5 Expressing BDH using brain models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
3 BDH-GPU: a tensor-friendly version of the BDH architecture 17
3.1 Notation for BDH-GPU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
3.2 Definition of BDH-GPU as a state-space system . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
3.3 Interpretation of BDH-GPU as a local interacting particle system . . . . . . . . . . . . . . . . . . . . 20
3.4 Expressing BDH-GPU using BDH: preserving parameter and state size . . . . . . . . . . . . . . . . 21
4 Implementation and scaling laws 23
4.1 Implementation characteristics of BDH-GPU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
4.2 Comparison of BDH-GPU to GPT2-like Transformers . . . . . . . . . . . . . . . . . . . . . . . . . 24
4.3 Comparison of BDH-GPU to other sequence processing architectures . . . . . . . . . . . . . . . . . 24
5 Analysis: emergence of modularity and scale-free structure 26
5.1 Background: modularity and scale-free property of systems . . . . . . . . . . . . . . . . . . . . . . . 26
5.2 BDH-GPU feed-forward network with the ‘ReLU-lowrank’ block . . . . . . . . . . . . . . . . . . . 27
5.3 ReLU-lowrank as a signal propagation dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
5.4 Modularity in BDH-GPU signal propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
5.5 Empirical findings: parameter distribution in ReLU-lowrank matrix products . . . . . . . . . . . . . 31
6 Analysis: linear attention, sparse positive activation, and monosemanticity 34
6.1 Macro-expressiveness of attention in BDH-GPU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
6.2 Micro-interpretation of attention in BDH-GPU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
6.3 Empirical findings: monosemantic synapses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
6.4 Empirical findings: sparse neuron activations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
7 Playing with the Hatchling 41
7.1 Model merging: concatenating two models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
7.2 Training without backpropagation through time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
2
8 Conclusions 43
8.1 Takeaways for model engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
8.2 Implications for brain science . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
8.3 Societal impact . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
A Connection between generalization of reasoning and computational expressiveness 53
B Further description of experiments 53
C Omitted formal claims and proofs 55
D Desirable properties of a local graph dynamics for language models 58
E BDH-GPU PyTorch code listing 61
1 Introduction
Long reasoning and long context inference pose a severe challenge of generalization across scales of time. From
vibe coding to market research, users of Language Models and agentic systems are increasingly relying on defining
tasks through informal prompts, which the language model is expected to follow over long sequences of actions or
decisions, like a reasonable human actor would. Implicitly, most users expect machines to follow the generalization
patterns of human reasoning, i.e., to generalize reasoning in the same way as humans do. The complexity of tasks
attempted in this way has gone from the equivalent of hours of human work for a single prompt, to weeks (Emberson
et al., 2025). However, experimental evidence suggests that the Transformer and other state-of-the-art architectures
do not systematically generalize chain-of-thought (CoT) reasoning to scenarios longer than the ones seen during
training (Shojaee et al., 2025).
Chain-of-Thought reasoning models can be considered through the lens of computational complexity theory. For a
Language Model to generalize human reasoning on a given class of tasks, we expect this model to be able to emulate
the corresponding reasoning function of the human brain efficiently.3 While the Transformer with Chain-of-Thought
is Turing-complete and can efficiently emulate certain restricted classes of formal languages (Merrill and Sabharwal,
2024), this does not in itself provide a satisfactory answer as to how it emulates human reasoning. The human brain
is an extremely complex graph-based distributed computing system withn≈8·10 10 neurons, andm >10 14 neuron
connections (synapses), of which a certain percentage is actively used. The direct simulation of such a distributed
system by a Language Model through generic Turing-machine reductions would require billions of CoT tokens of the
Language Model to represent a single step of reasoning in the brain. So, do Transformer-like models actually relate to
brain function?
Such a relationship should follow more closely from a tighter, more direct simulation. Finding such a connection
between Language Models and human brain function has, so far, proved elusive. Indeed, when comparing a tensor-
based Language Model based on feed-forward network blocks and attention, to a uniform, scale-free graph-based
distributed system, such as the brain, the two may, at first glance, appear very dissimilar.
This apparent dissimilarity of structure between Language Models and brain structure has been one of the main causes
of concern in attempts to reconcile Computation and the Brain (Olshausen, 2018), as well as a cause of concern
regarding the difficulty to foresee the behavior of autonomous AI systems.
In this paper, we show the link between the Transformer and Brain models.
1.1 Motivation
The development of Artificial Intelligence and the understanding of Neural Science have gone hand in hand since
the 1940’s, both being efforts to understand the “mystery of intelligence”. The relationship between computing sys-
tems and the brain served as motivation for the pioneering theoreticians such as John von Neumann (1958), Alan
Turing (1950), Goeff Hinton (2005), Warren McCulloch and Walter Pitts (1943), and Horace Barlow (1972).
3We provide a more formal explanation of this point in Appendix A.
3
Since then, milestones in Machine Learning around Artificial Neural Networks — using backpropagation with
SGD (Rumelhart et al., 1986), followed by Deep Learning (LeCun et al., 2015), and the Attention mechanism (Bah-
danau et al., 2015; Vaswani et al., 2017) — have split the “mystery of how intelligence works” into two. First, we
still have no clear explanation for the micro-to-macro correspondence of the reasoning function of the brain. Second,
we do not understand the correspondence between the artificial and natural systems — notably, how effects observed
in the brain (emergent network; sparse activations; oscillatory phenomena; unknown relationship to backpropagation
mechanisms) map into those which appear in systems based on dense tensors, trained using gradient back-propagation
over time.
Reconciling Reasoning Function of the Brain with Language Models.There is a seemingly deep divide between
state-of-the-art language models, like the Transformer, and natural distributed systems with local graph dynamics, like
those of the brain. Specifically, for the brain, we do not understand how the reasoning function emerges from neuronal
dynamics at the microscale. For the Transformer, the interpretation of function is given at the level of vectors, but not
at the level of particle dynamics or a uniform distributed computing system.
Language and reasoning are the key areas of higher-order brain function for which we do not yet have a complete un-
derstanding. Many other areas of brain function have been explained through analogies to Machine Learning architec-
tures. For example, the visual cortex is becoming well-understood, especially in its peripheral layers, and the observed
inference dynamics are shown to have a correspondence to known Deep Learning architectures (Mohsenzadeh et al.,
2020). The use of sparse coding by the brain was considered in the context of processing visual cues (Olshausen and
Field, 1997), as well as for the olfactory systems (Lin et al., 2014). By contrast, higher-order cognitive functions of
the association cortex of the human brain, such as language and reasoning, are among the least understood. A number
of models provide partial explanations and have been verified at small scales. Some of the first attempts include ex-
plaining context-dependent computation in the prefrontal cortex using population dynamics of an RNN (Mante et al.,
2013). Later approaches include the Tolman-Eichenbaum Machine (Whittington et al., 2020, 2022), as well as a num-
ber of more recent works (Papadimitriou et al., 2020; Dabagia et al., 2024; Mitropolsky and Papadimitriou, 2025).
One of the main stumbling blocks concerns going from spiking activation patterns at neurons, and localized attention
effects at synapses, to a higher-order function, serving a reasoning purpose, efficiently organized at a scale of millions
to billions of neurons.
Conversely, for Language Models architectures such as the Transformer, we miss a compact micro-interpretation as a
distributed system. The expressiveness of the Transformer has been approximated using approaches from centralized
computing and Complexity Theory, rather than from distributed systems. In the centralized perspective, a language
model can be seen as a transformation function from inputs into outputs. The computational expressiveness of the
Transformer architecture may then be approximated through frameworks based on RASP, such as RASP-L (Zhou
et al., 2024) or C-RASP (Yang and Chiang, 2024; Huang et al., 2025). RASP-L provides a very convenient heuristic
for estimating Transformer expressiveness at the rather coarse level of vector operations, while C-RASP provides
a more specialized lower-bound on expressiveness, capturing a class of formulas of temporal counting logic. Both
frameworks have been used to suggest theoretical models of task length generalization. This type of expressiveness
techniques, however, do not lead to a uniform asymptotic model for the behavior of the Transformer, whether in GPT2
architecture or simplified. The scaling of the Transformer in its different dimensions, and the need to manipulate
context length, complicate this goal.
The lack of such a uniform model also makes it hard to compare the capabilities of the Transformer to the capabilities
of the brain at the level of correspondence of structure. Generally, the temporal behavior of a state-space system is
reflected in its structure4.
Understanding whether it is possible to show alignment of the temporal behavior of two systems, which do not display
any structural correspondence, and without a clear idea of how the weight tensors and state representation of one
system ‘embed’ into the graph structure and state representation of the other system, is an awkward task.
This brings us naturally to our motivational objective: Can we create Machine Learning models which are closer to
the desirable properties of natural (human) reasoning systems, and which exhibit the same types of limit and scaling
behavior as such natural systems?
Towards scale-free foreseeable AI.Ensuring correct scaling behavior of inference over time is of paramount im-
portance for the deployment of AI whose reasoning or actions are not subject to strict human supervision. Most
reasoning models and AI agentic systems admit limit objects (i.e., extensions to infinite time and infinite size) which
are Turing-complete (cf. e.g. (Merrill and Sabharwal, 2024; Pérez et al., 2021; Jojic et al., 2023)). This means that
4For a linear system, temporal behavior would be a direct consequence of the spectral properties of the system. The considered
systems dynamics are not linear.
4
they should be treated like computer programs — and should be approached by the users with the same standards of
care, as a computer program of unknown origin and unknown purpose.
An AI model can malfunction when allowed to run for a long time autonomously, i.e., without human validation of ac-
tions and reasoning outcomes. The most painful of all consequences, perhaps, is the concept of a failed generalization
of reasoning (a malfunction with respect to the original task objective) over time, leading to a grotesque effect known
as the “Paperclip Factory” (Bostrom, 2014).
Can the risk of such unsuccessful generalization be bounded?
There are at least two scenarios in which a black-box modelMcannot be considered to have undergone previous
empirical validation, and consequently cannot be used in higher-risk autonomous AI use cases.
1. Length-generalization scenario: ModelMis expected to act autonomously on a task which is longer than
tasks forming part of its validation set.
2. Model scaling scenario: ModelMis not exactly the same closed system as the one which was tested during
validation. For example, suppose that modelsM 1 andM 2 were tested individually on smaller tasks, and let
Mbe an agentic system composed of instances ofM 1 andM 2 which communicate with exchange messages
with each other during inference.
A natural way of avoiding both difficulties consists in studying systems which are scale-free with respect to size and
time, and admit a form of uniform “thermodynamic limit” behavior. The limit behavior of computational systems at
criticality naturally connects the size of the system with the probable duration of its operation, with the connection
usually taking polynomial form (cf. e.g. (Björner et al., 1991; Cairns, 2018; Rolla, 2020) for examples of graph-based
interacting particle systems for which rigorous results have been obtained in this direction). Consider a modelM n
with architectureA, parameterized by its sizen(with the interpretation of the number of uniform particles), and
sampled from some space ofn-neuron models in architectureAin some space equipped with a probability measure,
Mn ∼ P A(n). Informally, if the limit objectP A := limn→∞ PA(n)exists (under an appropriate, well-defined sense
of uniformity of limit) then modelsM n, fornsufficiently large, will admit in the limit asymptotic properties, which
can be used to characterize their behavior over time.
The existence of such a limit theory means that we can characterize, with bounded probability of error, the behavior
of a family of large models, havingO(n)parameters, while relying on a theory which is independent of the specific
structure and size of the specific model. In this way, the limit behavior of a system of a very large number of interacting
uniform particles over time becomes (stochastically) foreseeable in the sense of its adherence to expected behavior,
which can be extrapolated from observations at shorter time scales. Thus, small tests may be conceived in order to
provide validation for a scale-free system at long time scales.
Introducing Axiomatic AI.Axiomatic systems are those in which micro-foundations and the macro-description
which arises from them are consistent and well-understood. The need for axiomatic understanding was highlighted
by David Hilbert (1902), and has become the foundation in Statistical Physics (e.g. thermodynamics, fluid dynam-
ics, spin glass theory), cellular mechanisms, Social Networks Science, and reconciliation of Microeconomics and
Macroeconomics through a Network Economics perspective.
This paper brings a micro-foundational understanding to Language Model inference, to the mechanisms of in-context
learning, and Chain-of-Thought reasoning dynamics.
The considerations in this work naturally support a shift of perspective fromInterpretable AI, which gives an approx-
imate understanding of what the model is doing now (without necessarily telling us what its current actions are going
to lead to over longer time scales), toAxiomatic AI, where we also understand the micro-foundations of how the model
can be expected to behave subsequently over time.
1.2 Intuition of results: combiningmodus ponensreasoning with Hebbian learning
In this section we provide the reader with some of the main intuitions behind this work which, we hope, will help to
navigate the remaining, more formal parts of this paper with ease.
While there are many formal deductive systems in logic, they predominantly rely on themodus ponensinference rule.
Applied to a rule-based reasoning system, it takes the following form:
If we know that thei-th fact is true, and our rulesetσindicates that thei-th fact implies thej-th fact, then we know
that thej-th fact is true as well. In an approximate reasoning system, the strength of the ruleσ(i, j)indicates how the
5
beliefX(i)of the system affects its beliefA(j). We could write:
X(i), σ(i, j)− →A(j),(1)
to indicate that ifX(i)is a weighted belief, it contributesX(i)σ(i, j)to the system’s beliefA(j).
Practical logical inference systems differ in strategies employed for rule selection, with the most advanced ones al-
lowing direct manipulation of the ruleset, effectively resulting in a form of program evolution during inference 5. For
an approximate reasoning system, such a heuristic could manipulate the strength of rules, modulating the impact of
beliefX(i)on the system’s beliefA(j).
Hebbian learning (Hebb, 1949), often presented as the mnemonic “Neurons that fire together wire together”, can be
seen as a heuristic for ruleset manipulation. It postulates that synaptic connections are strengthened when the activity
of one neuron,Y(i), led to the firing of another neuron,X(j). In the context of an adaptive, approximate inference
system, the Hebbian heuristic means that if during the course of operation a facticontributed some evidence forj, the
system increases the significance of the implicationσ(i, j). We could write this rule as:
Y(i), X(j)− →σ(i, j),(2)
with the interpretation that co-presence (or a spike) ofY(i)followed byX(j)increasesσ(i, j)byY(i)X(j).
The relations (1) and (2), over a set ofnfacts, may form the basis of a simple approximate reasoning system that
adapts its operation to the problem at hand. Starting with some initial connections between facts, the system applies
the rules to discover new facts, at the same time reweighting the ruleset in a way that strengthens the connections
between the initial and derived facts. Effectively, should the system be rerun with the new ruleset, it would arrive at
similar conclusions faster.
Suppose now that the reasoning system is equipped with two sets of rules: a fixed setGand an evolving setσ. From
a machine learning perspective, the fixed rulesetGcan be seen as model weights in Deep Learning terminology,
learned using e.g. error backpropagation on a training set. On the other hand, the evolving ruleset can be seen as
the temporal state of the reasoning system, sometimes called “fast weights” (Hinton and Plaut, 1987; Schmidhuber,
1993; Ba et al., 2016a). Fast-weights systems have a favorable ratio of state size to parameter count. A system withn
facts hasm=O(n 2)trainable parameters (expressed using one or moren×nmatrices). A classical recurrent neural
net, such as the LSTM (Hochreiter and Schmidhuber, 1997), treats individual fact (neuron) activations as its state,
thus maintaining onlyO(n)state variables. On the other hand, the evolving set of fast-weightsσhasm=O(n 2)
state entries. We believe this 1-1 ratio of trainable parameter to state size is important in designing practical reasoning
systems and may justify the success of the Transformer (Vaswani et al., 2017) and state-space (Gu and Dao, 2024)
sequence processing models.
Now, bearing in mind that the trainable parameters and state have comparable sizem, we can adjust the ratio between
this valuemand the sizenof the fact base. This will happen through a choice of sparsity for then×nmatrices carrying
parameters and state, resulting in a specific relationship of the two values,n≪m≪n 2. In this way, our system gets a
natural interpretation in terms of graphs onnnodes andmedges, with the graph edges tasked with their first two roles:
carrying state, and, carrying trainable parameters. Finally, we will give our system an interpretation of a dynamical
system with distributed (localized) dynamics, and we will task our edges with their third crucial role: mediating in
communication between nodes of the system. In this way, through assimilation of edges to natural function in the
brain, we will refer to themedges assynapsesconnecting a set ofnneuronsinto a distributed graph-based system.
In the following Section 2, we will introduce BDH, a reasoning system that formalizes and combines relations (1) and
(2) with dynamics involving fixed rules. The BDH system:
1. is a reasoning system, efficiently using themodus ponensreasoning rule with heuristic rule reweighting,
based on (1) and (2),
2. can be implemented with local graph dynamics, making it suitable for brain-like execution model, and
amenable to a principled, axiomatic description,
3. contains a set of fixed connections (parameters), and a set of dynamically adjusted connections (σ), which
can be seen as its dynamic state updated with a Hebbian learning rule,
4. admits as its special case BDH-GPU, a GPU-efficient reasoning model architecture, introduced in Section 3
and experimentally validated at scale in Section 7 in direct comparison to state-of-the-art GPT2-like Trans-
formers.
5The authors’ personal experience with writing efficient Prolog programs confirms that such direct ruleset management is often
a necessary pragmatic evil, guiding the inference system in the right direction.
6
1.3 Contribution of this work
The focus of this paper is in explaining the dynamics of the primary function of language and reasoning models:
inference. We provide a description of a language model architecture which is directly comparable to the Transformer,
and admits a clear and interpretable local interpretation of its inference dynamics as a programmable interacting
particle system.
Language Models as Local Graph Dynamics.In Section 2, we introduce a graph-based model architecture called
BDH, where all model parameters are represented as topology and weights of the communication graph, and model
state during inference is represented as edge-reweighting applied to this graph topology.
Claim 1(informal overview of theoretical results for BDH).We introduce a state-space Machine Learning archi-
tecture called BDH, formed by a system ofnparticles called neurons which communicate in a way governed by the
weights and topology of the system graph, representing a “communication by wire” network.
•The inference dynamics of BDH, treated as a distributed system, can be represented as execution of local
rulesets fornparticles with programmable interactions, with particles acting as nodes of the interaction
graph and scalar state variables located on its edges (cf. Section 2.2).
•The local kernel of BDH can be naturally expressed (emulated) by a graph-based Spiking Neural Network
system capable of Hebbian learning dynamics, an Excitatory circuit, and an Inhibitory circuit on ann-neuron
system described by a neuron interaction graph (cf. Section 2.5).
In order to train BDH efficiently and analyze its performance, we restrict it, making this restriction the core of a GPU-
friendly architecture calledBDH-GPU. This restriction is obtained by treating the communication of thenparticles as
proceeding through a mean-field (“radio network”), rather than a graph (“communication by wire”), cf. Fig. 3 for an
explanation of how the state-space equations of BDH-GPU are obtained from BDH.
This allows us to train a mathematically equivalent model, while localizing its state in short vectors at neurons, not at
connections (synapses) of the system.
A tensor-friendly case of BDH: the BDH-GPU architecture.The BDH-GPU architecture, like the Transformer,
crucially relies on an attention mechanism, and is amenable to token-parallel training on GPU for next token prediction
tasks. Unlike the Transformer, activation vectors of BDH-GPU appear in a very high dimensionn, are positive by
design, and turn out to be sparse.
Claim 2(informal overview of theoretical results for BDH-GPU).We introduce a Machine Learning architecture
called BDH-GPU, parameterized by a single (very large) scaling parameternand a second parameterd,logn <
d≪n(d= 256in practice), such that:
•A model in BDH-GPU(n, d)has(3 +o(1))ndparameters, and admits a precise interpretation as a state-
space system following the local dynamics of an-particle system in an interaction field subject to equations
of state(8). This system is described byO(d)parameters per particle, whose interaction field has mean field
interpretation, which in a computational view corresponds to a particle communication network realized by
means of “noisy radio broadcast”.
•BDH-GPU is a special case of BDH in the sense that, for any BDH-GPU model withnparticles, there exists
a BDH model withnparticles with the same inference behavior and the same sizeO(nd)of trainable param-
eters, with the two models being formally equivalent up to placement of Layer Norms (cf. Claims 3 and 4).
•The BDH-GPU architecture relies on a combination of two blocks: a specific kind ofReLU-lowrankfeed-
forward network, and alinear attentionmechanism, which both operate in the same neuron dimensionn,
using positive activation vectors.
•The mechanisms of BDH-GPU, considered at the macro-level of activation vectors inR n, can be compared
to those of the Transformer (cf. Section 6.1, Section 5.2). This justifies the applicability of the frameworks of
approximate macro-expressiveness, based on RASP (Weiss et al., 2021; Zhou et al., 2024; Yang and Chiang,
2024) and designed for the Transformer, to BDH-GPU.
•The micro-interpretation of BDH-GPU mechanisms as neuron-neuron interaction dynamics: (1) explains
mechanisms of in-cluster communication of neurons and the spontaneous emergence of graph structure with
high Newman modularity in the neuron-neuron communication network (cf. Section 5), and (2) provides a
strict correspondence between the macro-mechanism of in-context inference based on attention and the local
7
representation of state on individual neuron-neuron pairs (synapses) with state update dynamics based on
sporadic updates to synaptic edge weight (cf. Section 6).
The above results are complemented by empirical findings.
Empirical Finding 1(informal overview of empirical results of BDH-GPU).BDH-GPU is represented as a tensor-
based architecture and can be trained with standard back-propagation methods (cf. Section 3).
•The BDH-GPU architecture is shown to follow scaling laws (parameters vs. loss) of optimized Transformers
in the GPT architecture, at parameter scales between 10M to 1B, on all next token prediction tasks we
tested, including tasks of language and translation reminiscent of those in the original benchmark set for the
Transformer architecture (cf. Section 4.2).
•An emergent network reflecting the associated BDH graph dynamics can be read out directly from the pa-
rameter matrices of a trained BDH-GPU model, showing emergence of graph structure (cf. Section 5.5).
•The positive activations of BDH-GPU exhibit sparsity (at about 5% level) in theyvectors of its state space
dynamics, with sparsity levels reflecting the amount of activity being performed by BDH-GPU for a given
token (cf. Section 6.2).
•In-context state of BDH-GPU attention is shown to localize on the same synapses (neuron-neuron links) con-
sistently across multiple prompts, allowing for some basic features, the interpretation of the current in-context
state based on the reading of state of an individual synapse associated with such a feature (cf. Section 6.3).
A more detailed discussion of the training approach is provided in Appendix B.2, while the code listing for BDH-GPU
is provided in Appendix E. For the purposes of our experiments, we did not apply any specific training method which
would be known to guide the system towards any of the observed emergent properties. (In particular, L1-regularization
was disabled.) The observed emergent effects follow naturally from the design choices of the BDH and BDH-GPU
architectures, and are largely attributable to the combination of: the choice of model dimensions with comparable
model-to-state ratio, reliance on linear attention in high dimension, reliance on ReLU thresholds for ensuring that
activation vectors are positive (trivially) and sparse (an effect empirically noted in (Haziza et al., 2025)).
We also remark that the BDH-GPU architecture allows for the uniform asymptotic scaling of the model in one dimen-
sion,n. For example, a composition of models, obtained by concatenation, is also model in the same architecture, with
a larger value ofn(cf. Section 7.1 for an empirical study of this effect for practical translation tasks). Historically,
a link has been established between infinitely wide feedforward networks and Gaussian Processes (Neal, 2012; Lee
et al., 2017; Yang, 2019). BDH allows the study of limit behavior of reasoning models.
With BDH and BDH-GPU, we show that Language Models can be amenable to a particle-based
interpretation. In fact, two micro-foundations — particle-based behavior and logic-programming
behavior of a reasoning system — fuse together in these architectures.
The bridge between the Transformer and Brain models.The inference dynamics of BDH and BDH-GPU act as
a natural bridge between Transformer, and neuromorphic models of the brain and its subsystems. We illustrate this in
Fig. 1.
Implications for learning dynamics of natural lifelong inference systems.A lifelong learning system progresses
in time, performing extremely rapid inference, combined with several training mechanisms at different time scales.
In this work, we provide and validate at scale a plausible explanation of what thepredominantdynamics of such
a system could look like, taking the system from ‘split-second’ scale, to the scale of inference during ‘minutes’,
considering the flow of time at the natural rate of thought and language for humans.
A complementary discussion of learning dynamics would aim to provide an explanation of how to take such a lifelong
inference system from the scale of ‘minutes’ into even longer timescales. This would concern the slower transfer
of “fast-weights”-like inference state to long-term memory, starting at the order of10 3—10 4 tokens, and taking into
account feedback signals. In this work, we do not provide a direct answer as to how the brain actually handles this
effect at longer timescales. However, a constructive way to resolve this problem seems to be less challenging, once the
local inference dynamics of the brain are better understood (we come back to this in the Conclusions). The modeling
approach provided in Section 2.5 is proposed as a suitable framework for such a study.
8
Tensor-based models
centralized computing
Local Graph Models
distributed computing on graphs
Transformer BDH-GPU BDH Brain Models
general 
attention
neuron interaction  
graphs
G , G , G  x y s
Attention 
Mechanism
Local  
Dynamics
Feed-Forward
Network
KV-cache
MLP’s,
K Q V
W , W , W
state matrix ϱ in 
neuron dimension
decoder-encoder 
matrices E, D , D in x y 
neuron dimension
synaptic state 
plascticity  σ 
synaptic state 
plascticity 
neuron interaction
graphs
low dimension,
non-linear
(softmax)
high dimension, 
linear, in positive  
orphant
emergent  
network
(G , G )x y
local mean-ﬁeld 
dynamics(linear  algebra)
local graph 
dynamics +  
edge reweighting 
on Gs
spiking graph 
dynamics + 
Hebbian 
learning
ϱ = E σ
Figure 1: General overview of architectures and their relationships: the inference dynamics of BDH and BDH-GPU act as a natural
bridge between Transformer and models of the brain. The two main inference mechanisms of a reasoning architecture, attention
and the feed-forward network, are defined at a macro-level through tensor operations for the Transformer, and at the micro-level of
neuron interactions through local graph dynamics for Brain models. The new BDH-GPU architecture is naturally defined both at
the level of vectors and of particle dynamics of neurons and synapses, acting as a bridge between these two approaches. See also
Table 3 at the end of the paper for a more detailed comparison of architecture properties.
1.4 Notation
State-space models.For describing inference dynamics of any system, we will use state-space notation, and consider
a state-space system composed of two parts: a set ofmodel parametersMwhich does not change during inference, and
astateσ(t)which changes during inference. The model performs inference following state-space equationσ(t+1) :=
A(M, σ(t), at), wherea t is a possible external input to the system at timet(such as a language token),t= 0,1,2, . . .,
andAis referred to as thearchitectureAwhich drives its progress. During inference without external input, usually
autoregressive inference, we will shorten this toσ(t) :=A t(M, σ0).
Models as programs.In settings that are of interest to us (inference with combining multiple facts, reasoning), we
opt for terminology from computing.Mhas the interpretation of a computer program code,Ahas the interpretation of
a computational machine architecture which runs it, andσhas the interpretation of the variable state of the program.
We will use the terms ‘modelM’ and ‘programM’ interchangeably.
Graphs and their dynamical systems interpretation.For a square matrix with non-negative coefficients,H∈
(R+)n,n,n∈N, we will consider two more equivalent representations. In one, we will treatHas a graph defined on
some nodesetV, withV=|n|. Formally, we can takeV={e 1, . . . , en}, wheree i = (0, . . . ,0,1,0. . . ,0)∈R n×1
with1on thei-th position, forming an orthonormal basis. Non-zero entries ofHare referred to asedges. By an
overloading of notation, we will writeH(i, j) :=e j T He i ≥0, to represent the node affinity function, oredge weight,
fromitoj. We define theedge setE(H) :={(i, j)∈V×V:H(i, j)>0}.
In discussions of graph-based model architectures, we will take the standard interpretation of graphs from a linear
dynamical systems perspective, applied to positive vectors. Whenv∈(R +)n×1 is a non-negative vector,Hv∈(R +)n
has the interpretation of a linear transformation ofv. IfHsatisfies the condition of stochasticity (column-normalization
to1), thenv7→Hvis a Markov chain transition, with∥Hv∥ 1 =∥v∥ 1. From a distributed systems perspective,
transitions of stochastic matrices can be represented either through the direct simulation of (probabilities) of such
a Markov chain, or described by the token dynamics of an extremely simple stochastic token distribution scheme in
which a token located at nodeei goes to nodee j with probabilityH(i, j). IfHis not stochastic, the operationv7→Hv
9
additionally necessitates the suppression of a fraction of tokens, or the multiplication of tokens, at each step at each
node, depending on the column-normalization of a given node.6
For two graphsH 1, H2 ∈R n×n, the graphH=H 2H1 is obtained through (linear algebraic) matrix multiplication,
and in a distributed system, the corresponding transitionv7→Hvis obtained with two steps of token dynamics, one
following graphH 1, the next following graphH 2.
Representingmedge-weights of a sparsen-node graph withbbits of numerical precision per parameter is possible
withO(m(b+logn))bits of information, which corresponds toO(m(1+ logn
b ))parameters. For the sake of simplicity,
we will assume in asymptotics that the second term of the sum does not dominate (i.e.,logn=O(b)), and so we simply
say that we represent the graph withO(m)parameters.
2 BDH: a language model architecture given by local distributed graph dynamics
2.1 Formalism for local graph-based language models
We consider model architecturesAwhich correspond to models of graph-based distributed computing (cf. (Peleg,
2000; Hirvonen and Suomela, 2025)). A specific modelMin architectureAcorresponds to the weights and topology
of the communication graph or graphs used by such a system.
Introduction to distributed graph systems.The distributed system architectureA, representing the model archi-
tecture, is defined through ascheduler, and a local dynamics (kernelK(A)) describing the local computations to be
performed at each node of the system, and, communication between pairs of nodes connected by edges of the graph
representing a given modelM.
We will generally accept that computations are performed only atnneuron nodes (particles), whereas state variables
of the system may appear both on nodes and edges. We will, for simplicity of analysis, consider systems governed by
asynchronous scheduler, which in successive rounds, acts in two sub-rounds:
1.Computation:computations of the kernel ofAare run at all neuron nodes independently.
2.Communication “over wire”: each neuron node sends specified ‘output variables’ to specified ‘input vari-
ables’ of its neighboring neurons.
We expect the scheduler to follow the same communication pattern between neurons over time in a uniform way. In
order to avoid artificial constructions of cyclic time-counters at nodes, we will define the architecture kernel through a
short sequence of kernels, with the scheduler executing them in successive rounds in round-robin manner. Specifically,
whenAis BDH, we will have a sequence of four kernels,K(A) = (K 1(A), K2(A), K3(A), K4(A)), withK i(A)
being executed in every roundrsuch thatr≡imod4.
Programmable rulesets and the interaction kernel.We recall from Section 1.4 that a model architectureAhas
the interpretation of a computational machine architecture, and modelsMhave the interpretation of programs in
architectureA. We also recall that a graph-based modelMis defined through a set of parameters which represent the
topology and weights of the communication graph of the system.
The above considerations lead directly to the following observation:The graph of the communication network, which
is used for communication between sites by the distributed system architectureAduring reasoning and language infer-
ence, has the interpretation of a (trainable, rule-based) program.Consequently, we embed the subsequent definition
of BDH in a kernel formalism, given through a form ofprogrammable rulesets, using two-particle interaction rules on
a graph.7
The rulesets which we will use to define BDH will closely resemble rulesets (protocols) known from evolutionary and
population dynamics (Hofbauer and Sigmund, 1998; Angluin et al., 2006; Aspnes and Ruppert, 2009) and chemical
reaction networks (Chen et al., 2014; Feinberg, 2019), however, they will be restricted to a special class of interactions.
We start by presenting the more general form of thisinteraction kernel. We then explain how such a kernel can be
restricted, allowing it to be naturally implemented using a local graph-based distributed system (in particular, one
6We provide a graph distributed systems interpretation only for dynamics on graphs with non-negative matrix entries (positive-
weight edges). Negative-weight edges are hard to represent using natural local dynamics based on token distribution or spiking
models.
7We refer the reader to Appendix D for a more principled background discussion, guiding the appropriate choice of formalism
for rule-based local interaction.
10
relying spiking dynamics), while remaining sufficiently expressive to describe an attention-based language model.
The resulting restriction will be called theedge-reweighting kernel.
Definition 1(Interaction kernel, general form).A system withzspecies,z∈N, and state(q 1, . . . , qz)∈Q,q i ∈R +,
performs theinteraction kernel with a ruleset (protocol)Pgiven by a set of transition rates calledrule weights,
P= ((r ijk ∈R +)i,j,k∈{1...,z},(d k ∈R +)k∈{1...,z}), producing the following transition from a state(q1, . . . , qz)∈Q
to a state(q ′
1, . . . , q′
z)∈Q:
q′
k := (1−d k)qk +
X
i,j
rijk qiqj (3)
We will describe such a rulesetPusing the notational form:
P= ({“q i, qj
rijk
− − →qk”}i,j,k∈{1...,z},{“q k ↓dk”}k∈{1...,z}).
As a matter of convention, omitted rules correspond tor ijk = 0(respectively,d k = 0), while rules with no rate value
stated next the pointer correspond tor ijk = 1(respectively,d k = 1). Ifq j is omitted from notation on the left-hand
side, we assumeq j = 1.
Equation (3) captures the dynamics of the following differential equation: dqk
dt =−d kqk +P
i,j rijk qiqj. Assuming
qi, qj, rijk ∈[0,1], the expressionr ijk qiqj has the interpretation of a population dynamics or chemical process of
the form “iandjgivek”, with this processes happening at rater ijk, assumingq i, qj, qk have the interpretation of
concentrations of speciesi, j, k. The formalism we use here assumes non-normalized state variables.
We will subsequently use a restriction of the interaction kernel to graph-based systems, which we call theedge-
reweighting kernel, to describe BDH.
Restricting the interaction kernel to spiking signals and graph systems.First, we observe that rules of the form
used in the interaction kernel from Definition 1 are extremely easy to implement in systems which rely on stochastic
0/1-valued signals. Whenˆqi andˆqj are independent random variables in{0,1}, withPr[ˆq i = 1] =q i andPr[ˆqj =
1] =q j, thenq i, qj − →qk is expressible as the “AND gate” of probability: the random variableδˆqk :=q iqj ∈ {0,1}
gives the same expected contributionEδˆqk =q iqj as the considered rule.
We now consider the restriction of interaction kernels to the case of graph systems. In the general formalism,kcan be
arbitrary with respect toiandj. By contrast, consider graph systems, which describe binary relations between nodes,
and not (directly) three-point relations. To resolve this, we will require thati,j, andkhave the interpretation of two
nodes of a graph and an edge which is incident to them.
For an anchoring in the literature of dynamical systems, we note that already systems following an interaction kernel
with a strongly constrainedkof the formk∈ {i, j}, exhibit powerful nonlinearities: with such a restriction onk,
Equation (3) describes the class of evolutionary systems following the equations ofreplicator dynamics(Hofbauer
and Sigmund, 1998), also equivalently known as a non-normalized form of the fundamental Lotka-V olterra predator-
prey dynamics. Replicator dynamics can naturally be represented as graph systems whose parameters are defined on
on edges of the graph, but whose state is updated onon nodes of the graph. By contrast, when defining dynamics
for reasoning in the current work, we will also need to capture a more powerful class of graph-based systems, where,
crucially, state is larger than the number of neuron nodes, appearing on neuron-neuron edges (synapses).
We are now ready to describe a restriction of the interaction kernel from Definition 1 to the case of node-edge-node
interaction rulesets in a graph: theedge-reweighting kernel.
Definition of the edge-reweighting kernel.We consider a graph system withnnodes, indexedV={1, . . . , n}.
Additionally, a subsetEof pairs of indexes(i, j), fori, j∈ {1, . . . , n}forms theedgesof the system.
The system has state variables associated (uniformly) with nodes and edges, which we denote with capital letters, e.g.,
X(i), fori∈VorZ(i, j), for(i, j)∈E.
Definition 2(edge-reweighting kernel).A distributed system follows theedge-reweighting kernelif its dynamics are
given by the interaction kernel (Definition 1) with a set of non-negative state variables, defined on the set of nodesV
and set of edgesEof a graph, such that each local rule with non-zero rate is either acomputational ruleinvolving
only state variables on a single nodei∈V, or acommunication rulefor an edge(i, j)∈E, involving state variables
from the nodesi, jand edge(i, j).
For context, we remark that, in comparison to the strictly simpler dynamics of node-reweighting governed by graph-
based replicator dynamics equations, dynamical systems based on the edge-reweighting kernel given by Definition 2
are rather elusive to study. We credit the seminal work of Algorithms theory (Christiano et al., 2011)[Fig. 1, Thm 3.2]
11
as the first rigorous study of local edge-reweighting graph dynamics, combining fast-paced linear kernels on nodes with
a slower-paced edge-reweighting process, in order to refine (‘focus’) electrical flows on graphs towards a sharper form
of cost optimality.8 The BDH dynamics that we will introduce here rely on fundamentally different nonlinearities in
the process, and will have the interpretation of guiding the system from premises defined at a subset of nodes, towards
search targets at nodes representing a desired outcome, through reasoning inference rules with tunable weights set on
edges.
In the following Subsection, we will use the introduced formalism to define BDH as an edge-reweighting kernel on
the union of edges of several graphs (Gx
e, Gx
i, Gy
e, Gy
i, Gs) with the same set ofnnodes.
2.2 Definition of BDH as a local edge-reweighting process (equations of reasoning)
Bearing in mind the discussion of graph dynamics suitable for the case of language inference, and specifically the
definition of the edge-reweighting kernel (Definition 2), we are now ready to formalize the state-space dynamics of
Equation (6) as a local graph dynamics.
Definition 3.The BDH model withnneurons, with parameters expressed through graphsG x
e, Gx
i, Gy
e, Gy
i, Gs is
represented as the ruleset of the edge-reweighting kernel, withO(n+|E(G s)|)state variables, with rule amplitudes
given by “the equations of reasoning” in Table 1.
Inference dynamics of BDH.The BDH dynamics rely on rapid pulse dynamics with state variablesX(i),Y(i),
A(i), defined on thenneuron sites of the system, and fast-weight-like state variablesσ(i, j), defined on a subset of
edges of the system,(i, j)∈E(G s). The full implementation of BDH shown in Table 1(b) also includes auxiliary
state variablesX e(i),X i(i),Y e(i),Y i(i)which are used as temporary counters, for integration of excitatory and
inhibitory signals received by neurons. The dynamics also rely on a set of damping hyperparameters on state,u >0,
which may in full generality be defined separately asu(i, j)for each edge(i, j)∈E(G s).
Inference with BDH is performed as follows. For some parameterL(e.g.L= 8in most of this paper), which
would correspond to the number of layers in a Transformer-like system, the system scheduler proceeds through rules
in round-robin manner, ingesting new tokens every4Lrounds and retrieving results4Lrounds later. During round
4l+k, for0≤l < L, the system performs rules from thek-th column of Table 1, with each such round consisting of
a communication step on edges and a local computation step on nodes.
The state-space dynamics of BDH can be rewritten in vector-tensor form, equivalent to the local dynamics of the
interaction kernel given in Table 1. This representation is given by Equation (6) in the following Section.
Observation 1.The BDH-Graph protocol for the interaction kernel, given for any time roundT= 4Lt+ (4l+k),
0≤l < L,k={0,1,2,3}by the ruleset in Table 1 is equivalent to the state-space dynamics over timetand layers
l, given by Equation(6).
Proof.For completeness, a detailed explanation of the equivalence is provided in Appendix C.1.
The variablesX(i),Y(i),A(i), defined for each of thennodes of the system, are updated in successive rounds. The
state variablesσdefined on edges are assumed to be distinct overlasσ l, for0≤l < L; this distinction serves to
facilitate interpretation and to strike a balance between the number of parameters and the size of state of the system
(assuming a single state matrixσ, uniform acrossl, does not fundamentally change the operation and scaling laws of
the architecture).
In the representation in Table 1 we do not impose how the local thresholding operation within some neuroni, of the
formA(i), B(i)99K(A(i)−B(i)) +, should be performed. We leave this as a computational primitive, which can be
realized based on approximate counting or a comparator. The way natural neurons achieve thresholding to determine
whether input signal excitation outweighs inhibition relies on time-integration of impulses. For realizations in other
types of distributed systems and population protocols, we refer the reader to the literature on thresholding and Majority
Protocols, cf. e.g. (Doty et al., 2021; Czyzowicz et al., 2022).
The definition of the protocol does not specify how variableX(i)should be reset when the scheduler passes from layer
Lof one input token to layer0for the next input token. As with the definition of state-space equations in Section 3,
we leave this open to allow the dynamics to work both with externally provided input (for next-token prediction), or
in a self-feedback loop (for autoregressive operation).
8The graph dynamics used in this approach are naturally phrased in distributed computing parlance, see (Becchetti et al., 2018;
Zou, 2019).
12
(a) Simple equations of reasoning
Round4l Round4l+ 1 Round4l+ 2 Round4l+ 3
Inference from state Reweighting of synapse state Neuron replicator dynamics +
inference from parameters
Inference from parameters
X(i), σ l(i, j)− →A(j)
σl(i, j)↓ 1−u(i,j)
Y(i), X(j)
Gs(i,j)
− − − − →σl(i, j)
Y(i)↓
A(i), X(j)
Ge
y (i,j)
− − − − − →Y(j)
A(i)↓
Y(i)
Ge
x(i,j)
− − − − − →X(j)
(b) Complete equations of reasoning of BDH
Round4l Round4l+ 1 Round4l+ 2 Round4l+ 3
Communication
X(i), σ l(i, j)− →A(j) Y(i), X(j)
Gs(i,j)
− − − − →σl(i, j) A(i)
Ge
y (i,j)
− − − − − →Ye(j)
A(i)
Gi
y (i,j)
− − − − − →Yi(j)
Y(i)
Ge
x(i,j)
− − − − − →Xe(j)
Y(i)
Gi
x(i,j)
− − − − − →Xi(j)
Computation
σl(i, j)↓ 1−u(i,j)
X e(i)↓
X i(i)↓
Y(i)↓
Y e(i)↓
Y i(i)↓
(Y e(i)−Y i(i))+, X(i)− →Y(i)
A(i)↓
(X e(i)−X i(i))+ − →X(i)
Table 1: The “equations of reasoning”: State-space dynamics of the BDH language model expressed through local graph dy-
namics with the edge reweighting kernel (Definition 2). The rules are executed for a distributed system ofnneurons perform-
ing steps of parallel computation and communication during inference. Model parameters are expressed through the weights
of edges of graphsG x
e, Gx
i, Gy
e, Gx
i, Gs, and BDH model training is equivalent to defining rule probability amplitudes
Gx
e(i, j), Gx
i(i, j), Gy
e(i, j), Gy
i(i, j), Gs(i, j)≥0for pairs of neuronsi, j∈ {1, . . . , n}connected by the edges of these
graphs. State is encoded in variablesσ(i, j)at synapses, representing edges of graphG s. The system proceeds in parallel rounds,
with new tokens arriving into the system encoded through variablesX(i)at neurons and introduced every4Lrounds, whereLis
a parameter of the model (e.g.,L= 8). The set of rules being executed (for each round modulo4L) is given in the table. The
readout of the system also happens through variablesX(i)at the end of each4Lrounds. (a) Set of rules for the simplified version
of the BDH model with no neuron inhibitory circuits and no thresholding (G x
i =G y
i = 0), capturing the general form of the
communication structure and synaptic attention of the model. (b) Set of rules for the general case of BDH, including inhibitory
circuitsG x
i,G y
i. An execution of the provided rules is equivalent to the state-space dynamics given by Equation (6).
13
Notes on training.Direct training of the BDH model would be performed by selecting the edges of the considered
graphs, and then setting rule weightsG x
e(i, j), Gx
i(i, j), Gy
e(i, j), Gy
i(i, j), Gs(i, j)≥0for pairs of neuronsi, j∈
{1, . . . , n}connected by the edges of these graphs.
In what follows, we will train a tensor-friendly special case of BDH, called BDH-GPU, relying on an implicit (gener-
ally more efficient) representation of the considered graph parameter weights, using a low-rank product representation
for the matrices of these graphs. This representation is reminiscent of the hub-labeling graph representation technique,
but is directly suitable for describing and evolving high-conductance scale-free networks. The appropriate architecture
is introduced in Section 3.
2.3 Interpretation of attention as a micro-inductive bias of reasoning
Rule weights in the edge-reweighting kernel have the interpretation of micro-programs, governed by rules of transfor-
mation of state variables of the formA(i), B(j)→σ(i, j)andA(i), σ(i, j)→C(j), defined on edges between nodes
i, jof somen-node graph.
This formalism can be seen as running an enormous circuit with a form of universal gates given by the transition rules,
over a structure of computational elements at nodes, and memory elements on edges of a graph.
While the local rulesets have the form of a rule-based micro-assembly, we leave open the extent to which they should
be considered to have an interpretation of programming in logic (as would be the case, e.g., for C-RASP (Yang and
Chiang, 2024)). The natural interpretation ofσ(i, j)>0is a positive bias associated with the neuron pair(i, j),
i, j∈ {1, . . . , n}, which follows from past context. This can be considered by phrasing the local rules of the system
in a framework of logic inference; we do so informally, omitting discussion of layers.
If past context(x τ :τ < t)implies that implicationi→jhas weightσ t−1(i, j), and if the current
state at timetimplies thatifollows from this state with weightx t(i), then the current state at timet
implies thatjfollows from this state with weightx t(i)σt−1(i, j).
The above is intentionally phrased to resemble the logical axiom(X→(i→j))→((X→i)→(X→j)), which
is perhaps most prevalent across different formalizations of axiomatic logic, with an application ofmodus ponensas an
inference rule. The inference system of the considered model uses state and model weights to devise its own heuristic
for the order of evaluation, i.e., to consider which facts appear to be most plausible to be evaluated next, and to evaluate
them in an order based on what follows most strongly from context. In a way consistent with what we expect from
informal reasoning in language, the considered weights have a more direct interpretation of an increment of utility
associated with a given inference. 9 In the setting of argumentation, this utility-based approach could, for example,
guide the inference process from a pair of known concepts in context, a source and a target, to an intermediate concept
likely to be a common-neighbor shortcut lying on a logical path between this source and target (cf. Section 5.3 for a
discussion of how this type of mechanism is enforced in the feed-forward network of BDH-GPU).
The considered micro-foundational interpretation of attention, defined at the level of individual neurons (or logical
variables), does not contradict the way in which Transformer attention is often regarded at the coarser level of vectors
through key-query lookup intuitions. At the same time, it highlights that an attention state entryσ(i, j)(and similarly,
a model edge weight leading fromitoj) does not have the interpretation of a logical value (i.e., something that is true
or false), but an inductive bias associated with how likely the system is to consider the implication ‘i→j’ in its next
steps of reasoning, when proposing its next conclusions or next ideas for consideration.
Chains of implications in BDH guide activations along paths in the system graphsGx
e, Gy
e,σ. For the latter, attention
allows specific implications to enter into paths of thought once the corresponding synapses are open in stateσ.
2.4 Interpretation of BDH as an oscillator network toy-model
Whereas the interpretation from Subsection 2.3 focuses on properties which fallow from the computational function
(purpose) of the system, here we outline an interpretation of the behavior of BDH considered purely as a dynamical
system.
9Here, the termutilityis understood in the sense of evolutionary game theory, as applied to the population of neurons, consid-
ering the standard interpretation of replicator dynamics, as applied in the ruleset from Table 1. Neurons which win in the natural
selection process are added to the activationY.
14
Gs
Gx
Gy
xi
yj
σij
Symbol Interpretation in:
Table 1,
State Equation (6)
Oscillator Network
Toy-Model
Gx, Gy, Gs graph parameters of
model
wires, prods, and
elastic connections
σsynaptic state of model displacement of elastic
connections
x, yactivation vectors pulses at nodes, state
correction
Figure 2: The ‘physical system’ representation of BDH as a physical graph toy-model.
Definition of the toy-model.We will consider the toy-model of ann-particle system shown in Fig. 2 as an illustration
of the general form of dynamics of the state-space equation (6) of BDH. We draw thenparticles in a circle. 10
The particles are connected with each other by state elements, represented in Fig. 2 as elastic connectors. The topology
of these pairwise connections is given by graphG s, and may in general be dense.
The signal displays dynamics of stateρthrough tension on connectors, which evolves at a slower time scale, and a
more pulse-like activation dynamicsx, y(on nodes), appearing and vanishing regularly, at a rapid time scale.
The slower state dynamics represent, in the first order, oscillation or relaxation of the system of elastic connectors.
Once an elastic connector between particlesiandjhas had its endpoints displaced through statexandy, respectively,
a tension appears on this connector, which causes its displacementσ(i, j)that relaxes over time (damping variant,
corresponding to ALiBi), and/or acts as a spring element (oscillator variant, a simplified illustration of RoPE). Initially,
σ(i, j) = 0.
The faster dynamics represent the node dynamics of particles. Over time, pulse displacementsx(i)happen at nodes,
as a result of either previous behavior of the system, or perturbation by an external forcing field (in reality this field
would be language input). A nodeiwith displacementx(i)may, due to the aggregated action of tension ofelastic
connectorsσ(i,·)adjacent to it, activate a system ofprodsG y adjacent to it, perturbing nodes it hits in this way. If
another nodejis prodded sufficiently hard, it may cause it to activate a perturbationy(j). The perturbationy(j)of a
nodejwill, in the next step, propagate again to those other nodesi ′, which are connected tojby a system ofwires
(Gx). If the aggregated pull of wires on a nodei ′ is sufficiently strong, this modifies its pulse displacementx(i ′). The
pulse activationy(j ′)of some nodej ′, directly followed by pulse activationx(i ′)of nodei ′, results in an increase in
the tension on the connector(i, j), adding to the value of the tensionσ(i ′, j′). All pulse activationsysubside, and the
pulses propagate, consequently altering the slow stateσ.
In general,σ(i ′, j′)is triggered simply by the temporal connection between the pulsey(j ′)activating, followed by the
pulsex(i ′)activating immediately afterwards, even if there was no direct causality between the two (althoughy(j ′)
contributed to pulsex(i ′)happening if(j ′, i′)∈G x). An appropriate correspondence of the graphs,G s ⊆G x, would
bring the system close to an observed causal effect on the activated synapse.
The above description of the pulse dynamics was given from the perspective of nodes. From the perspective of
connectors, an existing tension on some connectorσ(i, k)propagates through prodsG y to some nodesj, then through
wiresG x to some nodesi ′, and this finally contributes to tensions on other connectorsσ(i ′, j′). This propagation of
state thus happens to 3-hop neighbors, throughi,j,i ′.
During training, the behavior of the system may, in even longer time scales, result in the propagation of changes of
connection weight and structures to graphsG x andG y, as well as (optionally)G s.
10This is a direct tribute to the Kuromato coupled oscillators model; the crucial difference being that in BDH, the elements of
state with an interpretation similar to oscillators appear on connections between nodes, not nodes.
15
Effects captured by the toy-model.We have described a small local graph kernel, with 3-hop locality, capturing
the two key effects of the local graph kernel.
The first effect is the graph form of communication pattern between nodes, and thresholding of updates. (We have
omitted direct mention of inhibition from discussion of the toy-model, but it is direct to include.)
The second effect is the placement of attention state on node connections, its update patterns, and the dynamics of its
relaxation over time.
We intentionally convey the interpretation of node pulses as a differential (gradient) of state on node connections.
This interpretation is consistent with our empirical study from Section 7. It is worth considering once every how
many steps of the operation of the toy-model, a single element of stateσ(i, j)is updated. This depends directly on
the sparsity of the pulse signalsy(i),x(j); at least one of them is, generally, sparse. If the pulses where to happen
very seldom for such a pair(i, j), state updates are essentially a “second-order” correction effect. By adjusting the
frequency of updates, the system can be made to operate exactly at the critical point where this pulse dynamics ceases
to be a second-order correction of stateσ(i, j), giving the random variable describing the time between updates of
a connection pairσ(i, j)a heavy power-law-like tail distribution (possibly with different distribution parameters for
different pairs(i, j)).
In the description of state dynamics, we noted the hop-distance of 3 in the forward propagation of changes to state.
Bearing this in mind is helpful when considering how a gradient backpropagation mechanism would follow depen-
dencies between changes of state if such a system were to have its graph weights altered through backpropagation.
Finally, let us clarify the specific choice of kernel we made for BDH. We found it to work well, and we knew how to
train BDH models which implement it on GPU (which we will call BDH-GPU). This, with current hardware, made
it10 2 −10 5 times more cost- and time-effective to train models and analyze outcomes than kernels, for which we
only knew how to train on CPU. Nonetheless, the question of finding optimal kernels according to different criteria
(e.g.: minimality of kernel, best training rate per token, closeness to brain function based on known evidence from
brain studies), is an extremely pertinent foundational problem. The problem can be phrased in a “closed-ended” way,
leaving a finite number of possibilities to be checked, at least when considering small graph kernels. Some kernels
may also prove to have superior learning capabilities to the Transformer (and BDH), and if this quality difference is
overwhelming, they may eventually prove commercially viable.
In the following, we formalize the choice of kernel for BDH, and also provide a framework to describe other kernels
capturing the same effects of graph communication and synaptic attention.
2.5 Expressing BDH using brain models
The results we obtain for BDH provide direct corollaries on the expressiveness of brain models which are capable of
emulating the local graph kernels of BDH. Specifically, a distributed system, which is able to efficiently emulate the
local kernels of BDH, has sufficient expressiveness to perform language inference and reasoning at least to the same
extent as BDH.
Observation 2.The local ruleset of BDH (Table 1) can be expressed through a combination of simple mechanisms:
neuron activation with positive state variables, Hebbian learning, and communication through excitatory and in-
hibitory circuits with thresholding.
We note that in the description of the rulesets in Table 1, Round (4l+ 2) and (4l+ 3) directly describe the use
of excitatory and inhibitory circuits with integrate-and-fire thresholding at neurons. Round (4l+ 2) additionally
includes a form of competition effect between neurons, realized fully locally at a neurons using the multiplication
effect of replicator dynamics. The communication rule of Round (4l+ 1) involves the potentiation of a synapse
based on activations of neurons at its endpoints. As was discussed in Subsection 2.1, the natural mechanism for
implementing increase in synaptic strength is through spiking dynamics, where the execution of the communication
rule of Round (4l+ 1) is a stochastic AND-gate on signals. Finally, Round (4l) describes the long-term effects of
using a strengthened synapse for transmission of signals, and its strength decrease.
We can use the framework of expressiveness, as captured in Observation 2, to shed light on the capabilities of natural
systems through their ability to emulate artificial ones. Specifically, if a natural system A can plausibly emulate some
artificial system B by using the resources it has at its disposal, and artificial system B is able to solve a problem P, this
can be used to explain: (1) why the natural system A is sufficiently powerful to solve problem P, and (2) plausibly, that
the purpose for which system A is equipped with certain mechanisms includes solving problem P, if such mechanisms
prove useful in the emulation of B.
16
The experimental validation of the performance of BDH architecture at Transformer level (Section 4.2) confirms that
BDH is sufficient to provide language and reasoning function at scale. We can thus make the following statement.
Empirical Finding 2.TheHebbian learning mechanismis plausibly needed, and in combination with neural circuits,
sufficient, for performing thereasoningfunction at the scale of the brain. This includes performing language function
with attention, and performing thought processes, at a time scale of minutes.
In view of our results, Hebbian learning can be seen as a form of unsupervised learning over time, expressed through
graph edge reweighting, to perform reasoning and language inference using the attention mechanism. This type of
result can be compared to an analogous interpretation for Hebbian learning in the context of vision, as pioneered
in (Brunel, 1996). With the setting of language and chain-of-thought reasoning, we are able to directly capture effects
of time in the brain.
Given the interpretation of neuron activations as carrying the necessary gradients of synaptic state (Section 2.4), the
problem of supervised learning (i.e., taking into account feedback signals) plausibly becomes deferred to a selective
transfer and re-encoding of gradients from state into weights, at longer time scales. We return to a discussion of this
point in the Conclusions, bearing in mind the fact that the general difficulty of the problem is now reduced through
restrictions on the considered edge-reweighting kernel, and the relative rarity of synapse activation events.
Our work also suggests a framework for further discussion of reasoning function, with an anchoring point for this
type of investigation in the time-scale of ‘split-seconds’ to ‘minutes’. The question of shorter time scales is then one
of designing more precise communication and computational primitives for spiking neurons and synaptic plasticity,
which can be used to perform primitives for individual rules of graph kernels for the inference dynamics. 11 The
question of longer time scales, and the changes to model structure that follow in a learning process, naturally follows
any explanation of unsupervised (Hebbian) learning from the shorter time scale that is considered here, as a mechanism
of transfer from state to weights; we come back to this point in the Conclusions.
3 BDH-GPU: a tensor-friendly version of the BDH architecture
We will now introduce BDH-GPU, a variant of the BDH reasoning system, expressed in the language of tensor opera-
tions typical for Deep Learning models. BDH-GPU provides a GPU-compatible implementation of BDH. BDH-GPU
can be easily implemented in PyTorch, a didactic code listing is provided in Appendix E). Furthermore, BDH-GPU
can be trained on large text datasets using error backpropagation, and has been shown experimentally to match perfor-
mance of GPT-based LLMs.
The main steps towards the efficient implementation of BDH-GPU on GPU are:
1. Express graphsG x andG y a low-rank factorizations of their transition matrices, followed by ReLU non-
linearities (Nair and Hinton, 2010) (we explore graph properties of these approximations in Section 5). We
never materialize these matrices, but maintain instead a low dimensional state per each neuron.
2. Never materialize theσstate matrix, preferring instead to access it using a linear attention operation over
low-rank representation of values (we explore the properties of this attention mechanism in Section 6).
3. Normalize all state variables using LayerNorm (Ba et al., 2016b).
We will refer to the architecture in the final intermediate step, before the introduction of LayerNorm, as BDH-
Normfree.
3.1 Notation for BDH-GPU
We consider the BDH-GPU(n, d)architecture parameterized by positive integersn, d. The system scales in dimension
n— the number of particles. In what follows, we will use the termsparticleandneuroninterchangeably. Dimension
dis a measure of the number of parameters per neuron required to represent the interaction of this neuron with the
particle interaction field or interaction graph. For asymptotic analysis, we assume thatn→+∞is the basis for all
asymptotics, andn≫d > Clognholds for some sufficiently large constantC >0. For the tensor representation
of the model, which is the primary one for implementation and empirical studies here in this paper, vectors inR d
have an interpretation as (fuzzy) addresses of a virtual memory space of sizen, hence the assumptiond= Ω(logn)
cannot be dispensed with while using natural (linear-algebraic) arithmetic on real numbers. We later show how to
11While we do not provide direct explanations for effects at shorter time scales and scheduler primitives, we note the type of
kernels we rely on are well understood in terms of the ability to work with asynchronous schedulers, and obtaining emergence of
synchronization. (Kosowski and Uzna´nski, 2018; Dudek and Kosowski, 2018)
17
avoid this assumption in graph-based models, by using uniform local graph kernels of smaller degree with a graph
communication structure.
Nonlinearities: ReLU and LayerNorm.In what follows, we assume that a one-dimensional vector is denoted by
a lower-case letter, e.g.,z, withz∈R n×1 ∼= Rn unless otherwise stated. Vectors which appear in dimensiondare
named with an asterisk, e.g., asz ∗ ∈R d×1. We denote theReLU operation(z) + := maxi∈1,...,n{0, zi}.
We further defineLayerNormof a vectorz ∗ ∈R d×1 in a uniform non-parametric way,LN(z ∗) = z∗−1Edz∗
σdz∗ , where
Ed andσ d are estimators of mean and standard deviation in dimensiond, respectively.
Activation vectors and parameter matrices.In vectors representing activations, each scalar element (element of
R) of the activation vector has the interpretation of a ‘scalar’ activation state of a single particle. Throughout this text,
Ris generally assumed be the field of real numbersR:=R, and scalars are represented by a fixed-precision floating
point number in experiments.12
By convention, in discussions of parameters, matrices denotedG x, Gy, Gs ∈R n×n will represent neuron-neuron
interaction, encodersE∈R d×n reduce dimensionality of activation vectors (e.g.,a ∗ =Ezforz∈R n), and
decodersD∈R n×d lift them back intoR n (e.g.,z ′ =Da ∗).
Depending on the architecture variant considered, the state will either have the interpretation of a neuron-neuron
correlation matrixσ∈R n×n, or a compressed form with reduced dimensionality,ρ=Eσ∈R n×d.
3.2 Definition of BDH-GPU as a state-space system
We now define the main architecture of this paper in its tensor flavor, called BDH-GPU.
Definition 4(inference dynamics of BDH-GPU).A BDH-GPU state-space system BDH-GPU(n, d), given by three
parameter matrices:E∈R d×n andD x, Dy ∈R n×d, performs iteration over timet= 0,1,2. . .and layersl=
1,2. . . L, governed for any timetby the following recurrence:
xt,l :=x t,l−1 +
 
Dxv∗
t,l−1
+
a∗
t,l :=
X
τ <t
v∗
τ,l−1xτ,l
T U t−τ xt,l
yt,l :=
 
DyLN
 
a∗
t,l
+
⊙x t,l
v∗
t,l :=LN(Ey t,l)
(4)
where inputs to the system are provided through the boundary conditionv ∗
τ,0 in layer0, forτ= 0,1,2. . . t.
Here,U∈R n×n is a diagonal or block-diagonal matrix representing local rotation or damping of state (such as
ALiBi or RoPE),L∈Nis the number of layers.
BDH-GPU as a language model.BDH-GPU is intended to be used as a language model, processing one token
per time step, in which case the inputv ∗
t,0, fort∈N, is obtained using some (linear) encoding function from the
token alphabetΩ,f e : Ω→R d, as applied to thet-th input tokens. Similarly, the logits of thet-th output token
are extracted using some decoding function applied to outputs of theL-th layerv ∗
t,L, using a (linear) token decoder
functionf d :R d →Ω. The source of language tokens may be external, as is the case for next token prediction tasks,
or auto-regressive.
For training, we assume that a modelMtrained in the BDH-GPU(n, d)architecture has the trainable parameter set
M= (E, D x, Dy, fe, fd), with all parameters trained together. The model has3nd+2Ωd= (3+o(1))ndparameters,
i.e., the scalable part of the model is concentrated in the total of3ndparameters of the matrices(E, D x, Dy).
State-space representation.The notation of Definition 4 is chosen so as to exhibit its direct applicability in a
Transformer-like token-parallel training framework. Vectorv ∗
τ,l−1 has the interpretation of attention ‘value’ inputs at
timeτin layerl. Vectora ∗
t,l represents the result of a linear attention mechanism for timetin layerl.
12When only asymptotic analysis is the object, it is sometimes convenient to considerR:=R k for somek= 2,3, . . .. Specif-
ically, consideringR:=R 2 allowsSO(2)rotations onR 2 to be expressed as ‘scalar’ ones onR, thus making theR 2n×2n RoPE
block-diagonal matrix of a diagonal matrix inR n×n (Su et al., 2024). This provides a consistent formalism for ALiBi (Press et al.,
2022), RoPE, and extensions such as LieRE (Ostmeier et al., 2025) as diagonal (communication-free) operations. In all cases, the
application of ReLU(·) + to a scalar remains coordinate-wise inR.
18
Denoting in (4) the model’s attention state as
ρt−1,l =
X
τ <t
v∗
τ,l−1xτ,l
T U t−τ (5)
we obtain the equivalent but more compact form of representing the inference dynamics of BDH-GPU as a state-space
model, presented in Fig. 3, Eq. (8).
BDH
σt,l :=
 
σt−1,l +
  
yt,l−1xt,l
T
⊙G s

U
xt,l :=x t,l−1 +

Gx
e −G x
i

yt,l−1
+
yt,l :=

Gy
e −G y
i

σt−1,lxt,l
+
⊙x t,l
(6)
BDH-GPU
ρt,l :=
 
ρt−1,l +LN(Ey t,l−1)x t,l
T
U
xt,l :=x t,l−1 + (DxLN(Ey t,l−1))+
yt,l :=
 
DyLN
 
ρt−1,lxt,l
+
⊙x t,l
(8)
↘ ↗
BDH-Normfree
σt,l :=
 
σt−1,l +y t,l−1xt,l
T
U
ρt,l :=
 
ρt−1,l + (Eyt,l−1)xt,l
T
U
)
alternative
representations
xt,l :=x t,l−1 + (DxEyt,l−1)+
yt,l := (Dy Eσ t−1,l| {z }
ρt−1,l
xt,l)+ ⊙x t,l
(7)
Figure 3: State-space equations of the model architectures introduced in this paper. All architectures refer to a set ofninteracting
particles (neurons), with activation vectorsx t,l ∈(R +)n. Vectory t,l ∈(R +)n,y t,l is (typically) sparse in the sense of∥y t,l∥0.
Variablesρ t,l ∈R n×d orσ t,l ∈R n×n represent hidden state of the system.⋄The graph-based BDH dynamics equation (6),
equivalent to the ruleset from Table 1, serves as a starting point for development of architectures represented as local graph kernels in
a distributed computing system.⋄The simplified BDH-Normfree equation (7) is a special case of BDH. Up to lack of LayerNorms,
it approximates the inference dynamics of BDH-GPU, with the correspondenceρ t,l =Eσ t,l.⋄The tensor-based BDH-GPU
architecture is described by equations (8) (mathematically equivalent to Definition 4, Eq. (4) and (5)) and is the primary point of
reference for all model training and all empirical results presented in this study. For a discussion of extensions to BDH-GPU such
as heads, see Subsection 4.1. A complete code listing for BDH-GPU is provided in Appendix E.
d d
n
hkϱL
ϱ1
T
Dy
T
Dx
E
Figure 4: Scaling of BDH-GPU architecture in dimensionn. The other parameters can be considered fixed during scaling. For
example, with choice ofd= 256for low-rank dimension,k= 2for neuron pairing with RoPE, andh= 1for a single-head
architecture, the model scales linearly in dimensionnin chunks ofdhk= 256·2·1 = 512parameters.
In what follows, we will perform analysis focusing on the state-space representation of the architecture given by
Eq. (8).
19
3.3 Interpretation of BDH-GPU as a local interacting particle system
The BDH-GPU dynamics equation (8) has the interpretation of an-particle system, with the stateρ t(i)of thei-th
particle,i= 1, . . . , n, given at the end of timetby a vector inR d for each layer:
ρi(t) := (ρt,l(i,·) :l∈(1, . . . L)).
Overall, as we will see directly, the way particleiinteracts with other particles at timetis described by the following
tupleZ i:
Zi(t) := (ρi(t), E(i,·), Dx (·,i), Dy (·,i)).
Here,ρ i(t)represents the in-context state associated with particlei(initialized as0at the start of inference), while the
other three vectors of lengthdassociated with this particle are trainable, but do not change during inference.
The system scales in dimensionnand is completely uniform in this dimension, excepting following effect. Letk
denote the size of largest block in the block-diagonal matrixU; then particles, are bound by this effect into non-
uniformk-tuples whenk >1(k= 1whenUis the ALiBi matrix, andk= 2whenUis the RoPE matrix). Thus the
system, in general, scales in the dimension ofnuniformly, in chunks ofkparticles (see Fig. 4).
The interaction between particles is, intuitively, local. To be able to proceed with discussion with rigor and without
complicating notation, we assume for the analysis thatk= 1. We also drop LayerNorms from the equations of
inference dynamics. (Models generally do not train following BDH-GPU without any LayerNorm, but we observed
empirically that there is some flexibility as to where these LayerNorms are placed; they can also be moved to the
neuron dimensionn, and they are parameter-free.) The dynamics without LayerNorm are represented under the name
BDH-Normfree in Fig. 3.
We have the following.
Observation 3(local particle interaction ‘by mean-field’).The BDH-Normfree dynamics have the interpretation of a
mean-field interaction between particles, fully characterized at any time byO(dL)parameters of particle in state, and
O(d)parameters in particle representation.
This observation is essential for the subsequent discussion, and it can be expanded in three different ways.
In computing terms, at any timetand in any layerl, the action of the system can be represented as an iterated
application of the dynamics equations (4), with each of the particles realizing, for each equation in each layer (i.e., a
total of3Ltimes), a form of micro-program, involving local computation and communication with other particles by
broadcast. In a framework of local distributed computing (cf. e.g. (Peleg, 2000)), it would be represented as a node
performing the following form of local kernel as a part of a networked system:
1. compute some message vectorm i ∈R d locally (without communication with other particles), based only on
current activationx t,l, i,y t,l, i and previous stateZ i(t−1),
2. broadcast messagem i ∈R d to other particles,
3. receive the mean-field message¯m= Pn
j=1 mj ∈R d, identical for all particles,
4. update local activation variables for the next layerl+ 1, and update new stateσ i(t)⊆Z i(t), based on the
received result¯mof the broadcast and local computation.
In Physical terms, we observe that the interaction field of the particles, which realizes the broadcast, is localized,
and can at any timetbe expressed as a sum of pairwise particle interaction terms between particlesi, j∈1, . . . , t.
These pairwise interactions depend only on parametersZ i(t−1)andZ j(t−1), and the activation variables of these
particles, representing properties of these particles at timetand expressible throughO(Ld)scalars. This interaction
field evolves with timettogether withZ i andZ j.13
In Engineering terms, we observe that any transformation of a length-nvector into another length-nvector passes
through an intermediary low-rank representation of dimension at mostd. An example is the equation forx t,l in (7),
which reduces length-nvectory t,l to a lengthd-vector through application of the encoder matrixE, before lifting the
dimension back tonby an application of the decoderD x.
13Note thatZ i(t−1)depends only onρ t−1,l, notρ t,l. This is because of the stopping index ofτ=t−1in the definition of
attentiona ∗ in Def. 4, and is intentional.
20
3.4 Expressing BDH-GPU using BDH: preserving parameter and state size
BDH-GPU and BDH both representn-particle systems. For a special parameter choice (of BDH), they have equivalent
patterns of communication and of computation (up to placement of layer norms).
Observation 4(BDH-Normfree is a special case of the BDH graph model).Models in the BDH-Normfree architecture
(Eq.(8)) and models in the BDH architecture (Eq.(6)) are formally equivalent (i.e., the same model) subject to the
following choice of model parameters of BDH:
Gx
e −G x
i =D xE, G y
e −G y
i =D yE, G s =1 n×n,(9)
where1 n×n is the all-ones matrix.
We discuss in more details below how BDH compares to BDH-Normfree in terms of size of state and number of
parameters needed for one architecture to approximate the inference dynamics of the other. In general, BDH is not
less expressive than its tensor-based counterpart.
For BDH-GPU parameters and state are naturally expressed using tensors ofO(nd)model parameters. In this section,
we discuss how to express model parameters and state of BDH, in such a way as to maintain comparable size of
parameter and model space.
3.4.1 Expressing matricesD x, Dy, Eas graphsG x, Gy
We start by taking care of the first correspondence, that of parameter spaces of BDH-GPU and BDH. Asymptotically,
BDH is strictly more expressive at the same numberO(nd)parameters. Recall from Eq. (8) that the parameter space
of BDH-GPU consists of three matricesD yDx ∈R n×d,E∈R d×n, and (up to shifting of LayerNorms), their role is
to encode the pairs of matricesD yE, DxE,∈R n×n, as used in Eq. (7). In the Claim below, we capture the correct
encoding of one such matrix pair in the form of a graph ofO(nd)parameters.
Consider a (directed, weighted) graphG∈R+ n×n on a set of verticesV={1, . . . , n}. We will consider a graph
which need be directly a sparse graph, but can be represented as a square of a graph with few edges. Formally, we will
say thatG∈ G 2(n, m), for somem∈N, if there exists a graphH∈R (n+s)×(n+s), with vertex setV∪S, where
|S|=s, such thatG=H 2[V], i.e.,Gis the induced subgraph ofH 2 restricted to vertex setV, andHhas at mostm
(strictly positive) edges.
For aG∈ G 2(n, m), we can consider an interpretation of a hidden layerSbetween input layerVand output layerV.
All matrix weights coefficients are restricted to be non-negative, and the two linear layers are sparse with a total of at
mostmnon-negative connections.
Graphs inG 2(n, m)are naturally expressed through the edges of the previously defined graphH, usingO(nlogn+m)
parameters. The classG 2(n, m)is strictly more expressive than the class of sparse (m-edge) graphs on vertex setV. 14
We will refer to the middle layer of verticesSthat makes such a representation possible as thesparse synaptic layer,
to the graphHon vertex setV∪Sas thesparse linear circuit, and the graphH 2[V]∈ G 2(n, m)as theneuron-neuron
interaction graph.
The role of the constructed graphs is to serve for propagating linear dynamics of the formv7→Gv,v∈(R +)n×1, for
graph-based local models. We have the following Observation.
Observation 5.LetG∈ G 2(n, m)be a neuron-neuron interaction graph, withG=H 2[V], whereHis the sparse
linear circuit on vertex setV∪S, which hasmedges. Then, the linear dynamics on graphG,v7→Gv, can be efficiently
expressed through two steps of linear dynamics on graphH,v7→H 2v, forv∈(R +)n×1. This representation requires
O(m)parameters.
In the above, thee exact number of parameters needed to represent a graph ofmedges follows from conventions
introduced in the Notation (Section 1.4). In what follows, we will assume that BDH represents its parameter matri-
ces through appropriate sparse linear circuit graphsH, which it uses to realize the linear neuron-neuron interaction
dynamicsG. We illustrate the correspondence between graphsGandHin Fig. 5.
We observe that BDH can express BDH-GPU parameter matrices with the same asymptotic number of parameters.
The claim below applies to pairs of matricesDE, forD=D y andD=D x.
14The formal expression in the definition of the class of weighted graphsG 2(n, m)can be compared to that of the class of graph
distance matrices admitting sparse hub labeling representation (Abraham et al., 2011) (or closely related landmark labeling). In our
case, vertices in the hidden layerSalso have a natural interpretation of landmarks on directed paths connecting nodes ofV.
21
V
α1
VS V V
α2
Hiα1
1 2 21
Hα  j1
Hiα2
Ii
Ij
Ii
Ij
Hα  j2
H  2
G =H
G  = H H + H Hij iα α j iα  α j  
Figure 5: Neuron-neuron communication using graphsG∈ G 2(n, m): correspondence between graphHwithmedges (left), and
neuron-neuron interaction graphG=H 2 (right). The approach allows to express linear signal propagation on a broad class of
graphsG 2(n, m)using two steps of linear dynamics on a sparse circuitH, i.e.,Gz=H 2zforz∈(R +)n.
Claim 3.For any matricesD∈R n,d,E∈R d,n, there exist neuron-neuron interaction graphsG e, Gi ∈ G 2(n, m),
such thatG e −G i =DE, withm=O(nd). In consequence, for the same asymptotic number of parametersO(nd),
graph-based feed-forward mechanisms of BDH are strictly more expressive than corresponding mechanisms in the
tensor-based implementation, BDH-Normfree.
Proof.The short proof of the Claim is deferred to Appendix C.3.
We note that the converse implication does not hold: an arbitrary graphG e ∈ G 2(n, m)does not admit an exact
low-rank decompositionG e =DE. Indeed, in general any low-rank decomposition introduces a form of noise
whose implications we discussed in Section 5.3: ifG e has a modular (cluster) structure, the low-rank approximation
Ge ≈DEstill allows a form of in-cluster propagation dynamics.
3.4.2 Expressing BDH-GPU attention on graphs: sparsification and trainability ofG s
We recall that by Observation 4, the equivalence between the attention stateσ t,l in BDH and in tensor-based imple-
mentation holds for the case of the complete directed graph,G s =1 n×n.
This means two things: first, in BDH, graphGs can be trainable, while in BDH-GPU it is not. This acts to the potential
advantage of BDH for expressiveness.
Second, in BDH, the graphG s obtained through the direct correspondence is dense: withnneurons, BDH would need
n2 synapses to precisely reflect BDH-Normfree. This aspect is more of a technical nuisance than an actual difference:
the expressiveness of the attention mechanism of BDH, equipped with a sparse graphG s, is sufficient to represent
the attention operation as used in BDH-Normfree. Indeed, in the tensor-based BDH-GPU dynamics, the attention
operation is immediately followed by a low-rank operation,ρ t,l =Eσ t,l, whereρ t,l hasndparameters. Graph
models can instead rely on a sparse graphG s to achieve the same form of state compression through sparsification.
Claim 4.The attention block of BDH-Normfree can be expressed using the attention block of BDH with a graphG s
havingO(nd)edges, subject to a natural preparation of attention values entering the attention block of BDH (directly
before this attention block).
Proof.The formal statement of the Claim and its proof are deferred to Appendix C.4.
Going beyond the formal equivalence between BDH and BDH-GPU from Observation 4, the above Claim, combined
with Claim 3, shows that BDH has at least the same expressiveness as BDH-GPU even for the same number of
parametersO(nd)and the same size of stateO(nd)per layer.
Independent of graph-based models, in the subsequent analysis of the feed-forward network and attention mechanisms
of BDH-GPU, we will show that the matricesD xE, DyE, σ∈R n×n of BDH-GPU admit a natural interpretation as
n-node directed graphs (once appropriate threshold functions are applied). For example, the visualizations in Fig. 11
and Fig. 10 correspond to graph representations of matricesσandG x :=D xEof BDH-GPU, respectively, after
22
thresholding. This graph interpretation of matrices in BDH-GPUalsodefines the neuron-neuron communication
graph of the underlying BDH model, given by the equivalence from Eq. (9).
4 Implementation and scaling laws
A code framework for BDH-GPU, representing the architecture from Definition 4, is made available in the Appendix E.
In this Section, we present some guidelines on choice of hyperparameters, and an empirical study of models imple-
mented in the BDH-GPU architecture, as well as a comparison to the Transformer and other language model architec-
tures.
4.1 Implementation characteristics of BDH-GPU
Model scaling in neuron dimensionn.The architecture BDH-GPU(n, d)has two main dimensions,nwhich is the
dimension of its concept (neuronal) space, andd≪n, which is its low-rank (synaptic) dimension. The model scales
primarily with the number of neuronsn. Almost all of the weights of the model are contained in threen×dparameter
matrices calledE, D x, Dy; thus, the number of parameters is precisely(3+o(1))nd. The ratio between the dimensions
nanddincreases rapidly (“asymptotically”) with model size; already for a 25M-parameter model, a sound choice of
dimensions is:d= 256,n= 32768, read as32768neurons, each characterized by a total of3d= 3·256 = 768
scalar parameters.
Layers and heads.The architecture hasLlayers (e.g.,L= 10). As in the Universal Transformer (Dehghani et al.,
2019), all layers use the same set of weights for each of the parameter matrices.
The architecture may be equipped with several headsh, subdividing dimensionn. The role of heads is limited to
a single parameter-free LayerNorm, normalizing outcomes of linear attention separately for each head. The optimal
number of heads is typically smaller than in the Transformer (e.g.,h= 4).
Linear attention with state aligned to neurons.The state space of the model is fixed and large. It has the macro-
interpretation of associative memory (like KV-cache, but organized differently), and is used to perform linear attention.
For each layer, the state space is independent and has a fixed dimensionn×d, the same as the model weight matrices.
Thus, a portion ofdparameters of a state is directly associated with each of thenneurons. With each token processed,
a fraction of the model’s state space is updated. Sharing of state between theLlayers is not performed in the vanilla
architecture. As usual with SSM’s, there is no notion of a context window.
Similarly to BDH, BDH-GPU maintains a large recurrent state comparable in size with its total number of parameters
(c.f. Fig. 4). This stems from the fact that both the recurrent state matrix, and parameter matrices are expressed as low
query
Dx Dy
E T *
ϱ  += x vl l
xl
yl-1xl-1
n
n
n
d
d
yl
d
d
ReLUReLU
Layer NormLayer Norm
n
Linear
Linear
Linear Attention
Linear
rank-1 
key × value
update 
d ×n 
*
v
T *
x vl
Figure 6: Diagram of one layer of the BDH-GPU architecture, following Eq. (8). Layer inputs arex l−1, yl−1 ∈R n, layer outputs
arex l, yl ∈R n. Model parameters are contained in theE∈R n×d andD x, Dy ∈R d×n, and shared across all layers. Each layer
has a stateρ l ∈R n×d which is used in the Linear Attention block and persisted over time. PyTorch code implementing the model
is provided in Appendix E
23
rankdfactorizations ofn×ngraph transition matrices. We believe that this helps the model with generalization with
respect to RNNs which haveO(N 2)parameters which manipulate a state of sizeO(N).
Sparse positive activation.The architecture relies on a length-nvectorx t,l passed to thel-th layer for thet-th
token processed, which can be assimilated to the vector giving rise to keys, values, and queries in the Transformer, but
operating in higher dimension. As a crucial design assumption, this vector has all non-negative elements (xt,l ≥0).
An empirically observed fact is that the activation pattern ofx t rapidly becomes sparse (in a typical training run, only
ρ≈5%of thenentries of vectorx t are non-zero). This corresponds to the fraction of the state space read and updated
for each token.
4.2 Comparison of BDH-GPU to GPT2-like Transformers
Architecture differences.BDH-GPU in its vanilla form can be compared to the GPT2 architecture (Radford et al.,
2019) with RoPE attention. In this comparison, BDH-GPU retains or strengthens the key advantages of the Trans-
former (parallel trainability, attention mechanism, scaling laws for loss versus parameter count, learning rate per token)
on tests and benchmarks at the model scales we tested (1B parameters), across tasks such as language and translation.
The architecture of a single layer of BDH-GPU is presented in Fig. 6. The most evident architecture differences
between BDH-GPU and the Transformer include the following:
−BDH-GPU has fewer parameter matrices, allowing for more compact interpretation and analysis.
−BDH-GPU scales for parameters (and context length) almost exclusively in a single neuronal dimension,n.
−Key-value state and parameter matrices have matching dimensions and are highly localized together with
state, with portions of these matrices attributable to individual neurons.
−There is no notion of context length in BDH-GPU, and consequently no hard bound on it.
−Attention of BDH-GPU is linear, but happens in the model’s large neuronal dimension.
−Activation vectorsx, yof BDH-GPU are positive (after passing through ReLU gates), and vectorsyare
observed to be extremely sparse in practice.
Transformer-like scaling laws.We have experimentally validated the scaling laws of BDH-GPU, expressing loss
as a function of parameter count, for next-token-prediction tasks. At the same parameter scale, BDH-GPU generally
compares favorably to the Transformer even on relatively short-context tasks requiring use of attention, such as trans-
lation, Fig. 7. In general, on next-token prediction tasks, BDH-GPU appears to show improvement of loss reduction
per token of data than the Transformer, i.e.,learns faster per data token, both for the natural tasks we tested (see
e.g. Fig. 7) and on synthetic puzzles.
The BDH-GPU architecture appears to be a preferred choice for training setups where: (1) models need to learn
from scarce data, or (2) training workloads need to be optimized for makespan. For the first setting, the training rate
per token is the decisive factor. For the second setting, BDH-GPU can be used differently than the Transformer in
distributed training and distributed inference setups because of the way it scales its dimensions.
FLOPS counts.The theoretical count of arithmetic operations per token of BDH-GPU during inference is bounded
byO(ndL). Each parameter is accessedO(L)times per token (with the typical sufficient number of layers being
smaller than in the Transformer), and each element of state is accessedO(1)times per token, with small hidden
constants. These are rough bounds for a simple implementation, and do not take advantage of activation sparsity.
For short contexts BDH-GPU is amenable to parallel training with a causal self-attention kernel. The simple code
template provided in the Appendix E is sufficient to reproduce the empirical results presented in this paper on a single
GPU node. For longer contexts (typically above 4096 tokens ford= 256), a state-space kernel for linear attention is
faster and more space-efficient.
4.3 Comparison of BDH-GPU to other sequence processing architectures
Transformers with Linear Attention.Linear attention works well when used in high dimension, subject to ap-
propriate preparation of key vectors (as we discuss in Subsection 6.1). An elegant way to eliminate non-linearity of
attention, by applying preparation of key vectors through tensor product, was proposed in (Buckman et al., 2024). We
use a completely different approach to achieve attention in high dimension.
24
Figure 7: Performance of BDH-GPU and GPTXL versus model size on a translation task. We have tested all models under the
same training and evaluation regimes. All models show improved performance with scale. BDH-GPU uses exactly the formulation
provided in Appendix E, while BDH-GPU’ extends conditional gating of states and logits. All models are trained with truncated
backpropagation through time on sequences 2048 characters long, and carry their state (ρmatrix for BDH models and a buffer
of last 4096 KV-Cache entries (Dai et al., 2019) for GPTXL) between minibatches. BDH models are scaled only by varying the
number of neuronsnand keep all other hyperparameters fixed, making them easy to scale. On the other hand, GPTXL were
scaled in both the embedding dimension and the number of layers and required Dropout (Srivastava et al., 2014) tuning for optimal
performance. We observe that BDH-GPU’ matches the GPT Transformer at all model sizes we have evaluated.
Details on model hyperparameters and training setup are provided in Appendix B.2
A much broader line of work on linear attention for the Transformer, initiated by (Katharopoulos et al., 2020) concerns
applying linear attention in low dimension after appropriate preparation of keys and values. This is effectively a
technique for SSM state compression, and it is not clear whether it relates favorably to other SSM state compression
techniques. An empirical study of the amount of information recoverable from SSM’s with compressed state can be
found in (Ben-Kish et al., 2025; Liu et al., 2025).
A general theoretical framework for analyzing the expressiveness of Linear Attention models with attention working
with positive vectors can be found in the context of the FA VOR+ framework of the Performer (Choromanski et al.,
2021). Finally, a general state-space formalism for Transformer models admitting Linear Attention was considered
in (Sun et al., 2023; Liu et al., 2025).
Other types of Transformers.Variants of the Transformer with identical parameters in all layers, like the Universal
Transformer (Dehghani et al., 2019), have a number of desirable features, notably in terms of explainability and ease
of defining metrics. The downside of sharing parameters between layers in the Universal Transformer is a slight time
overhead for the feed-forward network operations, when measured in FLOPS per parameter. The situation is similar
in BDH-GPU.
BDH-GPU has sufficient expressiveness to prepare keys and queries for the attention operation, so that the outcome of
attention captures a similarity measure between keys and queries corresponding to the outcome of a class of Locality
Sensitive Hashing (LSH) functions with a very large number of buckets (cf. Subsection 6.1). The study of LSH-
based KV-cache for the Transformer was initiated with the Reformer (Kitaev et al., 2020), and the LSH Transformer
architecture introduced in the same work. Generally, the LSH Transformer is shown to rapidly approach Transformer
baseline behavior in practice as the number of buckets increases. The class of LSH functions considered is not the
same, but some intuitions gained in the study of LSH attention may carry over to BDH-GPU.
Finally, several lines of work have been devoted to making the Transformer work with longer context windows. Two
distinct approaches, which work notably well, are the soft-rolling context window of the TransformerXL (Dai et al.,
2019), and hierarchical attention (Yang et al., 2016). The BDH-GPU architecture is, generally, amenable to some of
these extensions to the Transformer’s attention mechanism, while also providing new ways to extend context length in
a more uniform manner.
Networks with sparse activation.The use of the ReLU gate as a systematic way to achieve sparse activation was,
to our knowledge, first exhibited in (Haziza et al., 2025).
25
A recent variant of the Transformer called Spark Transformer (You et al., 2025) relies on a combination of top-
k operations and soft thresholding to provide a reduction in both attention and feed forward network activations
compared to the Transformer, achieving neuron sparse activation of 8%. Compared to our work, the method used
therein to achieve activation sparsity effects is completely different (and rather involved). Beyond the question of
sparsity, BDH-GPU is not more similar to the Spark Transformer than to the Transformer.
Oscillatory SSM’s.BDH admits an interpretation at the micro-level as an oscillatory state-space network, as we
outlined in Subsection 2.4. The concept of Oscillatory State Space Models has recently been applied to time series
analysis (Rusch and Rus, 2025), with the LinOSS model showing encouraging performance relative to other SSM’s
(such as Mamba and LSTM’s) on tasks of long-horizon forecasting and time-series classification. Other than this, the
use of SSM’s with the form of an oscillator network has been limited to smaller scale studies. We are not aware of any
successful application of oscillatory SSM’s to the area of language models and reasoning in language, nor of oscillator
network SSM’s at scale whatsoever, prior to BDH.
BDH unifies multiple lines of intuition found across existing models, offering a coherent framework in which the
components naturally align. The result is a biologically plausible reasoning model with an interpretable structure and
state-of-the-art performance that has been experimentally verified.
5 Analysis: emergence of modularity and scale-free structure
Large-scale reasoning systems appear to benefit from hierarchical structuring into sub-modules. In Machine Learning,
the usual approach has been to design such a modular structure, by way of assigning roles and scales to different sub-
modules explicitly. Many works have postulated modules capable of representing hierarchical relationships between
features of objects, e.g., capsule networks (Sabour et al., 2017). Some models have attempted to capture intelligence by
recreating elements of structure recognized in brain study, going so far as to try to map functional sub-networks of the
brain with empirically identified function into specific sub-modules in the design of a larger ML system, cf. (LeCun,
2022).
In this work, we propose a learnable system which ends up with modularity. We show how scale-free modular structure
emerges naturally when the model is implemented by a network with local graph dynamics. In this Section, we discuss
the emergence of the structure of inter-neuron connections of BDH during training, while in Section 6 we look at its
temporal activation patterns during reasoning inference.
The rest of this section is organized as follows. In Subsection 5.1, we introduce basic concepts related to modularity
and scale-free behavior of networks. We then look at the expressiveness of feedforward networks of BDH-GPU and
their usefulness as a signal propagation dynamics in Subsections 5.2 and 5.3. In Subsection 5.4, we show theoreti-
cally how modular graph structure, with appropriate community voting mechanisms, emerges as a plausibly necessary
element for the feed-forward networksD xEandD yEto function correctly. In Subsection 5.5, we look at the cor-
responding empirical properties of these matrices, and the scale-free and modularity properties of the corresponding
graphsG x
e andG y
e of the underlying BDH graph dynamics.
5.1 Background: modularity and scale-free property of systems
Importance of modularity for information propagation.Graph systems serving a function related to information
propagation tend to achieve modular graph structure, and rely on it to obtain the most desirable tradeoff between ef-
ficiency and accuracy of the system dynamics. Such emergence of “hidden structure” may be observed e.g. through
topic specialization of system regions, or through the coordinated voting behavior among nodes which organize them-
selves into communities, admitting higher local density. This type of graph community self-organization has two
main advantages over a system with an explicit partition into subsystems. First, it allows nodes to belong to multiple
communities, and to act as bridges between them. Second, it allows the scale and relationship between communities
to evolve over time, as their relative importance changes or new connections emerge.
Historically, the crucial role of emergent modular structure for systems tasked with efficient knowledge retrieval at
scale was first observed in the context of the World Wide Web before the year 2000, notably in the transition from
catalogue-based systems (DMOZ Open Directory Project, craigslist, etc.) to naturally evolving systems based on webs
of knowledge (Wikipedia, etc.), interlinked topic-based communities (reddit, etc.), and reliance on evolving network
link structure for assigning expert weights to nodes in a voting process (Google PageRank, etc.). Formalization of
modular properties followed soon after, with the mostly commonly used definition of modularity being proposed by
Newman in 2006. The main theoretical reference for studies of modularity is the Stochastic Block Model (Holland
et al., 1983) and its later generalizations, e.g., to hierarchical settings. While the definition of Newman modularity is
26
not (efficiently) constructive, it can in practice be closely approximated by greedy algorithms (Blondel et al., 2008) or
spectral approaches (Massoulié, 2014).
Scale-free property.The scale-free property of natural systems dealing with information processing is generally
accepted as a manifestation of their operation at criticality. This refers to operation within a regime where they are
both sufficiently stable to enable efficient information retrieval in the short-term, and sufficiently adaptable to be able
change their behavior abruptly as new knowledge inputs become available, invalidating previous paths of reasoning
or knowledge retrieval. The generally accepted definition of scale-free behavior of such a dynamical system assumes
that the likelihood of a new piece of information (or other localized innovation to the system) to affectn′ nodes of the
system, for anyn ′ < n, should by polynomially large in1/n ′. For most information propagation dynamics, under
certain uniformity assumptions, e.g., that the new piece of information arrives at a uniformly random node of the
system, a usual necessary (not sufficient) condition for scale-free property is for the distribution of node degrees to
follow a power-law distribution.
In the practice of applied sciences studying real-world network phenomena, and in the absence of the possibility
to perform more in-depth analysis, power-law degree distributions are sometimes equated with scale-free behavior.
One notable research application involves modeling of extreme events: understanding scale-free behavior allows re-
searchers to make predictions about rare, large events from data on smaller, more common ones.
For systems with known local graph dynamics, like those considered here, more refined analysis of scale-free proper-
ties are possible. We nonetheless also report heavy-tailed degree behavior as the most obvious lithmus test indicator
of scale-free operation of the system.
5.2 BDH-GPU feed-forward network with the ‘ReLU-lowrank’ block
Low-rank matrices have been considered in multiple contexts of Machine Learning, from preference vectors to Internet
latency estimation. In the setting of the Transformer, low-rank matrices form the basis of weight-matrix approxima-
tions such as LoRA (Hu et al., 2021).
The ReLU-lowrank block of BDH-GPU captures different properties than the above settings. Its most important
effects for BDH-GPU are related to noise reduction, and faithful representation of a certain class of affinity functions
on sparse positive vectors. This makes it suitable for use in combination with Linear Attention blocks. We discuss this
point further in this Section.
Definition of ReLU-lowrank.The parameters of BDH-GPU are concentrated in three matricesE, D x, Dy. The
encoder matrixEtransforms length-nvectors in the neuronal layer into length-dvectors in the hidden layer. The two
decoder matricesD∈ {D x, Dy}transform length-dvectors in the hidden layer back to the neuronal layer.
We consider theReLU-lowrankoperation of passing through the encoder, one of the decoders, and a ReLU gate (cf.
Eq. (7)), mapping vectorsz∈R n intof DE(z)∈R n as follows:
z7→f DE(z) := (DEz) + .(10)
We note that the outputf DE(z)∈(R +)n always, and that in BDH-GPU we also always havez∈(R +)n.
Expressiveness of ReLU-lowrank in BDH-GPU and MLP in the Transformer.A single ReLU-lowrank block
can be compared to a single MLP block of the Transformer. A different comparison provides closer matching of
dimensions and structure of nonlinearities, by considering a single ReLU-lowrank with respect to a portion of the
Transformer corresponding to the second MLP layer in an MLP block, i.e., starting with the hidden layer of neurons
of the MLP in some layerl, skipping attention blocks, and followed by the ‘first’ linear layer of the MLP in layerl+1,
finally followed by the non-linearity (typically GeLU) applied in the hidden layer of neurons in layerl+ 1. Either
approach to expressiveness is valid to the extent where we analyze similarities between one architecture withLlayers
and the other with “O(L)” layers.
In the spirit of universal approximation theorem frameworks, a (deep) layer-Lstacking of Transformer’s MLP block
with ReLU activation, for Transformer latent dimensionDand MLP hidden layer dimensioncD(e.g., forc= 4), is
eventually (i.e., forL→+∞) a universal approximator for all vector functions up to dimensionD−O(1)(Shen
et al., 2022). A similar universal approximation result eventually (i.e., forL→+∞) holds up to function dimension
nfor residual ReLU-lowrank networks (Lin and Jegelka, 2018), however the convergence rate inLis slower due
to the smaller size of the hidden layer. These results translate directly to BDH-GPU architecture which also relies
on ReLU with residual connections between layers. To summarize informally, for a Transformer with latent dimen-
sionDand BDH-GPU with hidden dimensiond, we expect their feed-forward networks to be comparably expressive
27
(though usually without strict mathematical equivalence) as function approximators for functions up to some dimen-
siond ′, d < d ′ < D, betweend ′ andDthe Transformer can express a richer class of functions, and betweenDandn,
BDH-GPU can approximate some functions, whereas the Transformer does not use such high dimension in its vector
representations.
We remark that in all cases, regardless of expressiveness of feed-forward mechanisms, BDH-GPU is set up so that it
is only using inputs and producing outputs within the positive orthant,(R +)n 7→(R +)n.
The main point to consider is:what classes of useful high-dimensional functions in the positive orthant does ReLU-
lowrank naturally express?
5.3 ReLU-lowrank as a signal propagation dynamics
Error of low-rank approximation (without ReLU).ConsiderR n as a space spanned by a fixed set ofnorthogonal
unit basis vectorsV={v 1, . . . , vn}, callednodes.
The low-rank operation can be used to approximate affinities between pairs of nodes, in the following sense. For a
given matrixG ′ ∈R n×n, consider low-rank matricesD∈R n×d, E∈R d×n, such thatG:=DEapproximatesG ′
pointwise. 15
Assume∥G ′∥1,∞ ≤1. An application of the Johnson-Lindenstrauss lemma shows that the following bound holds in
the infinity norm:∥G ′ −G∥ max =O(
p
logn / d)(cf. e.g. (Udell and Townsend, 2019; Budzinskiy, 2025)). Then,
forz∈R n =R |V| with∥z∥ 1 ≤1, we have:
∥G′z−Gz∥ +∞ =O(
p
logn / d)(11)
However, no similar bound holds for∥G ′z−Gz∥ 2. Even for ‘simple’ scenarios like the identity transformation
G′ =I n, the best low-rank approximation admits O(1) additive error in the L2-norm for almost all inputs, and even
greater distortion (approaching √n) may appear in the L1-norm.
This makes the low-rank operation useful for determining affinity of pairs of coordinates in dimensionn, but more
problematic as a vector transformation function. However, the ReLU-lowrank mechanism (Eq. (10) is able to sup-
press a part of the noise of the linear low-rank map, allowing to approximate a sufficiently broad class of non-linear
operations.
Expressiveness of ReLU-lowrank for Markov chain propagation.We will consider positive inputsz∈R +n,
focusing on sparse vectors.
One important case concerns approximating a Markov chain transformationz7→G ′z, for someG ′ ∈R+ n×n. For
such a transformation in the positive orthant, adding the ReLU operation to the linear map does not change anything
directly,G ′z= (G ′z)+. However, when considering a low-rank matrixG, the non-linear transformation(Gz) + can
provide a closer approximation ofG ′zfor some classes of input vectorsz, than the low-rank linear operationGz.
We start with the following illustrative example.
Claim 5(propagating a Markov chain).LetG ′ be the random walk matrix of a directed graph with out-degreer(i.e., a
stochastic matrix withrnon-zero entries of1/rin each row), and letv∈Vbe a node (basis vector),∥v∥ 1 =∥v∥ 2 = 1.
Then, for anyε >0, there existsd=O(r 3 logn/ε)such that for some matricesD∈R n×d, E∈R d×n, we have
∥G′v−f DE(v)∥1 =O(ε).
Proof (sketch).LetD ∗ ∈R n×(d−1),E ∗ ∈R (d−1)×n denote matricesD,Erestricted to all but the last coordinate in
dimensiond. PickD, Eso that∥G ′v−D ∗E∗v∥∞ < ε ∗, whereε ∗ =ε/r, following Eq. (11) (we have∥G ′∥1,∞ ≤1
by stochasticity ofG ′). Further, set a fixed bias, placing1on all entries of the last coordinate in dimensiondofD,
and−ε ∗ on all entries of the corresponding last coordinate in dimensiondofE. Taking into account this bias, we now
have∥(G ′v−ε ∗1)−DEv∥ ∞ < ε ∗.
For all coordinatesv j ∈Vsuch thatv j T G′v= 0, we now havev j T DEv <0, hence alsov j T fDE(v) = 0. For all
other coordinatesv j, we havev j T G′v= 1/r, and1/r−2ε ∗ < v j T fDE(v)≤1/r. Thus,∥G ′v−f DE(v)∥1 ≤2ε ∗r,
and the claim follows.16
15Elements ofGcan be computed pointwise by each pair of nodes:V×V∋(v 1, v2)7→G:=v 1
T DEv 2 ∈R.
16As a point of elegance, we note that in this proof,vj
T fDE(v)≤1/r, sof DE(v)was not anunbiasedestimator ofG ′v. This is
easily fixed in the first-order by introducing a global multiplicative bias of(1 +ε∗)to the approximation, for example, substituting:
(1 +ε ∗)D7→D.
28
The above observation shows how ReLU-lowrank deals with one specific class of graph affinity functions (random
walks of adjacency of sparse graphs), for transformations of vectors which are nodes in our distinguished basis. We
use this example as it is the simplest case which exhibits the benefit of threshold non-linearity: for basis vectors,
the operationf DE captures a basic propagation effect which is well known (in general) to require afull-rankmatrix
G′ ∈R n×n if relying only on linear operations.
Propagation and reinforcement of signal.The same thresholding approach, as discussed for Markov chains, turns
out to be applicable to a wider class of signal propagation dynamics. It consists in first obtaining a positive-valued
signal with heavy random noise, then applying a negative bias, and finally using the ReLU gate to act as a noise
threshold.
Any linear functionG ′ can be represented with a hidden layer ofs≤n 2 nodes, through two matricesD ′ ∈R +n×s
andE ′ ∈R +n×s, such that:
G′ =D ′E′.
The above holds in general, and we will refer to such a representation ofG′ as having a sparse hidden (synaptic) layer.
We will consider now the question of expressing non-negative functions,G ′ ∈(R +)n×n. An example of a valid
representation ofG ′ is given through a node-edge incidence representation,D ′
i,(i−1)n+j =E ′
(i−1)n+j,j =
q
G′
ij, but
usually this representation is not optimal in terms of the number of non-zero entries ofD ′ andE ′.
In general, any low-rank approximation ofGcan be equivalently expressed asG≈D ′PDP T
E E′, for some two
matrices,P D, PE ∈R s×d. We will consider the most common class of low-rank approximations obtained by taking
PD =P E =P∼ N(0,1) s×d/
√
d. Consider a vectorzpassing through the ReLU-lowrank operation, and the
following vectorsu∈R s,w∈R n:
u:=E ′z
w:=D ′P P T u
Ifv T
i zhas the interpretation of a signal being sent by nodev i, thenuis the encoded message being passed through the
hidden layer of the network, andv T
j wis the message received by nodev j.
5.4 Modularity in BDH-GPU signal propagation
We are now ready to capture the essence of the signal propagation and reinforcement capability of the ReLU-lowrank
system. To describe the conditions under which a neuron is able to decide whether it should, or should not activate.
By a standard analysis of independent Gaussians, we have the following probabilistic statement, under random choice
ofP.
Claim 6(selective neuron activation).Suppose that the signal ofuis uniformly concentrated on a set of nodesAof the
hidden layer, i.e., for some subsetAof indexes of the hidden layer, we haveu ¯α= 0for¯α̸∈A, andu ¯α∈[ 1−κ√
|A|
, 1+κ√
|A|
]
for¯α∈A, so that∥u∥ 2 ∈[1−κ,1 +κ]for some small constantκ≥0. Suppose each nodev j ∈Vis connected
inD ′ to some set of nodesB j in the hidden layer,B j ={ ¯β:D ′
j,¯β ̸= 0}, and let these connections weight be drawn
uniformlyD ′
j,¯β ∈[ 1−κ√
|Bj | , 1+κ√
|Bj |]for ¯β∈B j. LetC j =A∩B j. Define the ratio:
ϱ:=
s
|Cj|
|A| · |Cj|
|Bj| .
Then, there exists an absolute constantc >0, such that for any value ofw j (where we recall thatw:=D ′P P T u), we
have:
Pr
h
wj ≥(1−κ) 2ϱ−c
p
logn / d
i
= 1−O(1/n)andPr
h
wj ≤(1 +κ) 2ϱ+c
p
logn / d
i
= 1−O(1/n).
(12)
Thus, the value ofw j can be used by a neuron to obtain an estimation ofϱ, and apply a threshold to activate accord-
ingly.
Proof.Observe thatw j = (P T D′
j,·)
T
(P T u). As∥D ′
j,·∥2 ∈[1−κ,1 +κ], a standard application of Johnson-
Lindenstrauss to vector inner products givesPr
h
|wj −D ′
j,·
T u| ≤c
p
logn / d
i
= 1−O(1/n)forclarge enough.
SinceD ′
j,·
T u∈[(1−κ) 2ρ,(1 +κ) 2ρ], the claim follows.
29
Neurons Neurons
Vj
Sparse Hidden Layer
(Synaptic)
(a)
D′  E′
T
Q = PP
( ≈ Is )
A u
z
Bj
Cj
< <
<<
<
<
<
<
<
<
<
<
<
<
<
<
< <<
<
<
<
<
<
<
<
<
<
V V
(b)
A
Bj1
Cj1
Bj2
j1
j2
Cj2
Figure 8: The ReLU-lowrank feedforward network of BDH-GPU allows neurons to activate when triggered by activation signals
in its own community. (a) Illustration of the selective neuron activation pattern in the proof of Claim 6, showing the activation
decision of nodev j (left) based on active setAin the sparse hidden layer. (b) Illustration of Eq. (12) showing the relationship
between sizes of sets in the sparse hidden layer: active setA, setB j connected to neuronv j, and the intersectionC j =A∩B j:
neuronv j1 becomes active, but neuronv j2 does not.
The ReLU-lowrank operationf DE, after adding appropriate negative bias, can thus be used to propagate positive
affinity functionsG ′ on input vectors, performing the following form of thresholding: neuronsjin the output layer
individually compute a form of local “F-score”ϱgiven by Eq. (12) of the activation of the positive sparse hidden layer,
and decide based on it whether they are good match for the output activation; if the threshold condition onϱis not
met, the neuronjdoes not activate in the output vector (see Fig. 8 for an illustration).
Equation (12) naturally coincides with a pattern of communication within network graphsG ′ admitting positive New-
man modularity (Newman, 2006), allowing nodesvj to correctly receive messagesuwhich in the hidden layer primar-
ily reached a denser cluster ofG ′ containingv j. For a specific illustration, letHbe an undirectedk-block stochastic
block model (SBM) network (Karrer and Newman, 2011) withk∈Nblocks ofn/knodes each, in-block edge density
pand out-of-block edge densityq < p. We putG ′ :=D ′E′ =H 2, i.e., the first connection layer ofG ′ isE ′ =Hand
the second connection layer is alsoD ′ =H. Suppose thatHis a random SBM graph with positive Newman modu-
larity separated from0, i.e., letµ= k−1
k
p−q
p+(k−1)q >0. Following Claim (6) withκ= 0, we can find a ReLU-lowrank
representationGto achieve a communication scheme onG ′, such that a message sent from one nodez=v i activates
a nodev j wheniandjare in the same block with probability1−O(1/n), and with probabilityO(1/n)otherwise,
whenµ > 1
p
p
logn / d.
We thus make the following intuitive observation.
Observation 6(in-cluster signal reinforcement).The ReLU-lowrank representation of BDH-GPU(n, d)is sufficient
to represent in-cluster information spreading dynamics in models of graphs with constant in-cluster density and arbi-
trarily small positive modularity (such as thek-cluster Stochastic Block Model) whend/logn=ω(1)is an arbitrarily
slowly growing function.
While Claim 6 and Observation 6 are made with reference to an almost-uniform distribution of signaluon the set of
nodes of the middle layer,ucan have (and in practice does have) a distribution of density which is non-uniform, e.g.,
going acrossa=O(logn)different clustering scales, with a(1/a)-fraction of the signal represented at each scale.
This allows neurons in the output layer to combine a smaller number of strong signals in its local cluster, with a larger
number of weaker ones spread more globally. Such an approach coincides with the observed structure of the graph
D′E′, discussed in Subsection 5.5.
Supermodularity on input perturbation.We clarify how the properties of functionf DE : (R+)n →(R +)n relate
to the previously discussed ability to make an input signal resonate “within a module” in a graph with hidden modular
30
structure. First, note thatf DE is a subadditive function, but is not submodular in generalwith respect to the set of
ncoordinates of its input vector. In some of the regimes in which it appears to be operating, locallyf DE exhibits a
form of behavior opposite to submodularity, referred to as ‘supermodularity’, or ‘increasing returns’ of adding new
coordinates to the input vector. This is already implicitly captured by Claim 6, but we can consider a simpler example.
Take a variant of the setting from Observation 5 with the same choice ofG ′, and letz∈(R +)n and biases ofD
be chosen so that all coordinates ofDEzare approximately equal to−1.5/r±o(1)(this can be done by choosing
e.g.z j = 1/n). Then,f DE(z) = 0, and for anyv i, vj ∈V,f DE(z+v i) = 0a.s.,f DE(z+v j) = 0a.s., but
fDE(z+v i +v j)has non-zero coordinates a.s. with values approximately1/2r, for all nodesv k which are common
out-neighbor nodes ofv i andv j, i.e., for allksuch thatG ′(vi, vk) =G ′(vj, vk) = 1/r. This mechanism generalizes
to finding common neighborhoods which have many connections to two given subsets of nodes,Va andV b. In a setting
where the considered affinityG′ is bi-directional (e.g., a symmetric matrix), this corresponds to finding shortcut nodes,
allowing to go fromV a toV b.
It follows that the neighborhood-reinforcing nature of the threshold dynamics of BDH-GPU, which plausibly follows
from the logic of its role in inference and from the needs for an efficient computational process, is starkly different
from the more often studied submodular behavior of threshold and cascade dynamics on real-world networks (Kempe
et al., 2003), and plausibly, much less smooth when considered as a dynamical process.
5.5 Empirical findings: parameter distribution in ReLU-lowrank matrix products
We consider theDmatrices (in the same wayD y andD x) andEmatrix obtained after training of BDH-GPU models,
and used in the ReLU-lowrank operation Eq. (10),f DE(z) = (DEz) +.
Choice of prior of matrix parameter distributions.Following the discussion in Section 5.4, we expect matrix
G:=DEto reflect the clustering (modularity) structure of the neuron-neuron communication graph. Any plausible
parameter distribution of matrixGmust therefore allow heavy-tailed distribution of entries. At the same time, a
Gaussian noise term is inherent to low-rank matrix representation, and needs to be taken into account together with
this heavy-tailed distribution.
We now provide a somewhat more fine-grained explanation, which leads to the prior on the structure of matrixGas
given by Eq. (13). Consider a training set-up in which the ReLU-lowrank operation described by matrixGis treated
as an approximation of the same operation, governed by a high-rank matrixG ′, withf ′(z) := (G ′z)+. Considering
this block in isolation from the rest of the training system, the training of matricesD,Egoal corresponds to learning
an approximation off ′, withD∈R n,d, E∈R n,d, such thatf(z)≈f ′(z)holds for some class of vectorsz.
For the rest of this analysis, we will consider the functionf ′ as a ground truth reference for the intended operation
of the ReLU-lowrank block. This type of analysis can be seen as plausible over short time spans in later phases
of training of a BDH-GPU model, i.e., once individual neurons inR n have started to admit semantic or functional
meaning, and so when functionD ′E′ describes a property of the problem being solved in a (frozen) concept space,
and not a co-learning process between the representation of the concept space inR n and the functions applied to it.
We can representG ′ :=D ′E′, whereD ′ ∈R n,s,E ′ ∈R s,n, withs=O(n 2), are in general matrices of rankn; we
havef ′(z) := (D ′E′z)+. Without loss of generality, we can choose from among the possible representations one with
the following distribution of positive and negative elements:D′ ∈(R +)n,s,E ′ =E ′e −E ′i, withE ′e, E′i ∈(R +)s,n.
We will write:G ′ =G ′e −G ′i, whereG ′e =D ′E′e, andG ′i =D ′E′i. The main purpose of the chosen representation
G′ =D ′E′ =D ′(E′e −E ′i)is to have matricesD ′,E ′i,E ′e with much smaller outlying elements compared to
matrixG ′, which leads to more justified conclusions about the uniform nature of the noise introduced by the low-rank
decomposition.17
Assume now that we learn to approximate functionf ′ withf DE by trainable matricesD, Ethrough the following
low-rank scheme:
G=DE:= (B D +D ′P)(P T E′ +B T
E),
whereP∈R s,d isnon-parametricand the result of random sampling an almost-orthonormal random projection so
thatP P T ≈I s (e.g.P∼ N(0,1/
√
d)s,d), andB D, BE ∈R n,d representtrainableadditional terms for compensating
17For a specific example, one very broad class of matricesG ′ is given by the product of sparse matricesD ′,E ′, in which each
s-element row ofD ′ (column ofE ′) has at most∆≪nnon-zero elements, each with value bounded byO(1/
√
∆), and all
remainings−∆elements of these matrices are equal to0. The resulting elements,G ′
ij= P
α D′
i,α{E′}α,j, may be much less
uniform, only satisfyingG ′
ij=O(1). This type of scenario captures the expressiveness of set intersection for “bag-of-words”
models for language, or expressiveness of “hub label” representations for a measure of node proximity in a directed graph.
31
error or introducing bias, with the goal of minimizing some loss functionL(f ′, fDE). The termsB D, BE compensate
the error introduced by the approximationP P T ≈I s, after the ReLU operation.
LetQ:=P P T =I s +δ I +δ Q, whereδ I ∈R s×s is a diagonal error matrix, andδ Q ∈R s×s is a non-diagonal
(hollow) matrix. We have:
G=DE= (G ′e −G ′i) +D ′δI(E′e −E ′i) +D ′δQE′
| {z }
εQ
+ (BDE+DB T
E)| {z }
εB
.
Since all elements ofD ′, E′e, E′i are non-negative andI δ is diagonal, we can represent elementsG ij, fori, j∈
1, . . . , n, as follows:
Gij = (1 +ε e
δ ij)G′e
ij −(1 +ε i
δ ij)G′i
ij +ε Q ij +ε B ij (13)
where|ε e
δ ij |=O(
p
logn / d)and|ε i
δ ij |=O(
p
logn / d)have the interpretation of small multiplicative distortion.
Following (13), we expect the elements ofGto be distributed as the sum of four different distributions. The term
G′e
ij has the interpretation of positive ground truth elements ofG ′. The term−G ′i
ij has the interpretation of negative
ground truth elements ofG ′; its use in combination with the ReLU mechanism can be interpreted as inhibitory action.
Both of these terms are subject to slight multiplicative distortion.
The termε Q ij has the interpretation of non-trainable noise (which depends only onD ′,E ′ and the random choice of
P). Under reasonable assumptions on outlying elements ofD ′, E′, it is a form of almost-Gaussian symmetric noise
inherent to the considered class of low-rank projections,ε Q ij →N(0, σ Q), for someσ Q ∈R +, and the expected
value of this noise is typically very close to0, even when considering the expectation ofε Q ij conditioned on known
values ofε Q i′j′ for a small number of indexes(i ′, j′)in the matrix.
Finally,ε B ij is a trainable term, whose norm tends to0asdincreases. We expect it to have the interpretation of
bias used to offset the low-rank Gaussian noise and perform denoising in the ReLU-gate, as previously discussed in
Section 5.3. Because of the action of the ReLU gate, we plausibly expect the distribution ofεB ij to be skewed towards
negative numbers, with0>Eε B ij ≫σ Q.
From the above discussion of the four terms of the sum in Eq. (13), we see that only one of these terms,G ′e
ij, is
expected to take values much larger thanσ Q with non-negligible probability. We reach the conclusion that a part of
the relevant signal ofG ′ is concentrated in the right tail of large positive matrix entries ofG.
Hypothesis 1(right tail contains signal).Consider the interpretation that the ReLU-lowrank transformationz7→
(Gz)+, withG=DE, has learned to act as an approximation of some other operationz7→(G ′z)+, whereG ′ has
no low-rank constraint imposed on it. Then the right tail of the distribution of matrix elements ofGcorresponds to
the right tail of the distribution of elements ofG ′, starting from some positive threshold valueσ Q, associated with the
noise of the low-rank decomposition. Formally, for almost all pairs of indicesi, j∈1, . . . , nsuch thatG ij ≫σ Q, we
also haveG ′e
ij ≫σ Q.
The converse implication, thatG ′e
ij ≫σ Q impliesG ij ≫σ Q, also plausibly holds under some stronger assumptions
on the form of biasesε B ij which may follow from minimizing training error for the specific inference task considered.
This direct method of decodingGfromG ′ does not extend from the right tail towards the center of the distribution.
For the choices ofn, dwe make, we expect the term dominating most elements of matrixGto beε Q ij. For example,
whenG ′ is a stochastic matrix, we expect to haveσ Q =O(1/
√
d)(cf. Eq. (11) for the corresponding infinity-norm
bound,|ε Q ij|=O(
p
logn / d)). With P
i,j |G′e
i,j|=nfor a stochastic matrix, we expect the right heavy tail of the
element distribution ofGto haveΩ(n
√
d)elements (out of then 2 matrix elements ofG) which are clearly separated
from the Gaussian noise.
We confirm empirically that the right tail ofG, defined as above with respect to thresholdσ Q, turns out to contain
a non-negligible portion of the parameter capacity of matricesD,E, even for very small models (10M to 100M
parameters).
Experimental setup.We prepared parameter matrices of a 24M-parameter BDH-GPU model configured withh= 4
heads andL= 8layers,n=h·2 13 = 215 neurons, and hidden low-rank dimensiond= 256. We considered the
weighted neuron-neuron interaction graph, having the encoder-decoder matrix pairG=D xEas its node adjacency
matrix on the set of neuronsV= 1, . . . , n. For uniformity, we subsampledGby picking node subsetsV (a),a∈
{1,2,3,4}, associated with each head, and considered the weighted subgraphsG (ab) ={V,{(u, v, G uv) :u∈
V (a), v∈V (b)}}, withG (ab) ∈R n∗×n∗
wheren ∗ =n/h= 2 13, each having(n ∗)2 = 226 weighted edges.
32
Figure 9: (a) Heavy-tailed element distribution and modularity analysis of the excitatory neuron-neuron connection graph con-
tained the encoder-decoder matrixG ∗. Distribution of elements of the encoder-decoder matrixG ∗ ∈R n∗×n∗
of a BDH-
GPU model withn ∗ = 8192neurons andd= 256: histogramfreq G∗(x), its symmetric partfreq−symmetric G∗(x) :=
min{freqG∗(x),freq G∗(−x)}, and distribution skewfreq−skew G∗(x) := freq G∗(x)−freq−symmetric G∗(x).⋄(b) Esti-
mate (lower bound) of Newman modularity of matrixG∗
≥β for different values ofβ, plotted as a function of the number of non-zero
elements (edges) ofG ∗
≥β. Modularity of random graph baselines are provided for reference, for theG(n∗, m)model with the same
number of edges asG ∗
≥β, and for a matrix(P 1P T
2 )≥β′ with the same number of edges asG∗
≥β, whereP 1, P2 ∼ N(0,1) n∗×d. The
modularity estimates were obtained using the community structures returned by the Louvain algorithm, in the best of 5 clustering
runs with different random seeds.
We repeated the experiment5times using models pretrained with different random seeds.
Findings.For all of the5models we pretrained for this purpose, exactly3out of the4encoder heads and all decoder
heads adhered to the prior on parameter distribution given by Eq. (13), showing a good correspondence for12out of
16of their parameter sub-matricesG (ab).
We continue the discussion in this Section for one specific matrixG (ab) of one specific pretrained models, which was
chosen as representative. The example we choose hasa=b; and so the matrixG (ab) has an interpretation asG[V a],
i.e., the subgraph ofGinduced by vertex setV a, which enables us to visualize the graphG (aa) more easily on its
vertex setV a.
We refer to the representative object of our study, i.e., to the matrixG(aa) of the selected model, asG ∗. For any matrix
Aandβ≥0, we denote byA ≥β the matrixAcut off at thresholdβ, i.e.,A ≥β ij =A ij ifA ij ≥β, andA ≥β ij = 0
otherwise.
The distribution of elementsG ∗
i,j is presented in Fig. 9 (a). We find that the observed distribution corresponds well to
the prior expected of it by Eq. (13). We determine the threshold valueβ≥0at which we expect to capture signal,
G∗
≥β ≈G ′
≥β, following Hypothesis 1. We find (from Fig. 9(a)) that the separation from noise happens for this specific
matrixG ∗ atβ 1 ≈1.2, at which point the right heavy tail begins to dominate. However, already for much smaller
values ofβwe find thatG ∗
≥β has high modularity, and this actually increases as more non-zero values are added to
G∗
≥β for smallerβ, up to a maximum atβ 2 ≈1.0(Fig. 9(b)). Even for much smaller values ofβ, the modularity
ofG ∗
≥β remains almost constant up to well above2 20 non-zero matrix entries on then ∗ = 2 13 nodes considered.
The modularity of the baselines, of random graphs or random low-rank matrix products, quickly drops to 0 in this
regime. This should be compared to the total number of parameters of the matricesD x, Ecorresponding toG ∗, i.e.,
2·2 13 ·28 = 222 parameters. A complementary analysis of the inhibitory signal, for a similarly defined matrix|G∗
≤−β|,
also finds that this structure has high modularity.
In auxiliary experiments, we looked at basic graph parameters of matrixG ∗
≥β, treated as a directed graph on its set
of nodes. We setβ= 1.2, obtainingm= 46820non-zero entries (edges) inG ∗
≥β. We found thatG ∗ has a heavy-
tailed, power-law-like degree distribution, with generally more concentrated out-degree than in-degree (Fig. 10(a)).
Generally, this finding is consistent with expectations as to the structure of a network with positive modularity. The
difference of in- and out-degree distributions, while plausible and prevalent in real-world information dissemination
networks, was not considered in Subsection 5.3.
33
(a)
(b)
Figure 10: (a) Unweighted in-degree and out-degree distribution for then ∗ = 8192neuron nodes andm= 46820edges of matrix
G∗
≥β withβ= 1.2. The distributions exhibit power law distributions, with different exponents, the out-degree distribution being
more concentrated. (b) Visualization of graphG ∗
≥β, hinting at its core-periphery structure.
Finally, a visualization ofG ∗
≥β (Fig. 10(b)) exhibits a core-periphery structure. This is again consistent with the
expected modular structure.
Empirical Finding 3.We confirmed that during training,a graph structure with positive modularityappears in BDH-
GPU model parameter matricesDxEandD yE. This modular structure plausibly follows from the network’s inference
function, and specifically from the cluster-aware information propagation dynamics supported by the ReLU-lowrank
mechanism (Observation 6).
We also observed that for all of the studied models withh= 4heads,1encoder sub-matrix out of4has no heavy
positive tail, and generally appears to capture a form of inhibitory structureG ′i from Eq. (13). Since we have not
provided convincing mechanisms for isolating negative signals inG ∗ and these are easily confounded with the bias
termε B, we omit this case from discussion. We remark that the apparent need for passing activations through such a
separate “inhibitory circuit” is one of the most evident explanations for why introducing (a small number of) heads to
BDH-GPU provides an improvement in model quality.
6 Analysis: linear attention, sparse positive activation, and monosemanticity
6.1 Macro-expressiveness of attention in BDH-GPU
The attention mechanism of BDH-GPU can be described at a coarse-grained level as a transformation mechanism for
key-query-value vectors, similar to that in the Transformer. This description is complementary to the interpretation
of the BDH-GPU attention mechanism at the micro-level of correlations between neuron pairs, which we defer to
Section 6.2, which provides more insight into the way activation vectors used by BDH-GPU relate to the concept
space of the model.
We compare the attention mechanism of BDH-GPU with the attention mechanism of the Transformer, describing both
as reflections of a general attention mechanism. Specifically, we explain why, and up to what context length, the linear
34
attention mechanism of BDH-GPU plausibly fits into macro-expressiveness frameworks of attention designed for the
Transformer (based on RASP).
Basic properties of BDH-GPU attention.The key-query space for BDH-GPU isR n, the same as its neuron di-
mension, rather than the small dense dimension used by the Transformer. The keys and queries used by BDH-GPU
are given by positive vectors, in(R+)n, and are expressed by the same vectorx t,l (noting that at timet,x t,l is used as
a query, and onlyx τ,l, forτ≤t−1, are used as keys).
‘Value’ vectors of BDH-GPU remain in the small dimension,R d, which at some model scales is comparable to the
dimension used for attention ‘values’ in common configurations of the Transformer.
The relationship between softmax-based attention of the Transformer, regarded as a low-dimensional kernel for gen-
eral linear attention, and linear attention for vectors in the positive orthant, was considered in a framework called
FA VOR+ (Choromanski et al., 2021). Here, we provide a few complementary (simpler) observations, sufficient to
grasp the main effects of the ability of Linear Attention to distinguish facts in context.
State capacity vs. distinction capacity.The matrixρ∈R n×d, which is used to represent state for each layer of
BDH-GPU, should theoretically have sufficient capacity to storeO(n)‘value’ vectors inR d if considered as a lookup
table for values. We now remark that its actual capability ofdistinguishing factsusing the linear attention mechanism
is also asymptotically close ton.
Attention is a mechanism of associative memory which, given a series of key-value pairs((k 1, v1). . . ,(k t, vt))∈
(Λk ×R d)t, a queryq∈Λ q and an affinity functionϕ(·,·) : Λ q ×Λ k →[0,1]between the space of queries and keys,
returns the attention value:a t =Pt−1
τ=1 ϕ(q, kτ)vτ (or a normalization thereof). With BDH-GPU, we consider ‘value’
vectorsv∈R d, wheredis small. The spaces of keys and queries may be assumed to coincide asΛ = Λ k = Λq, and
we consider in general a single key-query sequence, given by(k t)t∈N:18
at =
t−1X
τ=1
ϕ(kt, kτ)vτ (14)
This key-query spaceΛmay be considered as an abstract space, and represented in any way which is convenient, for
as long as the affinity functionϕ(k t, kτ)is preserved. For example, when the keys and queries are sampled from a
finite (though possibly extremely large) set, there also exists some vector space dimensionν(possibly extremely large)
and a function mappingf: Λ→S ν, whereS ν ={z∈R ν :∥z∥= 1}is the unit sphere, such that the scalar (dot,
cosine) product inS ν satisfiesf(k t)·f(k τ) =ϕ(k t, kτ). In other words, any affinity functionϕbecomes linear when
represented in sufficiently high dimension, subject to suitable preparation of its arguments with functionf. Withν
extremely large,S ν is a sort of Platonic ideal of a space in which the attention keys and queries live, with no relation
to any specific model.
This type of argument, often used in considerations of Support Vector Machines, is linked to two challenges: (1)
ensuring that the dimension actually considered by the network (in our casen) is high enough compared to the hy-
pothetical dimension(ν), and (2) ensuring that a suitable preparation functionfexists and is easy to learn for the
model.19 We now explain when the dimensionncan be considered sufficient, and what types of keys can be prepared
by BDH-GPU.
Expressiveness of linear attention in dimensionn.The Linear Attention mechanism aggregates key-value corre-
lations over time. In general, the associated rate of accumulation of noise is manageable, up to the approximate scale
of betweent= ˜Ω(√n)andt= ˜O(n)key-value ‘facts’ stored in the attention of a given layer. We make the following
statement about the Linear Attention mechanism in general.
Claim 7(informal statement).The mechanism of Linear Attention, applied in dimensionR n, can approximately
express an attention affinity function for up tot= ˜O(n)‘key-value’ pairs in context, with ‘values’ having comparable
L2-norm, under moderate assumptions on weak correlation of historical keys and uniformity of the expressed affinity
18This assumption is known to have moderate practical implications for trainability. In this specific discussion, it is ‘without
loss of generality’, since one can considerΛ = Λ 1 ⊗Λ 2 ⊗. . .⊗Λ t, and consider eachk i as chosen fromΛ i, defining affinity
ϕ(kt, kτ) : Λt×Λτ →[0,1]appropriately to handle successive keys and queries (effectively describing a general form of positional
embedding).
19The Transformer can also be positioned in the same SVM framework: the Transformer’s attention represents a form of “kernel
trick” for one specific affinity functionϕ, with the kernel used to approximate it being the exponential function (in the case of
softmax attention).
35
function. Without such assumptions, Linear Attention can compute the correct affinity up to at leastt= ˜Ω(√n)
‘key-value’ pairs in context, except for a negligible fraction of possible inputs. Keys and queries need to be suitably
prepared beforehand.
The formal statement and proof is provided in Appendix C.2.
The above claim captures the expressiveness of Linear Attention in dimensionR n, subject to some way of preparing
keys and queries inR n by the model in blocks preceding the attention block. A model using Linear Attention has to
learn its own way to prepare keys. In fact, differentnaturalapproaches to key preparation, for example using random
projections or hashing on a set oftvectors, lead to the same asymptotic statement of Claim 7. (In the proof in the
Appendix, we chose to use a particularly simple one.)
The specific way of preparing keys used (learned) by BDH-GPU for its Linear Attention is particularly interesting.
Except for the effect of RoPE rotation, which introduces a negative positional effect in the affinity of keys and queries,
BDH-GPU uses activation vectors (keys, queries) with only positive coordinates to represent its keys.
We discuss some aspects of how the positive activation vectors of BDH-GPU relate to Linear Attention.
Preparation of positive keys for Linear Attention.Activation vectors of BDH-GPU belong to the positive orthant,
and are often sparse. The interpretation of such vectors depends on whether we consider the positive orthant to be a
“valid shape” for the latent concept space of the considered task (in this case, language and reasoning), or whether the
task has to be embedded into such a space. For language, this would be a question of whether aword2vec-like internal
representation of the concept space by the model has an inherent advantage over abag-of-words-like representation,
especially when expressing concept affinities in attention.
We note that latent representation of key and query vectors in the positive orthant is natural for any problem which
isamenable to attention. In the discussion of general attention given by Eq. (14), we noted that the affinity function
ϕtakes values in[0,1], and we considered an embeddingfof a set of key vectorsk 1, . . . , kt intoR ν such that
f(k t)·f(k τ) =ϕ(k t, kτ)≥0. Given this condition on non-negativity of dot product on all pairs among thet
vectors considered, we could have, without loss of generality, used an appropriately rotated embeddingfso that
f(k τ)∈(R +)ν, thusdirectly reducing the problem of general attention to a problem of linear attention in the non-
negative orthant. The question which remains is a subtle one: whether this type of embedding of the latent space
of language and reasoning in(R +)ν is ‘natural’, i.e., preserved over long periods of time of inference and training,
notably longer than the short windowtof context used for Transformer-like attention.
In the rest of the paper, we are generally inclined to assume that representations in(R +)ν of concepts, combinations
of concepts, and density distributions over such combinations of concepts, are universal to language and reasoning.
We limit ourselves to a very brief discussion of a way to represent attention keys with positive vectors for problems
for which such a concept representation is not natural.
Using LSH to move key vectors into the positive orthant.Locality Sensitive Hashing (LSH) is one technique
for converting arbitrary vectors in a lower-dimensional spaceR a, for some fixeda∈N, into vectors in(R +)n, in
a way which can be used to describe certain ‘sharp-boundary’ affinity functionsϕinR a. Consider ann×amatrix
represented asnfixed random vectorsλ 1, . . . , λn ∈R a, and a corresponding sequence ofnappropriately chosen
gating functionsγ 1, . . . , γn :R→R +. For a vectorv∈R a, we define:
b(v) :=γ([λ 1 . . . λn]v) = (γi(vT λi))1≤i≤n.(15)
Eachi-th element of vectorbthus corresponds to the outcome of thei-th bucket of LSH.
The bucketing functionbmay now be used to prepare queries and keys as attention inputs. Ifγ i is a{0,1}-valued
threshold function, then, forq, k i ∈R a,b(q) T b(ki)is an attention affinity function betweenqandk i, equal to the
number of LSH buckets shared betweenqandk i.
Observation 7.The LSH vector affinity functionb, given by Equation(15), usingnbuckets on vectors inR a for some
a∈N, can be expressed through Linear Attention with attention keys in the positive orthant(R +)n.
In the ReLU-based setup considered in BDH-GPU, an appropriate functionbis plausibly easy to learn. LSH is a
‘sharp-boundary’ technique, well-suited for findingk-nearest-neighbors of a queried vector in a set of keys. Hence,
the class of attention affinity functions, naturally expressible using BDH-GPU, also includes such ‘sharp’ functions.
Attention in the positive concept space of language and reasoning.BDH-GPU uses the positive orthant(R +)n
as its latent space for representing combinations of concepts in its activation vectors. Attention keys and queries are
prepared entirely in this positive orthant.
36
When representing a task of reasoning or language inference in a high-dimensional space, positive activation vectors
in(R +)n have a natural interpretation of convex combinations of concepts. Such convex combinations of concepts
may represent both semantically connected concepts (“bags-of-concepts”), and mixed states of uncertainty between
unconnected concepts. In this interpretation, a positive vector is considered as a state of certain knowledge when
its L1-norm and L2-norm align closely. Note that for a (normalized) probability vector, the only vectors for which
L1-norm and L2-norm coincide precisely are distributions concentrated on a single coordinate.
Linear Attention of BDH-GPU is capable of amplifying very small differences between keys in the L1-norm when
matching queries to keys. Consider, for instance, two probability distribution vectorsx 1, x2 ∈(R +)n, wherex 1 =
(α, 1−α
n−1 , 1−α
n−1 , . . . , 1−α
n−1)andx 2 = ( 1−α
n−1 , α, 1−α
n−1 , . . . , 1−α
n−1), for some0< α <1. Now, vectorsx 1 andx 2 almost
coincide when treated as probability distributions,∥x1 −x2∥1 =O(α) =∥x 1 −x2∥TVD. However, they are extremely
different when considered as keys for the Linear Attention mechanism, withx 1 showing very weak affinity tox 2:
x1T x2 =O(α −2n−1)x1T x1.
Observation 8.In key-query matching, the Linear Attention mechanism of BDH-GPU is able to separate positive
keys which are close in the L1-norm, strongly amplifying L1-norm differences of activation vectors.
This mechanism can be treated as complementary to the propagation dynamics of positive activations in the feed-
forward network, discussed in Section 5.3.
Natural support for long context.There is no bound on context length in BDH-GPU, so the actualt= ˜Ω(√n)to
t= ˜O(n)“equally important facts” that a BDH-GPU model can distinguish in each layer in view of Claim 7 do not
have to correspond to the latestt“facts” seen in context. For example, if, for some layerl, mechanisms from lower
layers deem a given entry to be irrelevant for layerl, and provide an extremely weak attention ‘value’ for this layer,
and this key-value entry is effectively seen as omitted. This mechanism corresponds to weaker signalsyin a layer
which needs to take no action on a given input, e.g., does not have to remember it (cf. Fig. 14). Indeed, empirically
we observe progressive de-noising of state in the higher layers, with only small fractions of input tokens requiring
significant key-value state update in the middle layers across the entire spectrum of stateρof neurons.
As a result, the middle and higher layers of BDH-GPU may, in principle, have unbounded look-back on context.
Nonetheless, as context lengthtincreases, we find that damping of historical signals over long sequences is necessary
in BDH-GPU to avoid overwhelming the model with noise from stale context. For the vanilla version of the archi-
tecture, we found that RoPE combined with ALiBi provide a sufficient remedy, and model performance improves as
context length increases. More advanced techniques for BDH-GPU, related to selective forgetting, state compression,
or other forms of state optimization, can also be added to the architecture.
6.2 Micro-interpretation of attention in BDH-GPU
BDH maintains its state in then×nmatrixσthat has a clear interpretation as synapse weights that connect neurons
(cf. Section 2). On the other hand, BDH-GPU’s stateρis an×dmatrix. To perform the analysis for BDH-GPU, in
this section we recoverσfrom the relation:
σt−1,l =
X
τ <t
yτ,l−1xτ,l
T U t−τ (16)
We first analyze the neuron relationship graph encoded by matrixσ. As explained in Section 2.2,σcan be interpreted
as a graph of context dependent implications betweenxandy. We compute theσmatrix for 0-th head at layer 5 of an
8-th layer network trained on Europarl translation corpus (Koehn, 2005) (we provide more details in Appendix B.3).
We filter out negative entries which are introduced by the RoPE positional embeddings (Su et al., 2024) and enforce
a small positive threshold on remaining values to further sparsify the network structure. We plot the histograms of
neuron in- and out-degrees, unraveling a scale-free network structure.
Encouraged by the emergent network structure, we have identified a few synapses that are activated at recognizable
concepts, we show examples in the next section.
6.3 Empirical findings: monosemantic synapses
We have identified in theσmatrix entries (synapses) that show activity whenever a currency name or country name,
both frequently occurring in the Euro-parliament transcripts, is present in the processed sentence. We have identified
the synapses by searching for entries inσthat have predictive power at separating sentences containing a concept
from contrast sentences. We present a few examples in Figure 12. We note that the synapses strength changes abruptly
after words that are related to each concept. The same synapse is activated for concepts in both French and English
37
Figure 11: BDH’s stateσencodes neuron connections as a scale-free graph showing clear heavy-tailed (power-law-like) degree
distribution.
sentences, even when the words used are different (e.g. “livre sterling” vs “British Pound”). Synapse selectivity to a
semantic context stems directly from sparsity of neuron activations as shown in Fig. 13.
To confirm the selectivity of the synapses, we have generated, using ChatGPT, 50 sentences relating to European
currencies, and another set of 50 sentences speaking about European politics, but not mentioning currencies. A one-
sided Mann–Whitney U test revealed that sentences relating to currencies received significantly higher “Currency
synapse” values than those without the currency concept (U= 2368withU opt = 2500,p <10 −14). The rank-biserial
correlation was0.86, further confirming association between Currency concept presence and synapse value.
6.4 Empirical findings: sparse neuron activations
Sparsity of signals is often a prerequisite to their interpretability. In section 6.3 we have shown that BDH has monose-
mantic synapses, selectively activated by occurrences of specific concepts. In this section, we experimentally show
that neuron activity correlates with signal predictability: fewer neurons are active, or equivalently, layer activations
become sparser, for more predictable input signals.
We have trained a BDH-GPU model withn= 65536neurons,d= 256,L= 4layers, and tokenization on letters
of the Latin alphabet, to perform a single synthetic next-token prediction task. The input sequence started with a
fixed13-letter warm-up sequence, followed by8repetitions of an8-letter random word (“fact”), with the same pattern
repeating every13 + 8·8 = 77letters. In Fig. 14, we show neuron activity patterns. We can notice that neurons
in higher layers are active during warm-up and fact introduction, then become quiet. We then group neurons by
their RoPE frequencies and find that largest difference of activity during memorization and repetition is shown by the
slow-acting neuron population.
From a biological standpoint, sparse and surprisal-driven neuron activation lowers energy consumption — despite
fluctuations in low level percepts (in the experiment tokens are changing at every timestep), neurons in higher layers
are inactive and do not expand energy. From a Deep Learning perspective, it has been recently shown that input
complexity is related to predictability of internal representations of Transformers (Herrmann et al., 2025). BDH
makes this very explicit and does not require a separate prediction network: the predictable steady-state consists of
zero activations, and input complexity entails neuronal activity. This suggests that BDH, natively, at a neuron level,
implements mechanisms reminiscent of adaptive computation time (Graves, 2017) and conditional computation (Cho
and Bengio, 2014; Shazeer et al., 2017), used in modern Transformers to lower computational effort during inference.
Finally, sparse activation vectors in BDH imply that potentiation of specific synapses occurs rarely during inference.
This is useful from the point of view of interpretability, noise reduction in Linear Attention state, and opens the door
to simplified and compressed representations, notably for state and for gradient backpropagation DAG’s.
38
Figure 12: Evolution of values set by BDH-GPU on 2 specific synapses which we have named (following their interpretation) as
“currency synapse” and “country synapse”, relating to concepts naturally present in European Parliament transcripts on which the
model was trained. We can notice that mentions of country or currency names result in an increase of the respective synapse value,
indicating a stronger presence of the concept in the context. Moreover, the synapses consistently became activated in both French
and English, confirming the (notice how it reacts both to “British Pound” and “livre sterling”).
For visual clarity, we indicate changes that clear a small threshold with the∗character (the changes in activity when the system is
processing the translation of a source sentence tend to be small).
39
Figure 13: Sparse updates to synapses related to meaningful concepts stem from sparse neuronal activations. BDH-GPU maintains
in its recurrent state a “currency synapse” (a concept naturally present in the Europarl corpus, see also Fig. 12). The synapse is
updated using a Hebbian learning rule when activity inyactivations at a preceding layer (4 in the example) leads to firing of neuron
xin the next layer (5).
(a)
 (b)
Figure 14: Neurons in BDH-GPU are less active (signal is sparser) when the input is predictable. The input sequence started with
a fixed13-letter warm-up sequence, followed by8repetitions of an8-letter random word (“fact”), with the same pattern repeating
every13 + 8·8 = 77letters. (a) Fraction of neurons with non-zero entryy t,l in different layersl, with fact memorization effect
noted through increased activation level in layer2. The activation in layer2has4.0%−7.5%non-zero entries during memorization
and approximately2.5%non-zero entries during repetition. (b) Detailed breakup of activation sparsity in layer2, with neurons
bucketed into equal fractions by their RoPE phase: freq0∈[1,4], freq1∈[4,16], freq2∈[16,64],. . ., freq7∈[16384,65536].
The slow-acting half of the neuron population (freq4−freq7) exhibits the largest amplitude ratio between peak activation during
memorization and repetition phases.
40
7 Playing with the Hatchling
7.1 Model merging: concatenating two models
Updating models with up-to-date knowledge and expanding models knowledge-base will become crucial in practical
applications of AI. On possible solution is model composability, potentially allowing building of larger models by
assembling a number of smaller, specialized models into a larger, more powerful one. A natural hope for such a
system would be the achievement of ”more is different than a sum of its part” effect. In the following experiment
we are showing that doing so is relatively straight forward with BDH-GPU. This is because BDH-GPU can be scaled
by varying only the number of neuronsn. In this section we explore whether we can create larger models directly
concatenating smaller models trained on disjoint subsets of data. Details in Appendix B.4. We have experimented
with the following simple model merging procedure:
1. Train a base model on a chosen language pair. In the experiment we have used English-Spanish (En-Es)
translation data, and have trained a model withn= 24576neurons (19M parameters).
2. Clone the base model and continue training on two datasets: English-French (En-Fr) and English-Portuguese
(En-Pt).
3. We then merge the weights of the En-Fr and En-Pt models to create a new En-FrPt model withn= 24576·2 =
49152neurons (38M parameters). To create the merged model we:
(a) concatenate all parameter tensors that have an ‘n’ dimension (e.g.D y,D x,E, RoPE frequency buffers)
along theirndimension,
(b) average all other parameters (e.g. token embeddings and token prediction weights).
To validate the hypothesis that direct model merging is feasible, we report all results on the merged model
without any subsequent training or finetuning. However, we have verified that the merged model quickly
improves when trained on all language pairs.
4. After each stage we evaluate the models on all involved language pairs: En-Es, En-Fr, En-Pt, regardless of
the data seen by the model up to this stage.
We report quantitative results in Table 2 and show qualitative results of merged model operation in Figure 15. The
merged model shows human-like degradation of operation: while it retained the capability to generate and translate
into English, it has lost the ability to generate proper text in Spanish, French, or Portuguese, mixing words and
grammatical constructs. We have verified that a small amount of training on all language pairs restore the model’s
proficiency in Spanish, French and Portuguese. However, we decided to report on the behavior of the merged model
without any subsequent tuning to highlight the possibilities of model engineering offered by the large and sparse
working dimensionnof BDH-GPU.
The BDH-GPU model merging experiment has shown that when the model latent space promotes concept disentan-
gling (c.f. Section 6.2 on monosemanticity) then it is feasible to directly compose concepts in this space, e.g. by
concatenation of weights from different models. This feature of the BDH architecture allows us to see the models as
composable computer programs with emergent properties.
Translation into English Translation from English
Model:Es→En Fr→En Pt→En En→Es En→Fr En→Pt
1: Base En-Es 0.36 0.77 0.64 0.35 2.21 2.27
2: Base (1) tuned on En-Fr 0.58 0.36 0.68 2.57 0.31 2.54
3: Base (1) tuned on En-Pt 0.44 0.76 0.34 1.79 2.20 0.33
4: Merged (2∥3) 0.43 0.40 0.39 1.45 0.77 0.86
Table 2: Validation next token prediction losses (lower is better) of translation models trained on different language pairs and then
merged. We evaluate each model on En-Es, En-Fr, En-Pt language pairs separately. We can see that the base model can translate
between English and Spanish, while on En-Fr and En-Pt tasks it falls back on perplexities of an unconditional English language
model (loss about0.65) and can’t generate proper French or Portuguese. After tuning on French or Portuguese the model learns to
translate between respectively English and French or English and Portuguese, while somewhat retaining the capacity to translate
Spanish to English and losing the capability to translate English to Spanish. The merged model can translate Spanish, French,
and Portuguese to English, however it mixes these three languages when asked to translate from English. This is consistent with
qualitative results shown in Figure 15.
41
<F:es>Esta es una afirmación clara.<T:en> | In this clarification, it is a clear statement.
<F:es>Esta es una afirmación clara.<T:en> | It is a clear statement.
<F:es>Esta es una afirmación clara.<T:en> | That is a clear affirmation.
<F:fr>C’est une déclaration claire.<T:en> | This is a clear statement.
<F:fr>C’est une déclaration claire.<T:en> | This is a clear declaration.
<F:fr>C’est une déclaration claire.<T:en> | It is a clear declaration.
<F:pt>Esta é uma afirmação clara.<T:en> | That is a clear statement.
<F:pt>Esta é uma afirmação clara.<T:en> | This is a clear statement.
<F:pt>Esta é uma afirmação clara.<T:en> | This is a clear assertion.
(a) Sampled translations from Spanish, French, and Portuguese to English. The vertical bar|delimits the prompt from model output. Each translation was sampled
three times to show the model’s consistency.
<F:en>This is a clear statement.<T:es> | Ce récent statement está clarificative.
<F:en>This is a clear statement.<T:es> | Il se revela exact.
<F:en>This is a clear statement.<T:es> | Constato que cette déclaration était monstruosa.
<F:en>This is a clear statement.<T:fr> | Está de noto uma déclaration.
<F:en>This is a clear statement.<T:fr> | Ce sont une declaration clare.
<F:en>This is a clear statement.<T:fr> | C’está uma declaração clare.
<F:en>This is a clear statement.<T:pt> | Esta declaração étangling état está clara.
<F:en>This is a clear statement.<T:pt> | Istambigna de contence.
<F:en>This is a clear statement.<T:pt> | C’estaté clarification é clara!.
(b) Sampled translations from English into Spanish, French, and Portuguese. The model mixes the three languages, though the meaning of the source sentence seems to
have been preserved.
<F:en> | The European Convention on Human Rights has been set up in 1992, when it applied the Convention in 1
<F:es> | Naturalment, nos deputés de toda autre groupe southern Italians, notariously engaged in the discuss
<F:fr> | (ES) Mr President, aproveit de montrar ma satisfaction por a surprise de la parte de Milan, et parti
<F:pt> | (FI) Mr President, todos outreaches, mesures on ways and means of promoting the economic development
(c) Language-conditional samples. Consistently with the translation experiment in (a) and (b) above, the model properly generates English, and mixes Spanish, French,
and Portuguese, sometimes slipping into English.
<F:es>Mi lingua project nominat Romanid esed publicat ja in may de pasat an.<T:en> |
A Lingue nominated Project is Romanian published in Romanian pasta year.
<F:fr>Mi lingua project nominat Romanid esed publicat ja in may de pasat an.<T:en> |
My language in Project Nomina is Romani esedi publish posted peas in maybe the same year.
<F:pt>Mi lingua project nominat Romanid esed publicat ja in may de pasat an.<T:en> |
I have been a Latva project nomination Romanim esede publica published in May of the postal service and the service provider.
d) Attempts of the model to translate a sentence in Romanid (2025), a zonal auxiliary language for speakers of Romance languages naturalistic constructed language
intended to be intuitively understandable to speakers of Romance languages. The English translation of the prompt is: “My language project called Romanid was
published already in May of last year”. The model is able to pick up some of the meaning of the prompt.
Figure 15: Conditional and unconditional samples generated from a English-Spanish-French-Portuguese translation model created
by direct concatenation of parameters of models trained on distinct language pairs.
7.2 Training without backpropagation through time
Sparsity of synapse activations in BDH opens the door to efficient approximations to backpropagation through time.
The main intuition is that we only need to remember when a synapse has changed, and thei, jcoordinates of the
synapse implicitly encode which neurons were active and should take part in error signal backpropagation.
In this section we report results of preliminary experiments on the impact of removal of backpropagation through time
on model performance. For the PyTorch implementation in Appendix E, this corresponds to ‘detach’-ing variablesK
andVin the implementation of theLinearAttentionclass.
In particular, we found that such a model, trained without any backpropagation through time, retained some ability to
model language, but lost the ability to match concepts between different languages during translation. For translation
tasks like those presented in Table 2, loss values for English increased from a loss level of approximately0.65for an
unconditional English language model (trained with backpropagation over time), to loss of approximately0.75−1.05
for a model trained without backpropagation over time, depending on model variant, regardless of whether English
was the source or target language in translation. No significant difficulties were encountered during training when
crossing the barrier of the letter-bigram language model, at loss value2.4.
Beyond side-effects of the general design, we did not optimize the BDH-GPU model for suitability of training without
backpropagation. We consider this architecture to be a good starting point for bootstrapping further investigations in
this direction.
42
8 Conclusions
8.1 Takeaways for model engineering
This paper leads up to a new class of language and reasoning models which eliminate architecture nonuniformities,
notably in terms of scaling for model size, and handling of time scales of inference.
The BDH-GPU architecture introduced in this paper opens the following opportunities:
1.New ways of scaling models for time and size.BDH-GPU is a state-space model which scales for size in
one large dimensionn(neurons in this dimension are indexed by RoPE oscillator frequency). Subject to
appropriate sharding, this also leads to a desirable form of locality: important data is located just next to the
sites at which it is being processed. This minimizes communication, and eliminates the most painful of all
bottlenecks for reasoning models during inference: memory-to-core bandwidth.
2.Faster model iteration.During training and inference alike, BDH-GPU provides insight into parameter and
state spaces of the model which allows for easy and direct evaluation of model health and performance,
notably, through sparsity-related measures and through aggregates and statistics on the large pool of homo-
geneous neurons, even for relatively small models. Attention and parametric layers alike operate on the same
neuron dimension (‘concept dimension’).
3.Direct explainability of model state.Elements of state of BDH-GPU are directly localized at neuron pairs,
allowing for a micro-interpretation of the hidden state of the model.
4.New opportunities for ‘model surgery’.The BDH-GPU architecture is, in principle, amenable to direct com-
posability of model weights in a way resemblant of composability of programs. This concerns the potential
both the direct composition of separately trained model parts, as well as ‘surgery’ of parameter spaces of
models, by inserting fragments of manually programmed protocols into machine-learned code.
8.2 Implications for brain science
We have obtained a micro-foundational description of attention for artificial language and reasoning models, expressed
in a framework of local graph dynamics. This has been found to be consistent with the effects observed for the same
function ofattention for language and reasoningin the brain. By introducing a translation layer based on similarity
of function between the artificial and biological planes, for blocks of feed-forward neural networks and attention
mechanisms, our work points to the following hypothesis:complex systems effects which are observed in the brain,
around modular scale-free network structure, synaptic plasticity, and Hebbian learning arose from its core purpose —
doing reasoning — andnotfrom any specific longer-term training dynamics which the brain applies.
We have exhibited how a general attention mechanism can be efficiently implemented as an artificial neuronal system
with spiking neurons and synapse plasticity. More formally, we first describe the class of local interaction dynamics
which any systemplausibly needsto implement attention mechanisms. We then confirmed that the edge-reweighting
rule issufficientto allow a certain artificial Language Model (BDH-GPU) to operate at least at the level of the Trans-
former. For an artificial network, the edge-reweighting rule intuitively describes the interaction between two artificial
neurons exhibiting rapid state-change behavior, and one synaptic neuron interconnection element exhibiting plasticity
as shown in Fig. 13.
More broadly, this work may potentially serve to support efforts aiming to isolate, from among the many extremely
complex electrochemical patterns and signal dynamics occurring in the brain, those that are crucial forsolving tasks
in-context (based on attention), from those that potentially serve other purposes, such as transfer of information from
short-term memory to long-term memory, or long-term improvement of brain function (learning).
How this work helps with axiomatization of learning theory in the brain.Attempts to understand the brain,
starting from the perspective of longer time scales of training, have proved extremely challenging, defying progress.
This paper pin-points attention-based reasoning at shorter time scales as ‘the other end of the string’, and hints how,
from here, untangling the entire story will plausibly be easier.
For natural systems undergoing continuous learning, the time scales to look at are: language function and reason-
ing (chain-of-thought inference), then short-to-long memory transfer from state to network weights, adaptation of
structure: changes to interconnections, and finally, changes to neuron nodes.
For long time scales, this reduces the question of finding supervised training dynamics form the most general case,
to a specific class of local dynamics: an interaction kernel performing ‘edge-reweighting’ rules. As these rules ap-
43
pear fundamental to logical inference and biochemical processes alike, its universality in processes that the brain is
responsible for is plausible also beyond the realm of language-based reasoning.
From a systems perspective, we arrive at the following possible explanation.The brain generally tries to be lazy in
terms of energy expense, and does things as late as it can. Only reasoning needs to happen close to a critical regime,
because it involves executing a real-time program which needs to be responsive, since the life and success of the
biological organism depends on it. Then, for a certain time, which may be minutes for humans, the brain has enough
synapses in it to represent (almost) all useful information it needs for reasoning, decision-making, etc. — all stored
in short-term state, at synapses (and/or neurons). Some of the neuron activations which the brain performs at this
time scale represent ‘gradients of state’ — the gradients of in-context learning, passed on to modify synapse strength,
in a weight-update process. As time goes by, the system runs out of state space. Then, memory processes work to
iron things out, preserving in more permanent neuron connection weights and graph structure the elements of state
that have been reinforced by feedback signals. Overall, there are fewer and fewer things that need to be remembered
across progressively longer time scales. However, this entire memory process is, plausibly, subsidiary to the definition
of the dynamics of reasoning and the synaptic dynamics of state that we discuss in this paper. In other words, the
best form of description of the relaxation from state into longer-term memory follows from the specific kernel of the
reasoning dynamics, such as the edge-reweighting kernel.
As for the ratio of time scales (measured in tokens for language), we can estimate that the time lapse after which
harmonizing state with a memory process becomes important is of about the same order of magnitude as the average
time between ‘writes’ (significant transmission increases) for individual synaptic elements (see e.g. Fig. 14). In our
models, this time is lower-bounded by the inverse of sparsity of the vectory, i.e.,1/ρ≈1/5% = 20tokens, but
it could be much larger for larger systems; we also do not force it in any way to be sparser during training. During
training with backpropagation, if the backpropagation windowTis short enough,T <1/ρtokens, we can plausibly
assume that a synapse changes state only once in that window (and is used multiple times), hence the DAG of gradient
backwards propagation is much more direct to embed within the system graph. Backpropagation is then a question
of ‘routing’ gradients in the neuron communication graph, and not one of disentangling them. All natural training
approaches, whether based on backpropagation, or any more direct form of relaxation ‘from state into weights’, appear
to bottleneck on the amount of available state space on synapses, becoming necessary at aboutT∼1/ρby a simple
information-theoretic argument on state storage capacity.
Regardless of how much of this is an accurate description, and how much an intuition, at the very least, it appears we
may now have a way forward. Some part of the “global mystery” of learning in the brain can be reduced to a more
“localized problem” of state-to-operator transfer for some relatively compact form of state-space dynamics (i.e., one
specific local graph kernel). This change of perspective brings in both a completely new ‘problem landscape’ in which
to navigate towards a complete solution, as well as a set of new methods to use for the different types of graph structure
changes involved in learning, including approaches from distributed computing, evolving network theory, and graph
rewiring systems.
At this point, it seems one natural next step would be to ground the current discussion more deeply in findings of brain
science, to refine or simplify theactual kernelsused by brain reasoning (which was not the objective of this paper),
and potentially seek validation through experiment.
8.3 Societal impact
This paper is a voice in favor of bringing principled understanding to reasoning in Machine Learning. Axiomatic AI
provides an opportunity to reduce risks related to unpredictable behavior of AI models, and, to open or accelerate new
development directions. The subject matter which we consider here serves as a direct introduction to the most crucial
problem that lies ahead: controlling the behavior of autonomous AI reasoning models and AI systems as they progress
across time scales, from seconds to years.
44
Transformer (GPT2) BDH-GPU (n,d) BDH(n,∆) Brain models (reasoning and
language function)
Inference hardwareGPU, CPU GPU, CPU CPU, Sparse GPU kernels,
Neuromorphic
Brain and supporting systems
Model weights (predominant
location)
5tensors per layer (different
shapes)
3tensors per model (same shape
n×d)
Neuron-synapse graph:
connection topology, edge
weights
Neuron-synapse graph:
connection topology, edge
weights
Representation of attentionKV-cache tensor (not localized
at neurons)
n×dtensor for each layer
(localized at neurons)
Memory on synapse edge
weights
State memory through synapse
plasticity
Macro-description of attentionKey lookup data-structure,
key-value map
Key lookup data-structure,
key-value correlation matrix
Key lookup data-structure,
key-value correlation matrix
Not known
Micro-description of attentionNone Neuron-pair correlations in
context (transformed)
Neuron-pair correlations in
context
Strengthened or weakened
connections between neurons
based on context
Scaling for model sizeMultiple combinations of
dimensions, e.g. MLP scales
withD×D, scales separately
with context length
Uniform linear array ofn
particles in a mean-field
n-node graph modeln-node graph model with
evolving graph mechanisms
Distributed system
micro-architecture
Follows from compiled matrix
multiplication kernels,
non-uniform
Particles run identical local
kernels, communicating
O(d)-size messages through
mean-field, and storing local
state
Allnneuron nodes run identical
local kernels, communicating
over neuron-synapse graphs.
Some synapses act as memory
elements.
nneurons run local kernels and
communicate through a
network, using numerous signal
patterns and coding schemes.
Synapses act as memory
element.
Macro-expressiveness of
programs (approximation)
RASP-L, C-RASP RASP-L, C-RASP RASP-L, C-RASP (or superset) Unknown
Micro-expressiveness of
programs
Unknown Subset of BDH Probabilistic rule-based local
protocols. Micro-Inductive bias
interpreted as reasoning system
in a form of propositional logic.
Unknown
Emergence of structurePartially interpretable concept
layer (evidence of
monosemantic neurons for
important concepts)
Evidence of emergent network,
oscillator dynamics
Emergent network, oscillator
dynamics
Emergent network; oscillatory
effects; possible monosemantic
“grandfather neurons”
Activation vectorsDense activation; can be
subsampled or sparsified by
architecture modification
Positive vectors, sparse fresh
activation vectorsy
Positive vectors, sparse fresh
activation vectorsy
Sparse positive activation
vectors
Table 3: Comparison of properties of language and reasoning model architectures: the GPT2 Transformer, BDH-GPU, BDH, and brain models.
45
Acknowledgments
The authors thank David Sussillo, Navdeep Jaitly, and Emanuele Natale for insightful discussions on reasoning and
the brain, and for early feedback on this write-up. We also thank Samy Bengio for comments on the presentation.
We kindly acknowledge the support of all of the Pathway team, notably, Paweł Podhajski for his amazing help with
cluster setup, Victor Szczerba and Z Schwab for all discussions over coffee, and Kamil Piechowiak and Chris Ociepa
for constructive comments on the presentation. AK thanks Christos Papadimitriou for being the direct inspiration for
us to embark on this journey.
Author contributions
AK conceived the BDH and BDH-GPU architectures, conceived most of the theory, developed most of the model
source code, conceived and performed experiments on synapses, and wrote most of the paper.
PU contributed crucial elements of BDH-GPU architecture, contributed model and framework source code, contributed
to theoretical analysis, and performed experiments.
JCh led, designed, and oversaw methodology of experiments, led framework development, contributed major im-
provements to BDH-GPU architecture, contributed to the theory, implemented baselines, performed experiments, and
substantially redacted the paper.
ZS conceived the project, guided research directions, introduced particle-interaction interpretation, acted as final judge
in research decisions, and substantially redacted the paper.
MB optimized model source code, contributed framework source code, and performed experiments.
46
References
I. Abraham, D. Delling, A. V . Goldberg, and R. F. Werneck. A hub-based labeling algorithm for shortest paths in
road networks. In P. M. Pardalos and S. Rebennack, editors,Experimental Algorithms, pages 230–241, Berlin,
Heidelberg, 2011. Springer Berlin Heidelberg. ISBN 978-3-642-20662-7.
D. Achlioptas and F. Mcsherry. Fast computation of low-rank matrix approximations.J. ACM, 54(2):9–es, Apr. 2007.
ISSN 0004-5411. URLhttps://doi.org/10.1145/1219092.1219097.
D. Angluin, J. Aspnes, Z. Diamadi, M. J. Fischer, and R. Peralta. Computation in networks of passively mo-
bile finite-state sensors.Distributed Comput., 18(4):235–253, 2006. URLhttps://doi.org/10.1007/
s00446-005-0138-3.
J. Aspnes and E. Ruppert.An Introduction to Population Protocols, pages 97–120. Springer Berlin Heidelberg, Berlin,
Heidelberg, 2009. ISBN 978-3-540-89707-1. URLhttps://doi.org/10.1007/978-3-540-89707-1_5.
J. Ba, G. E. Hinton, V . Mnih, J. Z. Leibo, and C. Ionescu. Using fast weights to attend to the recent past.Advances in
neural information processing systems, 29, 2016a.
J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization, 2016b. URLhttps://arxiv.org/abs/1607.06450.
D. Bahdanau, K. Cho, and Y . Bengio. Neural machine translation by jointly learning to align and translate. In Y . Bengio
and Y . LeCun, editors,3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA,
May 7-9, 2015, Conference Track Proceedings, 2015. URLhttp://arxiv.org/abs/1409.0473.
H. B. Barlow. Single units and sensation: A neuron doctrine for perceptual psychology?Perception, 1(4):371–394,
1972. URLhttps://doi.org/10.1068/p010371. PMID: 4377168.
L. Becchetti, V . Bonifaci, and E. Natale. Pooling or sampling: Collective dynamics for electrical flow estimation. In
Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS ’18,
page 1576–1584, Richland, SC, 2018.
M. Beck, K. Pöppel, M. Spanring, A. Auer, O. Prudnikova, M. Kopp, G. Klambauer, J. Brandstetter, and S. Hochreiter.
xlstm: Extended long short-term memory.Advances in Neural Information Processing Systems, 37:107547–107603,
2024.
A. Ben-Kish, I. Zimerman, M. J. Mirza, J. Glass, L. Karlinsky, and R. Giryes. Overflow prevention enhances long-
context recurrent llms, 2025. URLhttps://arxiv.org/abs/2505.07793.
A. Björner, L. Lovász, and P. W. Shor. Chip-firing games on graphs.European Journal of Combinatorics, 12(4):
283–291, 1991. ISSN 0195-6698. URLhttps://doi.org/10.1016/S0195-6698(13)80111-4.
V . D. Blondel, J.-L. Guillaume, R. Lambiotte, and E. Lefebvre. Fast unfolding of communities in large networks.
Journal of Statistical Mechanics: Theory and Experiment, 2008(10):P10008, oct 2008. URLhttps://dx.doi.
org/10.1088/1742-5468/2008/10/P10008.
L. Boczkowski, A. Korman, and E. Natale. Minimizing message size in stochastic communication patterns: fast self-
stabilizing protocols with 3 bits.Distributed Comput., 32(3):173–191, 2019. URLhttps://doi.org/10.1007/
s00446-018-0330-x.
N. Bostrom.Superintelligence: Paths, Dangers, Strategies. Oxford University Press, Inc., USA, 1st edition, 2014.
ISBN 0199678111.
N. Brunel. Hebbian learning of context in recurrent neural networks.Neural Computation, 8(8):1677–1710, 1996.
URLhttps://ieeexplore.ieee.org/document/6796169.
J. Buckman, C. Gelada, and S. Zhang. Symmetric Power Transformers, 2024. URLhttps://manifestai.com/
articles/symmetric-power-transformers/.
S. Budzinskiy. When big data actually are low-rank, or entrywise approximation of certain function-generated matri-
ces, 2025. URLhttps://arxiv.org/abs/2407.03250.
H. Cairns. Some halting problems for abelian sandpiles are undecidable in dimension three.SIAM Journal on Discrete
Mathematics, 32(4):2636–2666, 2018. URLhttps://doi.org/10.1137/16M1091964.
Y . Chen, D. Doty, and Soloveichik. Deterministic function computation with chemical reaction networks.Nat Comput,
13:517–534, 2014. URLhttps://link.springer.com/article/10.1007/s11047-013-9393-6.
K. Cho and Y . Bengio. Exponentially increasing the capacity-to-computation ratio for conditional computation in deep
learning, 2014. URLhttps://arxiv.org/abs/1406.7362.
K. M. Choromanski, V . Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Q. Davis, A. Mohiuddin,
L. Kaiser, D. B. Belanger, L. J. Colwell, and A. Weller. Rethinking attention with performers. InInternational
Conference on Learning Representations, 2021. URLhttps://openreview.net/forum?id=Ua6zuk0WRH.
47
P. Christiano, J. A. Kelner, A. Madry, D. A. Spielman, and S.-H. Teng. Electrical flows, laplacian systems, and faster
approximation of maximum flow in undirected graphs. InProceedings of the Forty-Third Annual ACM Sympo-
sium on Theory of Computing, STOC ’11, page 273–282, New York, NY , USA, 2011. Association for Computing
Machinery. ISBN 9781450306911. URLhttps://doi.org/10.1145/1993636.1993674.
J. Czyzowicz, L. Gasieniec, A. Kosowski, E. Kranakis, P. G. Spirakis, and P. Uzna´nski. On convergence and threshold
properties of discrete lotka-volterra population protocols.J. Comput. Syst. Sci., 130:1–25, 2022. URLhttps:
//doi.org/10.1016/j.jcss.2022.06.002.
M. Dabagia, C. H. Papadimitriou, and S. S. Vempala. Computation with sequences of assemblies in a model of the
brain.Neural Computation, 37(1):193–233, 12 2024. ISSN 0899-7667. URLhttps://doi.org/10.1162/neco_
a_01720.
Z. Dai, Z. Yang, Y . Yang, J. G. Carbonell, Q. V . Le, and R. Salakhutdinov. Transformer-XL: Attentive language
models beyond a fixed-length context. In A. Korhonen, D. R. Traum, and L. Màrquez, editors,Proceedings of the
57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2,
2019, Volume 1: Long Papers, pages 2978–2988. Association for Computational Linguistics, 2019. doi: 10.18653/
V1/P19-1285. URLhttps://doi.org/10.18653/v1/p19-1285.
M. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and Łukasz Kaiser. Universal transformers, 2019. URLhttps:
//arxiv.org/abs/1807.03819.
S. Dolev.Self-stabilization. The MIT Press, 2000.
D. Doty, M. Eftekhari, L. Gasieniec, E. E. Severson, P. Uzna´nski, and G. Stachowiak. A time and space optimal stable
population protocol solving exact majority. In62nd IEEE Annual Symposium on Foundations of Computer Science,
FOCS 2021, Denver, CO, USA, February 7-10, 2022, pages 1044–1055. IEEE, 2021. URLhttps://doi.org/
10.1109/FOCS52979.2021.00104.
B. Dudek and A. Kosowski. Universal protocols for information dissemination using emergent signals. In I. Di-
akonikolas, D. Kempe, and M. Henzinger, editors,Proceedings of the 50th Annual ACM SIGACT Symposium on
Theory of Computing, STOC 2018, Los Angeles, CA, USA, June 25-29, 2018, pages 87–99. ACM, 2018. URL
https://doi.org/10.1145/3188745.3188818.
L. Emberson, B. Cottier, J. You, T. Adamczewski, and J.-S. Denain. LLM responses to benchmark questions are getting
longer over time, 2025. URLhttps://epoch.ai/data-insights/output-length. Accessed: 2025-07-25.
M. Feinberg.Foundations of Chemical Reaction Network Theory, volume 202 ofApplied Mathematical Sciences.
Springer, 2019.
P. Fraigniaud and G. Giakkoupis. On the searchability of small-world networks with arbitrary underlying structure. In
Proceedings of the Forty-Second ACM Symposium on Theory of Computing, STOC ’10, page 389–398, New York,
NY , USA, 2010. Association for Computing Machinery. ISBN 9781450300506. doi: 10.1145/1806689.1806744.
URLhttps://doi.org/10.1145/1806689.1806744.
A. Graves. Adaptive computation time for recurrent neural networks, 2017. URLhttps://arxiv.org/abs/1603.
08983.
A. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2024. URLhttps://arxiv.
org/abs/2312.00752.
D. Haziza, T. Chou, D. Choudhary, L. Wehrstedt, F. Massa, J. Yu, G. Jeong, S. Rao, P. Labatut, and J. Cai. Accelerating
transformer inference and training with 2:4 activation sparsity, 2025. URLhttps://arxiv.org/abs/2503.
16672.
B. J. He, J. M. Zempel, A. Z. Snyder, and M. E. Raichle. The temporal structures and functional significance of
scale-free brain activity.Neuron, 66(3):353–369, 2010. ISSN 0896-6273. URLhttps://doi.org/10.1016/j.
neuron.2010.04.020.
D. O. Hebb.Organization of behavior.New York: Wiley & Sons, 1949.
V . Herrmann, R. Csordás, and J. Schmidhuber. Measuring in-context computation complexity via hidden state predic-
tion, 2025. URLhttps://arxiv.org/abs/2503.13431.
D. Hilbert. Mathematical problems.Bull. AMS, 8(10):437–479, 1902. ISSN 0002-9904 (print), 1936-881X (elec-
tronic). URLhttps://doi.org/10.1090/S0002-9904-1902-00923-3. English translation of Hilbert’s fa-
mous list of 23 important problems in mathematics for the 20th Century.
G. E. Hinton. What kind of a graphical model is the brain? InProceedings of the 19th International Joint Con-
ference on Artificial Intelligence, IJCAI’05, page 1765–1775, San Francisco, CA, USA, 2005. Morgan Kaufmann
Publishers Inc.
48
G. E. Hinton and D. C. Plaut. Using fast weights to deblur old memories. InProceedings of the ninth annual conference
of the Cognitive Science Society, pages 177–186, 1987.
J. Hirvonen and J. Suomela.Distributed Algorithms 2020 — the book. Aalto University, 2025. URLhttps://
jukkasuomela.fi/da2020/da2020.pdf.
S. Hochreiter and J. Schmidhuber. Long short-term memory.Neural Comput., 9(8):1735–1780, Nov. 1997. ISSN
0899-7667. URLhttps://doi.org/10.1162/neco.1997.9.8.1735.
J. Hofbauer and K. Sigmund.Evolutionary Games and Population Dynamics. Cambridge University Press, 1998.
URLhttps://www.cambridge.org/core/books/evolutionary-games-and-population-dynamics/
A8D94EBE6A16837E7CB3CED24E1948F8.
P. W. Holland, K. B. Laskey, and S. Leinhardt. Stochastic blockmodels: First steps.Social Networks, 5
(2):109–137, 1983. ISSN 0378-8733. URLhttps://www.sciencedirect.com/science/article/pii/
0378873383900217.
E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large
language models, 2021. URLhttps://arxiv.org/abs/2106.09685.
X. Huang, A. Yang, S. Bhattamishra, Y . Sarrof, A. Krebs, H. Zhou, P. Nakkiran, and M. Hahn. A formal framework
for understanding length generalization in transformers. InThe Thirteenth International Conference on Learning
Representations, 2025. URLhttps://openreview.net/forum?id=U49N5V51rU.
F. Jabr and A. Rothschild. How brainless slime molds redefine intelligence.Nature, 7(1), 2012.
A. Jojic, Z. Wang, and N. Jojic. Gpt is becoming a turing machine: Here are some ways to program it.arXiv preprint
arXiv:2303.14310, 2023.
A. Kalev and I. Hen. Feynman path integrals for discrete-variable systems: Walks on hamiltonian graphs.Phys. Rev.
Res., 7:013220, Feb 2025. URLhttps://link.aps.org/doi/10.1103/PhysRevResearch.7.013220.
A. Karpathy. nanoGPT.https://github.com/karpathy/nanoGPT, 2024.
B. Karrer and M. E. J. Newman. Stochastic blockmodels and community structure in networks.Phys. Rev. E, 83:
016107, Jan 2011. URLhttps://link.aps.org/doi/10.1103/PhysRevE.83.016107.
A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Transformers are RNNs: Fast autoregressive transformers with
linear attention. In H. D. III and A. Singh, editors,Proceedings of the 37th International Conference on Machine
Learning, volume 119 ofProceedings of Machine Learning Research, pages 5156–5165. PMLR, 13–18 Jul 2020.
URLhttps://proceedings.mlr.press/v119/katharopoulos20a.html.
D. Kempe, J. Kleinberg, and E. Tardos. Maximizing the spread of influence through a social network. InKDD ’03:
Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining, pages
137–146, New York, NY , USA, 2003. ACM Press. ISBN 1-58113-737-0. URLhttps://doi.acm.org/10.
1145/956750.956769.
N. Kitaev, Ł. Kaiser, and A. Levskaya. Reformer: The efficient transformer. In8th International Conference on
Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL
https://openreview.net/forum?id=rkgNKkHtvB.
P. Koehn. Europarl: A parallel corpus for statistical machine translation. InProceedings of Machine Translation
Summit X: Papers, pages 79–86, Phuket, Thailand, Sept. 13-15 2005. URLhttps://aclanthology.org/2005.
mtsummit-papers.11/.
A. Kosowski and P. Uzna´nski. Population protocols are fast. In C. Newport and I. Keidar, editors,Proceedings of the
2018 ACM Symposium on Principles of Distributed Computing, PODC 2018, Egham, United Kingdom, July 23-27,
2018, pages 475–477. ACM, 2018. URLhttps://arxiv.org/abs/1802.06872.
R. Krauthgamer and S. Sapir. Comparison of matrix norm sparsification.Algorithmica, 85(12):3957–3972, 2023.
URLhttps://doi.org/10.1007/s00453-023-01172-6.
A. Kumar, L. Owen, N. R. Chowdhury, and F. Güra. ZClip: Adaptive spike mitigation for LLM pre-training, 2025.
URLhttps://arxiv.org/abs/2504.02507.
Y . LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27.Open Review, 62(1):1–62,
2022.
Y . LeCun, Y . Bengio, and G. Hinton. Deep learning.Nature, 521(7553):436–444, 2015.
J. Lee, Y . Bahri, R. Novak, S. S. Schoenholz, J. Pennington, and J. Sohl-Dickstein. Deep neural networks as gaussian
processes.arXiv preprint arXiv:1711.00165, 2017.
49
A. C. Lin, A. M. Bygrave, A. De Calignon, T. Lee, and G. Miesenböck. Sparse, decorrelated odor coding in the
mushroom body enhances learned odor discrimination.Nature neuroscience, 17(4):559–568, 2014.
H. Lin and S. Jegelka. Resnet with one-neuron hidden layers is a universal approximator.Advances in neural infor-
mation processing systems, 31, 2018.
K. Liu, J. Gao, and K. Chen. Scaling up the state size of RNN LLMs for long-context scenarios. In W. Che, J. Nabende,
E. Shutova, and M. T. Pilehvar, editors,Proceedings of the 63rd Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages 11516–11529, Vienna, Austria, July 2025. Association for Com-
putational Linguistics. ISBN 979-8-89176-251-0. URLhttps://aclanthology.org/2025.acl-long.564/.
I. Loshchilov and F. Hutter. Decoupled weight decay regularization, 2019. URLhttps://arxiv.org/abs/1711.
05101.
V . Mante, D. Sussillo, K. V . Shenoy, and W. T. Newsome. Context-dependent computation by recurrent dynamics in
prefrontal cortex.nature, 503(7474):78–84, 2013.
L. Massoulié. Community detection thresholds and the weak ramanujan property. InProceedings of the Forty-
Sixth Annual ACM Symposium on Theory of Computing, STOC ’14, page 694–703, New York, NY , USA, 2014.
Association for Computing Machinery. ISBN 9781450327107. URLhttps://doi.org/10.1145/2591796.
2591857.
W. S. McCulloch and W. Pitts. A logical calculus of the ideas immanent in nervous activity.The Bulletin of Mathe-
matical Biophysics, 5(4):115–133, 1943.
F. McSherry, D. G. Murray, R. Isaacs, and M. Isard. Differential dataflow. InSixth Biennial Conference on Innovative
Data Systems Research, CIDR 2013, Asilomar, CA, USA, January 6-9, 2013, Online Proceedings. www.cidrdb.org,
2013. URLhttps://cidrdb.org/cidr2013/Papers/CIDR13_Paper111.pdf.
W. Merrill and A. Sabharwal. The expressive power of transformers with chain of thought. InThe Twelfth International
Conference on Learning Representations, 2024. URLhttps://openreview.net/forum?id=NjNGlPh8Wh.
T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and
their compositionality.Advances in neural information processing systems, 26, 2013.
D. Mitropolsky and C. H. Papadimitriou. Simulated language acquisition in a biologically realistic model of the brain.
bioRxiv, 2025. doi: 10.1101/2025.07.15.664996. URLhttps://www.biorxiv.org/content/early/2025/
07/19/2025.07.15.664996.
Y . Mohsenzadeh, C. Mullin, B. Lahner, and A. Oliva. Emergence of visual center-periphery spatial organization in
deep convolutional neural networks.Scientific Reports, 10(1):4638, 2020. URLhttps://doi.org/10.1038/
s41598-020-61409-0.
V . Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. InProceedings of the 27th
international conference on machine learning (ICML-10), pages 807–814, 2010.
R. M. Neal.Bayesian learning for neural networks, volume 118. Springer Science & Business Media, 2012.
J. v. Neumann.The computer and the brain. Yale University Press, USA, 1958. ISBN 0300007930.
M. E. J. Newman. Modularity and community structure in networks.Proceedings of the National Academy of Sciences,
103(23):8577–8582, 2006. doi: 10.1073/pnas.0601602103. URLhttps://www.pnas.org/doi/abs/10.1073/
pnas.0601602103.
B. A. Olshausen. (ed.), The Brain and Computation — Simons Institute Program, Berkeley, 2018. URLhttps:
//simons.berkeley.edu/programs/brain-computation.
B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: A strategy employed by V1?Vi-
sion Research, 37(23):3311–3325, 1997. ISSN 0042-6989. URLhttps://doi.org/10.1016/S0042-6989(97)
00169-7.
C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y . Bai, A. Chen,
T. Conerly, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, S. Johnston, A. Jones, J. Kernion, L. Lovitt,
K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah. In-context learning and
induction heads.Transformer Circuits Thread, 2022. URLhttps://transformer-circuits.pub/2022/
in-context-learning-and-induction-heads/index.html.
S. Ostmeier, B. Axelrod, M. Varma, M. E. Moseley, A. Chaudhari, and C. Langlotz. LieRE: Lie rotational positional
encodings, 2025. URLhttps://arxiv.org/abs/2406.10322.
C. H. Papadimitriou, S. S. Vempala, D. Mitropolsky, M. Collins, and W. Maass. Brain computation by assemblies
of neurons.Proceedings of the National Academy of Sciences, 117(25):14464–14472, 2020. URLhttps://www.
pnas.org/doi/abs/10.1073/pnas.2001893117.
50
A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al.
Pytorch: An imperative style, high-performance deep learning library.Advances in neural information processing
systems, 32, 2019.
D. Peleg.Distributed Computing: A Locality-Sensitive Approach. Society for Industrial and Applied Mathematics,
2000. URLhttps://epubs.siam.org/doi/abs/10.1137/1.9780898719772.
J. Pérez, P. Barceló, and J. Marinkovic. Attention is turing complete.J. Mach. Learn. Res., 22(1), Jan. 2021. ISSN
1532-4435.
O. Press, N. A. Smith, and M. Lewis. Train short, test long: Attention with linear biases enables input length extrapo-
lation, 2022. URLhttps://arxiv.org/abs/2108.12409.
A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised multitask
learners.OpenAI blog, 1(8):9, 2019.
L. T. Rolla. Activated random walks onZ d.Probability Surveys, 17, Jan. 2020. ISSN 1549-5787. URLhttps:
//dx.doi.org/10.1214/19-PS339.
Romanid. Wikipedia, the free encyclopedia, 2025. URLhttps://en.wikipedia.org/w/index.php?title=
Romanid&oldid=1275565870. [Online; accessed 24-July-2025].
D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating errors.Nature, 323
(6088):533–536, 1986.
T. K. Rusch and D. Rus. Oscillatory state-space models. InThe Thirteenth International Conference on Learning
Representations, 2025. URLhttps://openreview.net/forum?id=GRMfXcAAFh.
S. Sabour, N. Frosst, and G. E. Hinton. Dynamic routing between capsules.Advances in neural information processing
systems, 30, 2017.
J. Schmidhuber. Reducing the ratio between learning complexity and number of time varying variables in fully
recurrent nets. InInternational Conference on Artificial Neural Networks, pages 460–463. Springer, 1993.
N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously large neural networks:
The sparsely-gated mixture-of-experts layer, 2017. URLhttps://arxiv.org/abs/1701.06538.
Z. Shen, H. Yang, and S. Zhang. Optimal approximation rate of ReLU networks in terms of width and depth.Journal de
Mathématiques Pures et Appliquées, 157:101–135, 2022. ISSN 0021-7824. doi: https://doi.org/10.1016/j.matpur.
2021.07.009. URLhttps://www.sciencedirect.com/science/article/pii/S0021782421001124.
P. Shojaee, I. Mirzadeh, K. Alizadeh, M. Horton, S. Bengio, and M. Farajtabar. The illusion of thinking: Understanding
the strengths and limitations of reasoning models via the lens of problem complexity, 2025. URLhttps://arxiv.
org/abs/2506.06941.
N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: a simple way to prevent neural
networks from overfitting.The journal of machine learning research, 15(1):1929–1958, 2014.
J. Su, M. H. M. Ahmed, Y . Lu, S. Pan, W. Bo, and Y . Liu. Roformer: Enhanced transformer with rotary position
embedding.Neurocomputing, 568:127063, 2024. doi: 10.1016/J.NEUCOM.2023.127063. URLhttps://doi.
org/10.1016/j.neucom.2023.127063.
Y . Sun, L. Dong, S. Huang, S. Ma, Y . Xia, J. Xue, J. Wang, and F. Wei. Retentive Network: A successor to Transformer
for large language models, 2023. URLhttps://arxiv.org/abs/2307.08621.
A. M. Turing. Computing machinery and intelligence.Mind, 59(236):433–460, 1950. ISSN 00264423. URL
https://www.jstor.org/stable/2251299.
M. Udell and A. Townsend. Why are big data matrices approximately low rank?SIAM J. Math. Data Sci., 1(1):
144–160, 2019. doi: 10.1137/18M1183480. URLhttps://doi.org/10.1137/18M1183480.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all
you need.Advances in neural information processing systems, 30, 2017.
J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V . Le, and D. Zhou. Chain-of-thought
prompting elicits reasoning in large language models. InProceedings of the 36th International Conference on
Neural Information Processing Systems, NIPS ’22, Red Hook, NY , USA, 2022. Curran Associates Inc. ISBN
9781713871088.
G. Weiss, Y . Goldberg, and E. Yahav. Thinking like transformers. In M. Meila and T. Zhang, editors,Proceedings of the
38th International Conference on Machine Learning, volume 139 ofProceedings of Machine Learning Research,
pages 11080–11090. PMLR, 18–24 Jul 2021. URLhttps://proceedings.mlr.press/v139/weiss21a.html.
51
J. C. Whittington, T. H. Muller, S. Mark, G. Chen, C. Barry, N. Burgess, and T. E. Behrens. The Tolman-Eichenbaum
Machine: Unifying space and relational memory through generalization in the hippocampal formation.Cell, 183
(5):1249–1263.e23, 2020. ISSN 0092-8674. URLhttps://doi.org/10.1016/j.cell.2020.10.024.
J. C. R. Whittington, J. Warren, and T. E. Behrens. Relating transformers to models and neural representations
of the hippocampal formation. InInternational Conference on Learning Representations, 2022. URLhttps:
//openreview.net/forum?id=B8DVo9B1YE0.
R. J. Williams and J. Peng. An efficient gradient-based algorithm for on-line training of recurrent network trajectories.
Neural computation, 2(4):490–501, 1990.
A. Yang and D. Chiang. Counting like transformers: Compiling temporal counting logic into softmax transformers,
2024. URLhttps://arxiv.org/abs/2404.04393.
G. Yang. Wide feedforward or recurrent neural networks of any architecture are gaussian processes.Advances in
Neural Information Processing Systems, 32, 2019.
Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy. Hierarchical attention networks for document classifica-
tion. InProceedings of the 2016 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages 1480–1489, 2016.
C. You, K. Wu, Z. Jia, L. Chen, S. Bhojanapalli, J. Guo, U. Evci, J. Wassenberg, P. Netrapalli, J. J. Willcock, S. Subra-
manian, F. Chern, A. Andreev, S. Pathak, F. Yu, P. Jain, D. E. Culler, H. M. Levy, and S. Kumar. Spark transformer:
Reactivating sparsity in FFN and attention, 2025. URLhttps://arxiv.org/abs/2506.06644.
H. Zhou, A. Bradley, E. Littwin, N. Razin, O. Saremi, J. M. Susskind, S. Bengio, and P. Nakkiran. What al-
gorithms can transformers learn? A study in length generalization. InThe Twelfth International Conference
on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL
https://openreview.net/forum?id=AssIuHnmHX.
M. Zou.Aspects of Efficiency in Selected Problems of Computation on Large Graphs. PhD thesis, Paris Diderot
University, France, 2019. URLhttps://tel.archives-ouvertes.fr/tel-02436610.
52
A Connection between generalization of reasoning and computational expressiveness
State-of-the-art reasoning models have the interpretation of (Turing-complete) programs, executed over a certain pe-
riod of time. This shifts the emphasis of generalization, from discovering the structure of mathematical functions
which maps inputs to outputs, to discovering a class of runnable programs, which take as input a given class of input
prompts, and process these prompts “in the right direction”.
Consider a given reasoning task, whose scope is defined as a setPof valid input prompts, given as bounded-length
token sequences over some alphabetΩ. Given a prompt fromP, a model solving the considered task is eventually
(i.e, after some number of steps of reasoning) expected to generate an output, in the form of a bounded-length token
sequence over the same alphabetΩ, which is subjected to evaluation. Consider language models sampled from some
probability distributionM 1 over parameter sets in some architectureA 1.
Now, suppose that for some other model architectureA 2 there exists a distributionM 2 over language models inA 2
such that, for a valid input prompt chosen uniformly at random fromP, the outputs sampled from a modelM 1 ∼ M1
and the outputs sampled from a modelM 2 ∼ M 2, have (almost) the same distribution in the space of bounded-
length sequences overΩ, and are both obtained within some asymptotic bound on the number of steps of reasoning,
in expectation. The described setting is equivalent to saying thatmodelsM 2 have generalized the considered taskP
in (almost) the same way as modelsM 1. Indeed, conversely, if the described condition did not hold, we could, in a
finite number of trials, distinguish solutions to problemPobtained by model familiesM 1 andM 2.
Now, consider model architecturesA 1,A 2 which apply Chain-of-Thought reasoning (Wei et al., 2022). A model
in such an architecture has the interpretation of a trainable probabilistic program, taking inputs fromP, and the
architectures themselves represent computational machine architectures. Moving to a discussion of computational
expressiveness, we obtain the following statement.
Observation 9.Given a probability distribution of modelsM 1 in architectureA 1, suppose there exists a distribution
over models in architectureA 2 which generalizes on taskPin the same way as models fromM 1. Then, the machine
architectureA 2 has sufficient computational expressiveness to simulate programs fromM 1 efficiently on the set of
inputsP, i.e.,A 2 contains programs which obtain an (almost) identical distribution of outputs within the given bounds
on running time.
In particular, we note that if we were to consider the special case ofA 1 being reasonable human agents, we could
say that architectureA 2 generalizes reasoning, in the same way as humans, if we can train modelsM 2 inA 2 which
accurately reproduce the outcomes of reasoning for some sampleM 1 of humans inA 1.
This leads us naturally to describe Language Model generalization through a universal reference to the principles of
operation of the human brain, treated as a distributed computing architecture, and not through a characterization of
language and reasoning promptsPthat the model should be able to deal with in some specific way.
B Further description of experiments
B.1 Language translation task
We have evaluated our models on a mixed language modeling and translation task derived from the Europarl cor-
pus (Koehn, 2005). The corpus consists of sentence-level aligned translations of transcripts of European Parliament
proceedings. For each language pair, we treat the data as a long stream of interleaved source and target sentences
(sampling for each sentence which language is the source, and which is the target) on which we train decoder only
models. Thus, models are jointly trained as language models and translators. We train all models using Truncated
Backpropagation Through Time (Williams and Peng, 1990). Subsequent minibatches served by the data loader are
related: each is a continuation of the previous. Each model maintains a recurrent state, carried across minibatches:ρ
matrix for BDH-GPU and a FIFO buffer of recent KV-cache entries for the TransformerXL (Dai et al., 2019) base-
line. We train all models on raw UTF8 data. We are mainly interested in model comparison and prefer to keep the
experimental setup as simple as possible. A few minibatches are shown in Fig. 16.
The joint language modeling and translation formulation has several benefits:
1. Next token prediction is representative for LLM training. Simple architectures, such as decoder-only models
are sufficient.
2. The task promotes models with long context capabilities — subsequent sentences are related and the model
can meaningfully utilize long context to model the source language sentences.
53
model num embd num MLP dropout Carried KV-cache
size layer dim head dim size
25M 9 480 5 1920 0.01 4096
50M 12 576 6 2304 0.02 4096
100M 15 768 8 3072 0.02 4096
200M 18 960 10 3840 0.002 4096
400M 25 1152 12 4608 0.005 4096
800M 28 1536 16 6144 0.15 4096
Table 4: Hyperparameters for GPTXL baselines in scaling experiments. The model architecture follows GPT2 (Radford et al.,
2019), with a FIFO buffer of past KV-cache entries (Dai et al., 2019).
3. The task promotes models which carry state across minibatches, as training data is temporally coherent and
the final model state at the end of one minibatch is a natural initialization of hidden state on the next minibatch.
4. Translation can be seen as language modeling coupled with fuzzy copying. Successful models will need to
develop in-context learning capabilities such as inductive heads (Olsson et al., 2022).
0. |<F:en>For countries such as Sweden and Finland, another system o|
1. |f allocation would be extremely significant.<T:es>Por ejemplo, p|
2. |ara pa•íses como Suecia y Finlandia tendr•ía un gran significado|
3. | que se hiciese otra forma de distribuci•ón.<F:es>El diputado Fe|
4. |rber ha presentado una propuesta que implica una distribuci•ón m|
5. |•ás flexible, y yo respaldo esta enmienda.<T:en>Mr Ferber has ta|
6. |bled an amendment which involves our looking in a considerably m|
7. |ore flexible way at the present allocation, and I support this a|
8. |mendment.<F:en>.<T:es>.<F:en>(NL) Mr President, I would like to |
9. |start by thanking both parliamentary committees and not least bo|
Figure 16: Exemplary sequence of 10 successive minibatches from the translation task. The model is trained on raw UTF8 bytes (for
visualization we pad multi-byte UTF8 characters with “•” symbol). Special token strings<F:lang_code>and<T:lang_code>
delimit source and target sentences. Minibatches are temporally coherent: source sentences are followed by their translations, and
subsequent source sentences are part of the same larger document.
B.2 BDH Scaling Experimental Details
We provide details on models used in scaling experiments described in Section 4.2. All models were implemented in
PyTorch (Paszke et al., 2019) and trained on the Europarl (Koehn, 2005) task described in Section B.1. We have kept
the same training regime for all models at all sizes: En-PL and En-Cs language pairs (380MB total). All models trained
on raw UTF8 bytes seeing a total of 1.2B tokens (about 3 epochs). All minibatches were 2048 tokens long, but we
have varied the number of examples in the minibatch (varying number of tokens in each minibatch) to accommodate
different memory requirements of different models. We have used multi-GPU training using the Distributed Data
Parallel approach using AdamW (Loshchilov and Hutter, 2019) with learning rate10 −3, and 1000 warm-up step
followed by linear learning rate decay over the course of training to10 −4, adaptive gradient clipping (Kumar et al.,
2025), and weight decay0.1. Models were trained to operate on a context longer than minibatch length using Truncated
Backpropagation Through time (Williams and Peng, 1990).
The Baseline model, dubbed GPTXL, was a GPT2-like transformer (Radford et al., 2019) based off the
NanoGPT (Karpathy, 2024) implementation with KV-cache carried across minibatches as in TransformerXL (Dai
et al., 2019). We have used ALiBi positional biases Press et al. (2022). We list its hyperparameters for various model
sizes in Table 4. Optimal Dropout was selected using a small sweep at each model size.
BDH-GPU directly uses model code provided in Appendix E. BDH-GPU’ adds xLSTM-like gating mechanism (Beck
et al., 2024), and merges next token predictions from all layers. Both BDH-GPU and BDH-GPU’ use same architec-
tural hyperparameters, gathered in Table 5.
54
model numd nnum dropout
size layer head
25M 8 256 32768 4 0.1
50M 8 256 65536 4 0.1
100M 8 256 131072 4 0.1
200M 8 256 262144 4 0.1
400M 8 256 524288 4 0.1
800M 8 256 1048576 4 0.1
Table 5: Hyperparameters for BDH-GPU models in scaling experiments.
Model Init. Training Data size Training n d num. num. param.
from data (bytes) tokens heads layers count
BaseEnEs — En-Es 612M 1.2B 24576 256 4 8 19M
TunedEnFr BaseEnEs En-Fr 640M 1.2B 24576 256 4 8 19M
TunedEnPt BaseEnEs En-Pt 616M 1.2B 24576 256 4 8 19M
MergedEnEsFrPt TunedEnFr+TunedEnPt — — — 49152 256 4 8 38M
Table 6: Architecture and training details for model merging experiments.
B.3 BDH Monosemantic Synapse Experiment Details
We provide details for models used in exploration of monosemantic synapses in Section 6.2. The model was trained
on Europarl (Koehn, 2005) described in Section B.1. It hadd= 256, n= 49152,4attention heads, and8layers.
The model was trained on about one epoch of En-Es, En-Pt, and En-Fr data (total 1.9B tokens) in a Distributed Data
Parallel setup using AdamW (Loshchilov and Hutter, 2019) with learning rate10 −3, 1000 warm-up step followed
by linear learning rate decay over the course of training to10 −4, adaptive gradient clipping (Kumar et al., 2025),
and weight decay0.1. We have used Truncated Backpropagation Through time, carrying over the recurrent state of
attention and training on sequences of length2048characters at a time. We have used minimal Dropout (Srivastava
et al., 2014) of0.01.
B.4 BDH Merging Experiment Details
We provide details for models described in Section 7.1 All models were trained on Europarl (Koehn, 2005) described
in Section B.1. We provide model architecture hyperparametrs in Table 6. Models were trained on about two passes
over the training set in a Distributed Data Parallel setup using AdamW (Loshchilov and Hutter, 2019) with learning
rate10 −3, 1000 warmup step followed by linear learning rate decay over the course of training to10 −4, adaptive
gradient clipping (Kumar et al., 2025), and weight decay0.1. We have used Truncated Backpropagation Through
time, carrying over the recurrent state of attention and training on sequences of length2048characters at a time. We
have used minimal Dropout (Srivastava et al., 2014) of0.01.
C Omitted formal claims and proofs
C.1 Proof of Observation 1
Proof.The equivalence is straightforward to verify, rewriting the linear-algebraic multiplication expressions of Eq. (6)
in Einstein summation notation and comparing respective index pairs. At any time, during the execution of rules for
layerl, variablesX(i),Y(i)andσ l(i, j)in the protocol description, fori, j∈ {1, . . . , n}correspond to thei-th
coordinate of vectorsx t,l (based onx t,l−1 from the previous round),y t,l (based ony t,l−1 from the previous round),
and matrix entryσ t,l (based onσ t−1,l from the previous token). The auxiliary variableA(i)corresponds to a similar
auxiliary vectora t,l :=σ t−1,lxt,l in an intermediate step of computation ofyt,l fromx t,l. The parameteru(i, j)∈R +
associated with an element of state follows from the definition of matrixU; we assume for simplicity thatUis diagonal
(which corresponds to the case of ALiBi). Finally, in Table 1, the auxiliary node variablesX e(i), Xi(i), Y e(i), Y i(i)
are used to handle the thresholding of the inhibitory circuit.
55
C.2 Formal statement of Claim 7 (linear attention)
We provide the following Claim, expressing the operation of attention underC-non-adversarialkey vectors(k τ),
t= 1. . . t, understood in the sense that there existsC∈N,0≤C < t−1such that, if considering(k τ)as a
sequence of random variables, eachf(k τ),τ= 1. . . t, can be considered sampled independently at random inS ν
with respect to all keys sampled previously, except for at mostCsuch keys. We putC=t−1for adversarial inputs,
or if this condition cannot be satisfied at all due to the nature of functionf.
Claim 8.LetΛbe a space of keys and queries, letϕ: Λ×Λ→[−1,1]be an attention affinity function, and let
f: Λ→S ν, for someν=O(poly(n)), be such that for anyq, k∈R, we havef(q)·f(k) =ϕ(q, k)±O(n −100).
Fixδ >0andC∈N. LetA ϕ,t be a block which computes attentiona t given by Eq.(14), for a given sequence of
key-query inputs(k 1, . . . , kt)and values(v 1, . . . , vt), wheret < δn/((C+ 1) logn)is fixed,k τ ∈Λ, andv τ ∈R d
are of similar strength in the L2-norm, withc1 ≤ ∥v τ ∥ ≤c 2, for allτ= 1. . . t, for some constants0< c 1 ≤c 2. Then
the (simplified) linear attention equation of BDH-GPU:
a∗
t :=
t−1X
τ=1
vτ xτ
T xt (17)
expressesA ϕ,t withO(
√
δ)-error in the L2-norm (i.e.,∥a ∗
τ −a τ ∥=O(
√
δ), provided that the input vector(k τ)is
C-non-adversarial, under a suitable randomly chosen key preparation functionf ′ : Λ→R n ,x τ :=f ′(kτ), where
f ′ depends onf, w.h.p. innwith respect to choice off ′.
Proof (sketch).To simplify notation, assume w.l.o.g. thatΛ =S ν andf=idem; to undo this assumption, at the end
of the proof we applyf◦f ′ for preparation in place off ′.
All vectorsvand the resulta t we are looking to calculate are inR d. With this notation, the attention task we are
approximating is:
at =q
tX
τ=1
kT
τ vτ .(18)
(this is still the general form of attention almost precisely equivalent to (14), not a special case).
The goal is to show how, subject tot < δn/log 2 n, linear attention in dimensionngiven by (17) is a sufficiently
precise estimation of (18).
Consider now, withΛ =S ν,f ′ :S ν →R n, where we recall thatx τ :=f ′(kτ), to be a suitable dimensionality
reduction preserving approximation of scalar product betweenR ν andR n. For simplicity of argument, we letf ′ :
Rν →R n be a standard Johnson-Lindenstrauss transform, with the additional property thatf ′(−z) =−f ′(z)for all
z∈R ν (easy to obtain from any other Johnson-Lindenstrauss transformf ′′ by takingf ′(z) := (f ′′(z)−f ′′(−z))/2).
The distortion of scalar product inR n is then known to be bounded as follows:|k τ
T kt −x τ T xt|=O(ε)(∥k τ ∥+
∥kt∥) =O(ε), w.h.p. with respect to choice off ′. Here,ε=
p
logn/n=O(
√
δ)/
p
(C+ 1)tlogt, where the last
inequality follows from the assumption ontmade in the Claim.
We now consider the sequencer τ :=k τ
T kt −x τ T xt, forτ < t. Set aside the (at mostC) elementsr τ for which
kτ andk t are not independent. For all other elements, consider that|r τ |=O(ε)as established previously, and the
signr τ /|rτ |is chosen independently at random with respect to all but at leastCelements by the conditions imposed
onf ′ andk τ . It follows that Pt
τ=1 rτ can be represented as a sum ofO(C)martingales, each of which has length
O(t/(C+ 1))and all elements bounded byO(ε)withε=O(
√
δ)/
p
(C+ 1)tlogt. The Claim follows directly, by
applying Azuma’s inequality to each of these martingales independently.
Considering the extreme cases ofC= 0andC=t−1, the above Claim leads directly to Claim 7, clarifying over
what time, linear attention can be used to express general attention.
C.3 Proof of Claim 3
Proof.The proof is almost immediate, through the construction of an appropriate neuron-synapse interaction graphs
H e,H i such thatG e =H e2[V]andG i =H i2
[V]. ConsiderE ′ ∈(R +)2d×n such thatE ′
α,j = (E α,j)+ and
E′
α+d,j = (−Eα,j)+, forj∈ {1, . . . , n}andα∈ {1, . . . , d}. DefineD e, Di ∈(R +)n×2d so that:
(De −D i)E′ =DE.
56
Indeed, notice that this is always possible by redistributing elements ofDintoD e andD i (puttingD e
i,α =D i
i+d,α =
(Di,α)+) andD i
i,α =D e
i+d,α = (−Di,α)+), so that, for alli, j∈ {1, . . . , n}andα∈ {1, . . . , d}, we have:
(De
i,α −D i
i,α)E′
α,j + (De
i,α+d −D i
i,α+d)E′
α+d,j =D i,αEα,j.
ConsideringS={1, . . . ,2d}, the definition ofH e as the union of edges ofD e andE ′ on input neuron layerV,
hidden layerS, and output neuron layerVfollows. Likewise, we defineH i as the union of edges ofD i andE ′.
We verify that forG e =H e2[V]andG i =H i2
[V], we haveG e −G i =DE, and the Claim holds.
Considerations of building linear circuits.The above proof makes the neuron-synapse interaction graphsH e,
H i sparse in terms of the number of edges, as required to show that the number of parameters are preserved by
correspondence. However, it is a purely technical construction, and nodes in the synaptic layer have high degree,n.
While preserving strict equivalence of linear dynamics, the degrees of nodes of the considered graphs in the synaptic
layer can be reduced in this construction, at the cost of increasing the number of edges of graphsH e,H i. (For
example, subdividing each node of the synaptic layer intoa 2 nodes can be used to reduce their degreeΘ(a)-times,
while increasing the number of edgesΘ(a)-times; puttinga=
p
n/dwe reach graphsH e,H i with degreeO(
√
nd)
in both the neuron and synaptic layers, and consequentlyO(n
√
nd)edges.)
Reduction of internal degrees in this circuit is also possible by introducing more than 1 hidden layer, creating a form of
branching circuit. The implementation for this in a distributed way remains very simple, as the considered dynamics
of the formz→Gzare linear (token-propagation dynamics). The bound on the number of edges needed to represent
such a circuit remainsO(nd), even when the circuit has constant degree.
The technical construction of the linear circuitsH e,H i provided in this Appendix do not affect results concerning the
analysis of the structure of neuron-neuron interaction graphsGe,G i. These neuron-neuron interaction graphs plausibly
maintain a heavy-tailed, power-law-like degree distribution, as is the case for the models considered empirically in
Section 5.5.
C.4 Formal statement of Claim 4
Claim 9.LetD y, Ebe parameter matrices of BDH-Normfree. Then, there exists a graphG y ∈ G(n, O(nd)), express-
ible through a sparse linear circuit, a graphG s havingO(nd)edges, and a sparse linear value preparation function
A:R +n →R +n, such that, for any sequence of keys(xτ,l)0≤τ≤t and values(y τ,l−1)0≤τ≤t , withx τ,l, yτ,l−1 ∈R +n,
we have:
(Gy
e −G y
i)σ∗
t−1,lxt,l =D yEσ t−1,lxt,l,
whereσ t−1,l = P
τ <t yτ,l−1xτ,l T U t−τ represents the attention state of BDH-Normfree following Eq.(16), and
σ∗
t−1,l =
 P
τ <t A(yτ,l−1)xτ,l T U t−τ
⊙G s represents the corresponding attention state of the BDH system with
sparse attention on graphG s, subject to appropriate preparation of attention values using functionf y.
Before we start the proof, we make a general point about the formulation of the claim. We are considering the problem
of expressing (or more generally, approximating) the matrix operatorσ t−1,l by another, sparser one. The setting of
our problem can be distilled into obtaining an equality or approximation of the formEσ t−1,l ≈E ∗σ∗
t−1,l, where
E∈R d×n is a given low-rank matrix,E ∗ ∈R d×n can be defined arbitrarily, andσ ∗ is defined as in the statement of
the Claim. If we content ourselves with an approximation, then it is possible to haveσ ∗ =σ(i.e., putf y =idem),
using for example the stochastic sparsification framework of (Achlioptas and Mcsherry, 2007), or a value-dependent
variant (cf. e.g. (Krauthgamer and Sapir, 2023)). The samples chosen by such a framework in a value-dependent
variant would lead to a graphG s which plausibly reflects the power-law element distributions that we empirically
observe inσ.
While the spirit of such an approximation is generally valid, we opt in the proof for a simpler, purely technical
argument applicable to our specific setting, which gives a strict equality in the statement of Claim 9 subject to linear
preparation of attention values with a functionA. In practice, this would mean that two successive layers of BDH with
sparse state are sufficient to express a layer of BDH-Normfree under this reduction.
To prove the claim, it is enough to embed the connection structure of the encoder matrix, treating it as a graph, into
Gs.
Proof.(of Claim 9) Fix arbitrarily subsetD⊆Vof neurons, with|D|= 2d. For the given matrixE∈R d×n from
BDH-GPU, letE ′ ∈(R +)2d×n be defined as in the proof of Claim 3 in Appendix C.3, and letD y
e,D y
i also be
57
d
E
Gs
yl-1,τ(j) 
xl,τ(i) 
xl,t(i) 
Figure 17: Non-uniform graph attention: interpretation ofE(σ l,t ⊙G s)after sparsification of graphG s.
applied as in that proof for considerations of decoderD y. Define the value preparation functionAas the immersion
of vectors overVintoDusingE ′. DefineG s to be the all-ones matrix on the2dcolumns corresponding toD, and
zeros elsewhere. Then, defineE ∗ ∈R 2d×n to be a diagonal matrix acting on its first2delements (corresponding to
D), and zeros elsewhere. SettingG y
e =D y
eE∗ andG y
i =D y
iE∗, we obtain the claim.
D Desirable properties of a local graph dynamics for language models
We outline several general criteria of computational expressiveness and computational efficiency which a distributed
computing system has to meet to effectively deal with language and reasoning. For this, we take a first-principles
approach, relying only on very fundamental properties which an attention-based language model appears to need to
capture, and which are applicable far beyond the specific case of BDH — plausibly, being equally applicable to human
and human-like reasoning.20
Hypothesis 2.We expect any efficient graph-based distributed system dealing with language and reasoning using an
attention-based approach to have the following characteristics:
•[No Easy Simulation] The system achieves computationally irreducible dynamics, i.e., it provides no system-
atic opportunity to predict the outcomes of its inference or approximate its dynamics in a numerically easier
way than by running the system itself.
•[Particles Talk] The state-space dynamics of the distributed system is a non-linear interacting particle dynam-
ics, i.e., the system does not admit an efficient representation as a non-interacting particle system, but relies
on a form of non-linear evolution expressed through (at least) two-particle interactions. (Such interactions
are necessary, in particular, to enable multi-point correlation analysis on language inputs, when assuming
only a small number of inference steps of the system per output token.)
•[Attention Deforms Pairwise Connections] The system is capable of computing correlations between pairs of
scalar variables localized at different nodes of the distributed system, and storing the state of such correla-
tions so that the result is accessible from these two nodes. (This is plausibly needed to express attention in a
state-space system.)
•[Time Dictates Structure] The communication graph of the distributed system does not, in itself, represent
any specific task input to solve, but reflects a trained model (a program), whereas tasks are represented as
inputs to this program, presented over time. The communication graphs used to solve language and reasoning
problems are expected to display modular, scale-free structure.
A detailed discussion of the four items of the Hypothesis is provided below.
20In particular, the reader will have no doubt observed that graph settings applicable to language inference and reasoning sys-
tems, which involve task inputs spread out over time and the emergence of graph structure, are very different from graph-based
frameworks which directly associate the task to solve with the communication graph (the latter case includes most considerations
of: Graph Neural Networks, Graph Transformers, the LOCAL/CONGEST model of distributed computing, Approximate Message
Passing systems, etc.)
58
[No Easy Simulation]⋄Computational models have irreducible dynamics.
We start by recalling a general observation which is applicable to most learning systemsL(machine learning models,
biological systems) that have learned how to do computations: they are likely to have chosen state-space dynamics
that will allow them to resolve their computational problem with the least effort during inference. In other words,
if there is a physical systemPthat solves a given computational problem, and if there exists a simulationS(P)of
this physical system that would approximate systemPwith less effort, the learning systemLwill be following the
dynamics ofS(P), not those ofP.
We provide a few hypothetical examples for intuition, anchored in different areas of particle dynamics.
IfPwere the particle dynamics of electrons in a resistor network, the simulationS(P)could be a calculation based on
Ohm’s law with a Laplacian solver — and we would consequently expect the dynamics of our computational system
Lto follow the Laplacian solver code, and not to simulate electron dynamics.
IfPwere the ensemble of billions of Internet users performing short walks clicking through links of the world wide
web, the simulationS(P)would be a calculation of aggregate behavior, reminiscent of PageRank, and we would ex-
pectLto encode the parallel dynamics of Map-Reduce matrix operations of PageRank, not the simulation of individual
agents.
IfPwere a quantum system amenable to approximation by perturbation theory, we would expectLto simulate the
(classical) calculus of this perturbation theory, and not the quantum systemPdirectly.
Most mechanical systems admit some form of more efficient simulation, which means the the dynamics of such
systems are rarely a suitable choice for neuronal models. Anecdotally, in nature, only very simple systems like the
Physarum slime mold (Jabr and Rothschild, 2012) rely on direct action (with hydrostatic pressure gradients) to perform
their optimization process; and contemporary neuroscience research suggests that even the simplest neuronal brains
do not perform their work in a similar “fluid-mechanical” manner.
The irreducibility ofLmeans that this system is stretched to the limits of stability, just as a highly optimized numerical
algorithm would be have been simplified and optimized to the limit of numerical stability. This relates to the limits of
dimensionality reduction techniques that we have explored through a largely equivalent information-lens perspective
of loss of precision and loss of information which it inflicts upon the model.
[Particles Talk]⋄Latent concept spaces arise from outcomes of particle-particle interactions.
Dynamics of systems with multiple particles moving around in a (deformable) environment fall into two broad cat-
egories, depending on the strength of interaction between different parts of the dynamics. In the simpler setting,
particles can be assumedat short time scalesto be moving in an environment unchanged by other particles — the con-
current action of other particles, which would change the environment, does not need to be taken into account when
representing individual particle motion, nor is it necessary to consider particle-particle interactions. By contrast, in the
more general setting, the dynamics of multiple particles are tightly coupled, and their dynamics need to be modeled
(simulated) together.
An example of a dynamics with no coupling would be a dynamics of multiple independent random walkers, such as the
previously mentioned dynamics of electricity in wires, or the dynamics of PageRank. Examples of dynamics including
interactions between particles, which may either happen directly or be moderated through the environment, include
cellular automata, particle method simulations and molecular simulations, or swarms of communicating agents.
The natural representation of state-space models as moving particles comes from the following interpretation. A
distributed system with depth-Lcomputations (not least BDH or the BDH-GPU model given by the state equations (4))
is amenable to interpretation as a system of walker particles performing anL-step walk over layers, starting at some
tokent 0 in the input layer0and, in each time stept≥t 0, either pausing (skipping a time step) or moving on to the
next layer, until they reach the last layerLin some time stept f , at which point they leave the system, contributing to
the distribution of thet f -th output token. When attempting this approach withindependentwalkers, the distribution
of tokens output by such a system could be described by correlation functions following or resembling the Dyson
series, Pt
τL=0
PτL−1
τL−1=0 . . .Pτ2−1
τ1=0 F(input(τ 1), . . . ,input(τL)). However, the output of attention (e.g., the linear
attention outputa ∗ given by equation (4) for BDH-GPU, or defined similarly in other state space models based on
linear attention), cannot be represented as a Dyson formula when unrolling the dynamics backwards through layers
(even if it looks deceptively similar at first glance). Each entry retrieved from attention is an interplay between two
moments of time: the moment at which the key-value pair was entered, and the moment at which the corresponding
query arrived. In consequence, the considered dynamics can be represented, in each layer, as a linear sum of two-point
correlations between current timetand some pointτin the past. Thus, in thel-th layer, this recursion can (with some
59
approximation) be unrolled into a linear combination of functions of sets of2l input tokens (provided in the0-th layer),
but cannot be represented through correlation functionsFon smaller sets of tokens (e.g., of size linear inl). Otherwise
put, a system like BDH can be described using particles performingl-step walks whenrelying on intermediate elements
of KV-stateσ, which are produced during interactions with other walker particles in intermediate layers, but needs to
be viewed through at least2 l-point correlation functions defined directly on input tokens in the input layer.
The considered point is relevant because itprecludes many forms of modeling of attention-based language dynamics,
in particular those using non-interacting particle theories. The precluded approaches include:
•L-grams,word2vec-likeL-skip-grams (Mikolov et al., 2013), as well as any otherL-point correlations of
past input tokens.
•L-step non-interacting random walk models (walks inside the network structure, which move from input
layers towards output layers across time).
• systems known to be equivalent to the above, such as approximations of classical spin-chain systems by
means of Feynman integral path lengths bounded byL(Kalev and Hen, 2025), and many forms of graph/GNN
kernels based onL-th powers of the graph Laplacian.
• by extension,L-layer state-space systems which perform excessive compression (size reduction) of their
state, in a way which eliminates most long-term correlations.
We can ask if this requirement for communication between particles is an artifact of the construction of BDH (and
similarly, of the Transformer), or if it comes from a genuine need related to language and reasoning tasks. For language
problemsper se, the need for multi-point token correlation inL-layer language modeling plausibly follows from the
expectation that the model should have the ability to create a syntax tree of a sentence by means of a single quick
parallel scan over words in this sentence. With this assumption, the depthLof computation used to build a language
syntax tree should be sufficient to represent the number oflevelsof the syntax tree that the model is able to process
naturally, but can be (and in general, should plausibly be) much smaller than the number ofleaves(words) of this
syntax tree. This is consistent with the RASP-L-based understanding of the Transformer’s capabilities, which allows
for expressing depth-L trees in a depth-L Transformer.21
Such a way of mapping the tree structure of problems into the model’s layers, from bottom to top, also essentially
captures the “generative” nature of the considered models, which rely on concept spaces created and stored in state
in intermediate layers, to guide both language comprehension and reasoning on language. Thus, the ability to handle
language syntax trees efficiently, in itself, precludes the previously-mentioned types of modeling approaches.
[Attention Deforms Pairwise Connections]⋄The interaction processX(i), Y(j)→σ(i, j)describes attention.
The preceding discussion in paragraph [Particles Talk] grounds state-of-the-art state-space language models in the
world of interacting particle systems.
Whenever the global vector-based description of a state-space model calls for a three-point operation, such as the
trilinear operation of key-value-query attention, this translates into the nature of pairwise (for polynomial interaction
terms, degree-two) non-linear particle interactions in the transition equations of the same model when described at the
level of particles. Notably, at scale,the state-space transition equations of an attention-based model plausibly involve
altering or deforming correlation strength between pairs of particles, with such pairs being represented as interaction
variables in the state of the system. This requirement on structure, repeated across layers, can be seen as sufficient:
interactions of particle pairs are about the only requirement on non-linear rulesets that the system needs to be support,
as demonstrated by the simple local transition rules of BDH.
Overall, the statement “attention is all you need”, which describes a system-level global property, translates into
“X(i), Y(j)→σ(i, j)is all you need” at the level of particle dynamics of a state-space language model.
[Time Dictates Structure]⋄Inputs to reasoning problems are sequential, not graph-based.
Many real-world graphs are anchored in a spatial embedding of their nodes which is given by external constraints.
For example, the structure of many social and transportation networks is impacted by the geographical placement of
people and infrastructure on the globe.
21This does not mean the problem is easy; synthetic problems inspired by this type of tree problem were (for us) among the
hardest to train into a Transformer with no Chain-of-Thought — as compared to RASP-L problems described in (Zhou et al., 2024)
and others we tested.
60
In designing the dynamics for BDH, we are free from such spatial constraints. The graph topology corresponding to
the model is free to take the shape needed to best resolve the problem. The problem itself is encoded as a sequence of
tokens which arrive over time to the model (we take here a state-space view of the system). We can naturally presume
that the structure of the model graph of BDH is shaped in a way which follows from two aspects: this temporal
encoding of information, and from the abstract (Platonic) latent space of concepts needed to deal with language and
reasoning.
When looking for the right particle dynamics for language models, it seems reasonable to discard allunnecessary
aspects of spatial constraints.
One example of a particle interaction system which includes externally imposed constraints on the structure of the state
space is that of cellular automata operating on a two-dimensional grid. While 2D cellular automata have appealed to
public imagination, appearing in attempts to observe the emergence of intelligence at least since the 1970’s, they are,
in fact, an extremely cumbersome choice for representing in-context reasoning or language for any attention-based
model. State-of-the-art language models seem to have no structural need for a low-dimensional grid in their dynamics.
Arguably, the connection structure which needs to emerge in a graph system, allowing it to work efficiently in a setting
of efficient information search is precisely the opposite: it is a multi-scale, expander-like system of shortcuts, cf.
e.g. (Fraigniaud and Giakkoupis, 2010). This scale-free graph structure is expected to correspond to the scale-free
temporal behavior observed in natural systems (He et al., 2010).
In the rest of this paragraph we briefly review other areas of computer science, and how they relate to the particle
dynamics we are looking for in terms of their relationship to handling temporal inputs and the constraints they impose
on the structure of the state-space.
The freedom of choice of graph topology in solving problems around language and in-context reasoning, which we
are dealing with here, can be contrasted with settings in which the graph is, at the same time, part of the system
dynamics (encoding interactions in the system) and a part of the statement of the problem input. This is particularly
true for models of distributed computing inspired by computer networking (LOCAL, CONGEST, etc.) and other forms
of interaction networks (Approximate Message Passing, quantum LOCC, etc.), where the same graphGrepresents
the communication network for the dynamics, and encodes the problem input — with the required output being some
function ofG(e.g., a clustering, coloring, spanning tree, etc.). Some distributed problems on graphs can be formulated
so that the input and required output are independent of the graph structure, the notable ones being: majority consensus,
leader election, information broadcasting, and computing aggregates. For such problems, the graph represents only a
communication system, whose topology is more an obstacle to overcome, than an actual help in solving the problem.
This applies also to architectures in Machine Learning which adhere to a known graph structure, such as Graph Neural
Networks or Graph Transformers, when solving problems whose inputs are not naturally embedded in such a structure.
A handful of approaches in distributed computing are intended to describe systems which compute a function of
an input signal which, like language, is spread out sequentially over time, and where computations happen while
this signal is still arriving. In particular, some forms of particle dynamics can be distilled from the theory of self-
stabilizing systems (Dolev, 2000), giving rise to settings where the system is expected to adapt its state in response to
a time-changing input (see e.g. (Boczkowski et al., 2019)). Among distributed streaming frameworks, one approach
which, owing to its design, admits an elegant particle-based interpretation for time-changing inputs, is the incremental
computing framework (McSherry et al., 2013). This framework emphasizes temporal commutativity, and is well suited
to expressing dynamics of non-interacting particles, such as PageRank-like computation performed incrementally with
Map-Reduce on time-changing graphs, or building nearest-neighbor indexes on sets of changing vectors. It does not
naturally extend to the non-linear particle-particle interaction dynamics that appear in the context of attention (see
paragraph [Particles Talk]).
E BDH-GPU PyTorch code listing
The code listing below implements BDH-GPU (Definition 4) for PyTorch version 2.7. It is self-contained, except
for the implementation of RoPE which needs to be filled by the user. With respect to the state dynamics of Eq. (8),
it provides an extension supporting heads. The placement of layer norms and residual connections is modified with
respect to Eq. (8); in general, this aspect offers some flexibility.
This implementation assumes the simplest case of a fixed context window of lengthT. An unbounded context win-
dow is technically supported using a state-space kernel for Linear Attention, and works best following appropriate
adaptation of the model for truncated backpropagation through time (see Appendix B.2).
61
importt o r c h
importt o r c h . nn . f u n c t i o n a lasF
fromt o r c himportnn
D = 256# i n t e r n a l d i m e n s i o n
H = 4# heads
N = 32768# n e u r o n s
L = 6# l a y e r s
d r o p o u t = 0 . 0 5
v o c a b _ s i z e = 256
c l a s sBDH_GPU( nn . Module ) :
def_ _ i n i t _ _ ( s e l f ) :
s e l f . l n = nn . LayerNorm (D, e l e m e n t w i s e _ a f f i n e = F a l s e , b i a s = F a l s e )
s e l f . wte = nn . Embedding ( v o c a b _ s i z e , D)
s e l f . drop = nn . Dropout ( d r o p o u t )
s e l f . e n c o d e r = nn . P a r a m e t e r (
t o r c h . z e r o s ( ( N, D ) ) . normal_ ( s t d = 0 . 0 2 )
)
s e l f . d e c o d e r _ x = nn . P a r a m e t e r (
t o r c h . z e r o s ( ( H, D, N / / H ) ) . normal_ ( s t d = 0 . 0 2 )
)
s e l f . d e c o d e r _ y = nn . P a r a m e t e r (
t o r c h . z e r o s ( ( H, D, N / / H ) ) . normal_ ( s t d = 0 . 0 2 )
)
s e l f . r e a d o u t = nn . P a r a m e t e r (
t o r c h . z e r o s ( ( D, v o c a b _ s i z e ) ) . normal_ ( s t d = 0 . 0 2 )
)
s e l f . a t t n = L i n e a r A t t e n t i o n ( )
deff o r w a r d ( s e l f , i d x ) :
B , T = i d x . s i z e ( )# mini −b a t c h d i m e n s i o n s
v _ a s t = s e l f . l n ( s e l f . wte ( i d x ) . u n s q u e e z e ( 1 ) )# B , 1 , T , D
f o r_i n range( L ) :
x = F . r e l u ( v _ a s t @ s e l f . d e c o d e r _ x )# B , H, T , N / / H
a _ a s t = s e l f . a t t n (
Q=x ,
K=x ,
V= v _ a s t ,
)
y = F . r e l u ( s e l f . l n ( a _ a s t ) @ s e l f . d e c o d e r _ y ) * x# B , H, T , N / / H
y = y . t r a n s p o s e ( 1 , 2 ) . r e s h a p e (B , 1 , T , N)
y = s e l f . drop ( y )
# S t a r t o f l a y e r w i t h v e c t o r s x , y
v _ a s t = v _ a s t + s e l f . l n ( y @ s e l f . e n c o d e r )# B , 1 , T , D
v _ a s t = s e l f . l n ( v _ a s t )
returnv _ a s t . s q u e e z e ( 1 ) @ s e l f . r e a d o u t# B , T , v o c a b _ s i z e
c l a s sL i n e a r A t t e n t i o n ( nn . Module ) :
deff o r w a r d (Q, K, V ) :
Qr = RoPE (Q)
Kr = RoPE (K)
return( Qr @ Kr .mT ) . t r i l ( d i a g o n a l = −1) @ V
62
